[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "확률과정론 (2023)",
    "section": "",
    "text": "보충\n\n1wk-1, 6wk-1, 9wk-2, 10wk-1\n\n중간고사\n\n4월11일 수업시간: 범위 4wk-2까지.\n예상문제 업로드 예정\n\n질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS:\n\n참고도서\n\nDurrett, R. (2019). Probability: theory and examples, (Vol. 49). Cambridge university press.\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\nq_net\n\n\n \n\n\n\n\nSep 1, 2023\n\n\nA3: 강화학습 (3) – LunarLander\n\n\n최규빈 \n\n\n\n\nAug 30, 2023\n\n\nA2: 강화학습 (2) – 4x4 grid\n\n\n최규빈 \n\n\n\n\nAug 29, 2023\n\n\nA1: 강화학습 (1) – bandit\n\n\n최규빈 \n\n\n\n\nJun 20, 2023\n\n\n15wk-2: 기말고사\n\n\n최규빈 \n\n\n\n\nJun 1, 2023\n\n\n15wk-1: MCMC (3)\n\n\n최규빈 \n\n\n\n\nJun 1, 2023\n\n\n14wk-1,2: MCMC (2)\n\n\n최규빈 \n\n\n\n\nMay 30, 2023\n\n\n13wk-2: MCMC (1)\n\n\n최규빈 \n\n\n\n\nMay 25, 2023\n\n\n13wk-1: 마코프체인 (12)\n\n\n최규빈 \n\n\n\n\nMay 23, 2023\n\n\n12wk-2: 마코프체인 (11)\n\n\n최규빈 \n\n\n\n\nMay 18, 2023\n\n\n12wk-1: 마코프체인 (10)\n\n\n최규빈 \n\n\n\n\nMay 16, 2023\n\n\n11wk-2: 마코프체인 (9)\n\n\n최규빈 \n\n\n\n\nMay 11, 2023\n\n\n11wk-1: 마코프체인 (8)\n\n\n최규빈 \n\n\n\n\nMay 9, 2023\n\n\n10wk-2: 마코프체인 (7)\n\n\n최규빈 \n\n\n\n\nApr 27, 2023\n\n\n09wk-1: 마코프체인 (6)\n\n\n최규빈 \n\n\n\n\nApr 25, 2023\n\n\n08wk-2: 마코프체인 (5)\n\n\n최규빈 \n\n\n\n\nApr 20, 2023\n\n\n08wk-1: 마코프체인 (4)\n\n\n최규빈 \n\n\n\n\nApr 13, 2023\n\n\n07wk-1: 마코프체인 (2)\n\n\n최규빈 \n\n\n\n\nApr 13, 2023\n\n\n07wk-2: 마코프체인 (3)\n\n\n최규빈 \n\n\n\n\nMar 30, 2023\n\n\n05wk-1: 마코프체인 (1)\n\n\n최규빈 \n\n\n\n\nMar 29, 2023\n\n\n05wk-2: HW1\n\n\n최규빈 \n\n\n\n\nMar 28, 2023\n\n\n04wk-2: 측도론 intro (6)\n\n\n최규빈 \n\n\n\n\nMar 23, 2023\n\n\n04wk-1: 측도론 intro (5)\n\n\n최규빈 \n\n\n\n\nMar 21, 2023\n\n\n03wk-2: 측도론 intro (4)\n\n\n최규빈 \n\n\n\n\nMar 16, 2023\n\n\n03wk-1: 측도론 intro (3)\n\n\n최규빈 \n\n\n\n\nMar 14, 2023\n\n\n02wk-2: 측도론 intro (2)\n\n\n최규빈 \n\n\n\n\nMar 9, 2023\n\n\n02wk-1: 측도론 intro (1)\n\n\n최규빈 \n\n\n\n\nMar 6, 2023\n\n\n01wk-2: 강의소개\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-07-1wk-2.html",
    "href": "posts/2023-03-07-1wk-2.html",
    "title": "01wk-2: 강의소개",
    "section": "",
    "text": "수업구성\n1. 측도론(실변수함수론)\n\n확률과정을 이해함에 있어서 필요함.\n그런데 학부수준에서는 꼭 필요한 내용은 아님.\n대학원 진학 등 깊이 공부 할 학생들은 필요함.\n\n2. 확률과정론\n\n원래는 금융통계을 위한 백업과목\n여러가지 확률과정 중 우리는 마코프체인에만 집중\n\n3. 마코프체인의 응용 (유동적으로 변경가능)\n\n“마코프체인”이라는 용어가 나오는 응용분야를 리뷰.\nMCMC, 베이지안모형, 토픽모형(LDA), 강화학습, 구글페이지랭크 –&gt; 몇 개만 다를 수 있지 않을까?\n\n\n\n이 수업을 들어야 하는 이유\npass\n\n\n이 수업을 듣지 말아야 하는 이유\n1. F학점 줄 수 있음.\n\n진짜 줌.\n\n2. 쓸모가 없다.\n\n그동안 제가 강의했던 과목들: R입문, 파이썬입문, 통계전산, 데이터시각화, 딥러닝(파이토치/텐서플로우),\n측도론: 재미는 있음. 그런데 대학원가서 고급이론을 공부할 것이 아니면 쓸모가 없다. (통계학과에서 배우는 가장 이론적인 과목)\n확률과정론: 확률과정론 \\(\\to\\) 금융공학으로 가는 교과과정은 학부수준에 다루기 어려움. 마코프체인은 응용이 많이 되는 편이지만 이론을 꼭 알아야 하는건 아니야.\n마코프체인의 응용: 꽤 재미있는 토픽들이 많음. 그런데 이 과목에서 깊게 다루기 불가능.\n\n3. 회귀분석2와 시간이 겹침\n\n이영미교수님 수업!\n회귀분석2는 엄청 중요한 수업이에요.\n지금이라도 늦지 않음"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-21-3wk-2.html",
    "href": "posts/1. 측도론/2023-03-21-3wk-2.html",
    "title": "03wk-2: 측도론 intro (4)",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yaBxW0S3fsO1d-kYIqZa62\n\n\n\n시그마필드 motivation (1)\n(예제1) – 잴 수 있는 집합의 모임\n\\(\\Omega=\\{H,T\\}\\)라고 하자. 아래집합들은 모두 확률을 정의할 수 있는 집합들이다.\n\\[\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\]\n따라서 \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)가 합리적일 것이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n\n이때 \\({\\cal F}\\)는 집합들의 집합인데, 이러한 집합을 collection 이라고 한다.\n\n(예제2) – 집합 \\(A\\)를 잴 수 있다면, 집합 \\(A^c\\)도 잴 수 있어~\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\Omega\\big\\}\\]\n(해설1)\n이러한 묶음이 의미하는건 “앞면이 나올 확률은 모순없이 정의할 수 있지만, 뒷면이 나오는 확률은 모순없이 정의하는게 불가능해~” 라는 뜻이다. 그런데 뒷면이 나올 확률은 “1-앞면이 나올 확률” 로 모순없이 정의할 수 있으므로 “앞면이 나올 확률이 모순없이 정의되면서” 동시에 “뒷면이 나올 확률이 모순없이 정의되지 않는” 상황은 없다.\n(해설2)\n\\(\\Omega\\)의 어떠한 부분집합 \\(A\\)에 확률이 모순없이 정의된다면 그 집합의 여집합인 \\(A^c\\)에 대하여서도 확률이 모순없이 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A \\subset {\\Omega}: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n(예제3) – 전체집합이 잴 수 있는 집합이니까 공집합도 잴 수 있는 집합이야\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)를 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{ \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n(해설)\n전체집합의 확률은 \\(P(\\Omega)=1\\)로 정의할 수 있다. 그런데 전체집합의 여집합인 공집합의 확률을 정의할 수 없는건 말이 안되므로 공집합은 \\(\\cal F\\)에 포함되어야 한다.\n(예제4) – 원소의 수가 유한한 경우 \\({\\cal F}=2^\\Omega\\)은 잴 수 있는 집합의 모임이야.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음은 \\({\\cal F}\\)은 합리적이다.\n\\[{\\cal F}=\\text{all subset of $\\Omega$}= 2^\\Omega = \\big\\{ \\emptyset, \\{1\\}, \\{2\\}, \\dots, \\{6\\}, \\dots, \\{1,2,3,4,5\\} \\dots \\Omega\\big\\}\\]\n(해설)\n\\(\\Omega\\)의 모든 부분집합에 대하여 확률을 모순없이 정의할 수 있다. 예를들면\n\n\\(P(\\Omega)=1\\), \\(P(\\emptyset)=0\\)\n\\(P(\\{1\\})=\\frac{1}{6}\\)\n\\(P(\\{1,2,4\\})=\\frac{3}{6}\\)\n\\(P(\\{2,3,4,5,6\\})=\\frac{5}{6}\\)\n\\(\\dots\\)\n\n이런식으로 정의할 수 있다.\n(예제5) – 동일한 \\(\\Omega\\)에 대하여 잴 수 있는 집합의 모임 \\({\\cal F}\\)는 유니크하지 않음.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{6\\}, \\{1,2,3,4,5\\},\\Omega \\big\\}\\]\n(해설)\n어떠한 특수한 상황을 가정하자. 주사위를 던져야하는데 6이 나오면 살수 있고 6이 나오지 않으면 죽는다고 하자. 따라서 던지는 사람 입장에서는 주사위를 던져서 6이 나오는지 안나오는지만 관심있을 것이다. 이 사람의 머리속에서 순간적으로 떠오르는 확률들은 아래와 같다.1\n1 공평한 주사위라고 하자..\n살수있다 =&gt; 1/6\n죽는다 =&gt; 5/6\n살거나 죽는다 =&gt; 1\n살지도 죽지도 않는다 =&gt; 0\n\n이러한 확률은 합리적이다. 즉 아래의 집합들만 확률을 정의한다고 해도, 확률을 잘 정의할 수 있을 것 같다.\n\\[\\emptyset, \\{6\\}, \\{1,2,3,4,5\\}, \\Omega\\]\n(예제6) – \\(\\Omega\\)를 어떠한 사건의 집합으로 보느냐에 따라서 \\({\\cal F}\\)를 달리 구성할 수 있다.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\{2,4,6\\},\\Omega \\big\\}\\]\n(해설)\n전체사건을 “주사위를 던져서 짝이 나오는 사건”, “주사위를 던져서 홀이 나오는 사건” 정도만 구분하겠다는 의미\n(예제7) – \\(A\\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\Omega \\big\\}\\]\n(해설)\n“주사위를 던져서 홀수가 나올 사건”에 대한 확률을 정의할 수 있는데, 짝수가 나올 사건에 대한 확률을 정의할 수 없다는건 말이 안되는 소리임.\n(예제8) – trivial \\(\\sigma\\)-field\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\{\\emptyset, \\Omega \\}\\]\n(해설)\n아예 이렇게 잡으면 모순이 일어나진 않음. (쓸모가 없겠지)\n(예제9) – 서로소인 두 집합의 합, 포함관계에 있는 집합의 차\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 어떠한 필요에 따라서 1이 나올 확률과 2가 나올 확률에만 관심이 있고 나머지는 별로 관심이 없다고 하자. 그래서 \\({\\cal F}\\)을 아래와 같이 정의했다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\]\n(해설1)\n\\({\\cal F}\\)은 전체집합과 공집합을 포함하고 여집합에 닫혀있으므로 언뜻 생각해보면 합리적인듯 보이지만 그렇지 않다. 왜냐하면 \\(\\{1,2\\}\\)이 빠졌기 때문이다. 1이 나올 확률 \\(P(\\{1\\})\\)와 2가 나올 확률 \\(P(\\{2\\})\\)를 각각 정의할 수 있는데, 1 또는 2가 나올 확률 \\(P(\\{1,2\\})\\)을 정의할 때 모순이 발생한다는 것은 합리적이지 못하다. 왜냐하면 \\(\\{1\\} \\cap \\{2\\} = \\emptyset\\) 이므로\n\\[P(\\{1\\} \\cup \\{2\\})=P(\\{1\\}) + P(\\{2\\})\\]\n와 같이 정의가능하기 때문이다. 따라서 집합이 아래와 같이 수정되어야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\}\\]\n(해설2)\n생각해보니까 \\(\\{2\\}\\)는 \\(\\{2,3,4\\}\\)의 부분집합이다. 그런데 \\(P(\\{2\\})\\)와 \\(P(\\{2,3,4\\})\\)를 각각 정의할 수 있는데\n\\[P(\\{2,3,4\\} - \\{2\\}) = P(\\{3,4\\})\\]\n를 정의할 수 없는건 말이 안된다. 따라서 \\({\\cal F}\\)를 아래와 같이 수정해야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{3,4\\}, \\{1,2\\} \\}\\]\n(해설3)\n\\(\\Omega\\)의 어떠한 두 부분집합 \\(A\\), \\(B\\)가 서로소라고 상상하자. 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(A\\cup B\\)에 대한 확률도 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n또한 \\(\\Omega\\)의 임의의 두 부분집합이 \\(A \\subset B\\)와 같은 포함관계가 성립할때, 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(B-A\\)에 대한 확률로 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n(예제10) – \\({\\cal A}=\\{\\{1\\},\\{2\\}\\}\\) 일때, \\(\\sigma({\\cal A})\\) 를 구하는 문제\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심이 있는 확률은 \\(P(\\{1\\})\\), \\(P(\\{2\\})\\) 밖에 없다고 하자. 이러한 확률들이 무모순으로 정의되기 위한 최소한의 \\({\\cal F}\\)를 정의하라.\n(해설) – 좀 귀찮네..?\n0차수정: \\({\\cal A} = \\big\\{\\{1\\}, \\{2\\}\\big\\}\\)\n1차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\Omega \\big\\}\\)\n2차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\big\\}\\)\n3차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\big\\}\\)\n\n사실 우리가 관심 있는건 \\({\\cal A} = \\{ \\{1\\}, \\{2\\} \\}\\) 뿐 이었음. 그런데 뭔가 \\(P(\\{1\\})\\)와 \\(P(\\{2\\})\\)를 합리적으로 정의하기 위해서 필연적으로 발생하는 어떠한 집합들을 모두 생각하는건 매우 피곤하고 귀찮은 일임. 그래서 “아 모르겠고, \\(\\{1\\}\\) 와 \\(\\{2\\}\\)를 포함하고 확률의 뜻에 모순되지 않게 만드는 최소한의 \\({\\cal F}\\)가 있을텐데, 거기서만 확률을 정의할래!” 라고 쉽게 생각하고 싶은 사람들이 생김. 그러한 공간을 \\(\\sigma({\\cal A})\\)라는 기호로 약속하고 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\) 라는 용어로 부름."
  },
  {
    "objectID": "posts/1. 측도론/2023-03-09-2wk-1.html",
    "href": "posts/1. 측도론/2023-03-09-2wk-1.html",
    "title": "02wk-1: 측도론 intro (1)",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yPGeQuQgZaqhpUJujTtP4g\n\n\n\n예제1: 동전\n- \\(\\Omega =\\{H,T\\}\\): sample space\n- \\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\): prob\n- 질문: \\(\\Omega\\)의 임의의(=모든) 부분 집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이 정의할 수 있을까?\n\n당연한거 아냐?\n이게 왜 안돼?\n\n- 질문에 대한 대답\n\n\\(\\Omega\\)의 부분집합: \\(\\emptyset, \\Omega, \\{H\\},\\{T\\}\\)\n\\(P(\\{H\\})=\\frac{1}{2}\\), \\(P(\\{T\\})=\\frac{1}{2}\\), \\(P(\\Omega)=P(\\{H,T\\})=1\\), \\(P(\\emptyset)=0\\)\n\n- 모순없이의 의미?\n\n우리가 상식적으로 확률에 적용가능한 어떠한 연산들이 있음. (확률의 공리 + 기본성질) // 네이버검색\n이러한 연산을 적용해도 상식적인 수준에서 납득이 가야함\n\n(상식적인 연산 적용 예시1)\n\\(\\{H\\} \\subset \\Omega \\Rightarrow P(\\{H\\})&lt;P(\\Omega)\\)\n\n집합 \\(\\{H\\}\\)은 집합 \\(\\Omega\\)보다 작은 집합임\n상식적으로 작은집합이 일어날 확률이 큰 집합이 일어날 확률보다 클 수 없음\n동전 예제의 경우 모든 \\(A,B \\subset \\Omega\\) 에 대하여, \\(A\\subset B\\) 이라면 \\(P(A) &lt; P(B)\\) 가 성립함\n\n(상식적인 연산 적용 예시2)\n\\(\\{H\\} \\cap \\{T\\} = \\emptyset \\Rightarrow P(\\{H\\} \\cup \\{T\\})=P(\\{H\\}) + P(\\{T\\}) =1\\)\n\n우리의 상식에 따르면 \\(A,B\\)가 서로소인 사건이라면 \\(P(A)+P(B)\\)이어야 함.1\n이 예제는 실제로 그러함.\n사실 이 예제의 경우 \\(P(\\{H\\} \\cup \\{T\\})=P(\\Omega)=1\\) 와 같이 계산할 수도 있음.\n하지만 어떠한 방식으로 계산해도 모순이 없음.\n\n1 확률의 공리\n\n예제2: 바늘이 하나만 있는 시계\n- \\(\\Omega = [0,2\\pi)\\)\n\n시계바늘을 돌려서 나오는 각도를 재는일 \\(\\Leftrightarrow\\) \\([0,2\\pi)\\)사이의 숫자중에 하나를 뽑는 일\n\n- 질문: 바늘을 랜덤으로 돌렸을때 12시-6시 사이에 바늘이 있을 확률? \\(\\frac{1}{2}\\)\n\n\\(\\Omega^* = [0,\\pi)\\)\n\\(P(\\Omega^*)= \\frac{1}{2}\\)\n\n- 계산하는 방법? 아래와 같이 계산하면 가능!!\n\\[\\forall \\Omega^* \\subset \\Omega, \\quad P(\\Omega^*)=\\frac{m(\\Omega^*)}{m(\\Omega)}\\]\n단 여기에서 \\(m\\)은 구간의 길이를 재는 함수라고 하자.\n연습: \\(m\\)의 사용\n\n\\(m(\\Omega)=m\\big([0,2\\pi)\\big)=2\\pi\\)\n\\(m(\\Omega^*) = m\\big([0,\\pi)\\big)= \\pi\\)\n\n- 위와 같은 방식으로 확률을 정의하면 잘 정의될까? 이게 쉽지 않음. 왜냐하면 확률을 잘 정의하기 위해서는\n\n\\(\\Omega\\)의 모든 부분집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이\n\n정의할 수 있어야 하는데, 이게 쉬운일이 아님.\n(질문0) 그냥 몸풀기 용 질문\n\n\\(\\Omega^*=\\emptyset\\) 일 확률이 얼마인가?\n\n(답변)\n\n0 이야2\n\n2 이걸 좀 더 엄밀하게 따질수도 있는데 일단 직관적으로 0이라 생각하고 넘어가자(질문1) 첫번째 도전적인 질문\n\n\\(\\Omega^* =\\{0\\}\\)일 확률이 얼마인가?\n\n(답변)\n\n즉 바늘침이 정확하게 12시를 가르킬 확률이 얼마냐는 것\n한 점으로 이루어진 집합 \\(\\{0\\}\\)은 분명히 \\(\\Omega=[0,2\\pi)\\)의 부분집합 이므로 앞서 논의한대로라면 이러한 집합에 대한 확률을 명확하게, 모순없이 정의할 수 있어야 함\n많은 사람들이 이 질문에 대한 답은 \\(0\\) 이라고 알고 있고 그 이유를 “점의 길이는 0 이니까” 라고 이해하고 있음.3\n\n3 이해 안되면 약속이라고 생각하자.4 자연어에서는 “확률=0” 와 “불가능” 은 동일하지만 여기서는 아니다.답변이 사실 좀 찝찝해. 바늘침이 정확하게 12시를 가르키는 것은 우리가 분명 하루에 한번씩은 경험하는 사건임. 그런데 그 사건이 일어날 확률은 0이다?4\n(참견질문) 생각해보니까 이런게 있었잖아?\n\\[A \\subset B \\Rightarrow P(A)&lt;P(B)\\]\n그런데 \\(\\emptyset \\subset \\{0\\}\\) 인데 \\(P(\\emptyset)=P(\\{0\\})\\) 이다..?\n(답변)\n\n원래식은 이거임: \\(A \\subset B \\Rightarrow P(A)\\leq P(B)\\).\n즉 \\(A\\)가 \\(B\\)의 진 부분집합이더라도 \\(P(A)=P(B)\\)인 경우가 존재함.\n\n(질문2) 두번째 질문은 아래와 같다.\n\n그렇다면 사건 \\(\\{0,\\pi\\}\\)가 일어날 확률은 얼마인가?\n\n(답변)\n\n질문을 다시 풀어쓰면 바늘침이 정확하게 12시를 가르키거나 혹은 정확하게 6시를 가르킬 확률이 얼마냐는 것\n따라서 이 질문에 대한 대답은 \\(0+0=0\\) 이므로 \\(0\\)이라고 주장할 수 있음.\n\n(질문3) 세번째 질문은 아래와 같다.\n\n구간 \\([0,2\\pi)\\)는 무수히 많은 점들이 모여서 만들어지는 집합이다. 그런데 점 하나의 길이는 0이다. 0을 무수히 더해도 0이다. 그러므로 구간 \\([0,2\\pi)\\)의 길이도 0이 되어야 한다. 이것은 모순아닌가?\n\n(답변)\n\n까다롭다.\n\\(m([0,2\\pi))=0\\) 임을 인정하면 전체확률은 1이어야 한다는 기본상식5에 어긋나 모순이 생김.\n질문의 논리는 타당해보임. 이 논리의 약점은 딱히 없어보임. 굳이 약점이 있다면 “무한”이라는 개념?\n어쩔수없이 직관에 근거한 약간의 약속을 또 다시 해야할 것 같음. 예를들면 “점들을 유한번 합치면 그냥 많은 점들이지만 무한히 합치면 이것은 선분이 된다. 따라서 길이가 생긴다.” 와 같이.\n우리는 이 약속을 “무한번의 기적”이라고 칭하자.\n\n5 심지어 이건 확률의 공리(질문4) 그렇다면 아래의 질문은 어떻게 대답할 수 있을까?\n\n\\([0,\\pi)\\) 에서 유리수만 뽑아낸 집합이 있다고 생각하자. 편의상 이 집합을 \\(\\mathbb{Q}\\) 라고 하자. 이 집합은 분명히 무한개의 점을 포함하고 있다. 그렇다면 이 집합도 길이가 있는가? 있다면 얼마인가?\n\n(답변)\n\n이미 점들의 길이를 무한번 더하면 길이가 생긴다고 주장한 상태이므로 (무한번의 기적) 길이가 0이라고 주장할 수 없다. 따라서 길이가 있다고 주장해야 한다.\n\\(\\pi\\)말고 딱히 떠오르는 수가 없는데 단순히 길이가 \\(\\pi\\)라고 주장한다면 바로 모순에 빠짐을 알 수 있다.6\n길이는 일단 0보다 커야하고 \\(\\pi\\)보다 작아야함은 자명하므로 그 사이에 있는 어떤 값이 길이라고 주장하자.7\n따라서 (질문4)에 대한 답은 ‘’구체적으로 얼마인지는 모르겠지만 길이가 분명 존재하고 그 길이는 0 보다 크고 \\(\\pi\\) 보다는 작은 어떠한 값 \\(a\\)이다.’’ 정도로 정리할 수 있다.\n즉 \\(m(\\mathbb{Q})=a\\), where \\(0&lt;a&lt;\\pi\\).\n\n6 왜 모순에 빠지냐면 \\([0,\\pi)\\)에서 무리수만 뽑아낸 집합의 길이가 뭐냐고 물을경우 0이라고 말해야함7 구체적으로 어떤값인지는 모른다고 하자.(질문5) – 외통수\n질문4로부터 만들어지는 논리는 빌드업1-3으로 이어지는 콤보질문을 적절하게 대답하지 못한다. (질문이 좀 길어서 나누어서 설명합니다)\n(빌드업1) – 평행이동은 길이를 변화시키지 않아, 그렇지?\n\n\\(\\mathbb{Q}\\)의 모든점에 \\(\\sqrt{2}\\)를 더한다. 이 점들로 집합을 만들어 \\(\\mathbb{Q}_{\\sqrt{2}}\\)를 만든다.\n여기에서 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}}\\)의 길이는 사실 쉽게 \\(a\\)라고 정의할 수 있음8. 즉, \\(m(\\mathbb{Q}_{\\sqrt{2}})=a\\).\n\n8 평행이동은 길이를 변화시킬 수 없으니까(빌드업2) – 겹치지 않게 평행이동 시킨다음에 길이를 더한다면?\n이제 \\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)를 생각하자. 아래의 성질을 관찰할 수 있다.\n\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) 따라서 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)의 길이는 각각 \\(a\\)로 정의할 수 있다.9\n\\(P(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3})=P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)10\n\n9 평행이동은 길이를 변화시키지 않으니까10 \\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 서로소 임을 이용굳이 \\(P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)를 계산하면 아래와 같이 계산할 수 있겠다.\n\\[P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})=\\frac{a}{2\\pi}+\\frac{a}{2\\pi}+\\frac{a}{2\\pi}=3 \\times \\frac{a}{2\\pi}\\]\n(빌드업3) – 그런데 난 겹치지않게 평행이동시킬 방법을 무한대로 알고 있는데?\n눈 여겨볼 점은 아래 식이 성립해야 한다는 것이다. (\\(\\because\\) 확률의 공리)11\n11 첫 등호는 서로소인 사건에 대한 공리, 그다음 부등호는 확률의 총합은 1보다 같거나 작다라는 공리\\[P\\big(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3}\\big) = 3 \\times \\frac{a}{2\\pi} \\leq 1 \\quad \\cdots (\\star)\\]\n\n그런데 \\((\\star)\\)에서 좌변의 값은 편의에 따라서 값을 임의로 키울 수 있다.\n이렇게 임의로 키워진 좌변의 값이라도 항상 그 값은 1보다 작아야 하는데 (확률의 공리), 이게 가능하려면 \\(a=0\\)인 경우 말고 없다.\n그런데 \\(a=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n\n그런데 임의로 좌변의 값을 키워도 항상 그 값은 1보다 작아야 하는데 이러한 \\(a\\)는 0이외에 불가능하다.\n그런데 \\(a=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n\n\n르벡메져\n- 예제2에서의 마지막 질문은 지금까지 제시한 논리로 방어가 불가능. 이처럼 논리적 모순없는 체계를 만드는 것은 매우 어려운 일임.\n- 결론적으로 말하면 길이를 재는 함수 \\(m\\)을 아래와 가정하면 위의 모든 질문에 대한 대답을 논리적 모순없이 설계할 수 있다.\n\n한 점에 대한 길이는 \\(0\\) 이다.\n\\([0,2\\pi)\\) 사이의 모든 유리수를 더한 집합은 그 길이가 \\(0\\)이다.\n\\([0,2\\pi)\\) 사이의 모든 무리수를 더한 집합은 그 길이가 \\(2\\pi\\)이다.\n\n참고로 르벡측도(Lebesgue measure)를 사용하면 위의 성질을 만족한다.12 따라서 르벡측도를 활용하여 확률을 정의하는 것이 모순을 최대한 피할 수 있다.\n\n\n12 물론 르벡측도의 정의가 위와 같지는 않다"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-16-3wk-1.html",
    "href": "posts/1. 측도론/2023-03-16-3wk-1.html",
    "title": "03wk-1: 측도론 intro (3)",
    "section": "",
    "text": "https://youtube.com/playlist?list=PLQqh36zP38-y_-OXU_IFt6uH3oo61swW4"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-16-3wk-1.html#전사-단사-전단사",
    "href": "posts/1. 측도론/2023-03-16-3wk-1.html#전사-단사-전단사",
    "title": "03wk-1: 측도론 intro (3)",
    "section": "전사, 단사, 전단사",
    "text": "전사, 단사, 전단사\n함수 \\(f: X \\to Y\\) 를 상상하자.\n- 단사함수(일대일함수,인젝티브한 함수): \\(\\forall x_1,x_2 \\in X: ~ x_1 \\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n\n암기 (고등학교): 입력이 다르면 출력이 달라\n느낌: 화살표가 팍 퍼지는 느낌\n그래프를 이용한 판단 (고등학교): 수평선을 그어서 교점이 2개 이상이면 단사함수가 아님\n\n- 전사함수(위로의함수,서젝티브한 함수): \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\)\n\n암기 (고등학교): 치역 = 공역\n암기 (대학교): inverse image가 정의역에 있어야함 (\\(\\star\\))\n느낌: 화살표가 모이는 느낌\n그래프를 이용한 판단 (고등학교): 모양으로 판단하기 애매함..1\n\n1 \\(y=x^2\\)은 공역을 \\(\\mathbb{R}\\)로 설정한다면 전사함수가 아니지만 공역을 \\(\\mathbb{R}_{\\geq 0}\\)로 설정한다면 전사함수임- 전단사함수(일대일대응함수,바이젝티브한 함수)"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-16-3wk-1.html#예제-finite-cases",
    "href": "posts/1. 측도론/2023-03-16-3wk-1.html#예제-finite-cases",
    "title": "03wk-1: 측도론 intro (3)",
    "section": "예제 (finite cases)",
    "text": "예제 (finite cases)\n\n예시1\n\n\n\n그림1: 단사함수 O, 전사함수 X\n\n\n- 단사함수임을 따져보자!\n\\(\\forall x_1,x_2 \\in X: x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(f(x_1)\\)\n\\(f(x_2)\\)\n\n\n\n\n1\n2\nD\nB\n\n\n1\n3\nD\nA\n\n\n2\n1\nB\nD\n\n\n2\n3\nB\nA\n\n\n3\n1\nA\nD\n\n\n3\n2\nA\nB\n\n\n\n- 전사함수임을 따져보자!\n\\(\\forall y \\in Y ~ \\exists x \\in X\\) such that \\(f(x)=y\\)\n\n\n\n\\(y\\)\n\\(x\\) such that \\(f(x)=y\\)\n\n\n\n\nD\n1\n\n\nB\n2\n\n\nC\n?\n\n\nA\n3\n\n\n\n\n\n예시2\n\n\n\n그림2: 단사함수 X, 전사함수 O\n\n\n- 단사함수임을 따져보자!\n\\(\\forall x_1,x_2 \\in X: x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(f(x_1)\\)\n\\(f(x_2)\\)\n\n\n\n\n1\n2\nD\nB\n\n\n1\n3\nD\nC\n\n\n1\n4\nD\nC\n\n\n2\n1\nB\nD\n\n\n2\n3\nB\nC\n\n\n2\n4\nB\nC\n\n\n3\n1\nC\nD\n\n\n3\n2\nC\nB\n\n\n3\n4\nC\nC\n\n\n4\n1\nC\nD\n\n\n4\n2\nC\nB\n\n\n4\n3\nC\nC\n\n\n\n- 전사함수임을 따져보자!\n\\(\\forall y \\in Y ~ \\exists x \\in X\\) such that \\(f(x)=y\\)\n\n\n\n\\(y\\)\n\\(x\\) such that \\(f(x)=y\\)\n\n\n\n\nD\n1\n\n\nB\n2\n\n\nC\n3,4\n\n\n\n\n\n예시3\n\n\n\n그림3: 단사함수 X, 전사함수 X\n\n\n- 단사함수임을 따져보자!\n\\(\\forall x_1,x_2 \\in X: x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(f(x_1)\\)\n\\(f(x_2)\\)\n\n\n\n\n1\n2\nd\nd\n\n\n1\n3\nd\nc\n\n\n2\n1\nd\nd\n\n\n2\n3\nd\nc\n\n\n3\n1\nc\nd\n\n\n3\n2\nc\nd\n\n\n\n- 전사함수임을 따져보자!\n\\(\\forall y \\in Y ~ \\exists x \\in X\\) such that \\(f(x)=y\\)\n\n\n\n\\(y\\)\n\\(x\\) such that \\(f(x)=y\\)\n\n\n\n\na\n?\n\n\nd\n1,2\n\n\nb\n?\n\n\nc\n3"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-16-3wk-1.html#예제-infinite-cases",
    "href": "posts/1. 측도론/2023-03-16-3wk-1.html#예제-infinite-cases",
    "title": "03wk-1: 측도론 intro (3)",
    "section": "예제 (infinite cases)",
    "text": "예제 (infinite cases)\n\n예시1\n- 아래를 판단해보자.\n\n\\(f:\\mathbb{R} \\to \\mathbb{R}\\) defined by \\(f(x)=2x+1\\). // 답2\n\\(f:\\mathbb{R} \\to \\mathbb{R}\\) defined by \\(f(x)=x^2\\). // 답3\n\\(f:\\mathbb{R} \\to \\mathbb{R}_{\\geq 0}\\) defined by \\(f(x)=x^2\\). // 답4\n\\(f:\\mathbb{Z} \\to \\{0,1\\}\\) defined by \\(f(x)= x ~\\text{mod}~ 2\\). // 답5\n\\(f:\\mathbb{N} \\to \\mathbb{N} \\cup \\{0\\}\\) defined by \\(f(x)= x-1\\). // 답6\n\\(f:\\mathbb{N} \\to \\mathbb{N}^-\\) defined by \\(f(k)= -k\\). // 답7\n\n여기에서 \\(\\mathbb{N}^-\\{-1,-2,\\dots,\\}\\) 으로 정의\n\n\n2 단사 O, 전사 O3 단사 X, 전사 X4 단사 X, 전사 O5 단사 X, 전사 O6 단사 O, 전사 O7 단사 O, 전사 O\n\n예시2\n- 집합 \\(X\\)가 집합 \\(Y\\)의 부분집합이라면 항상 \\(X\\)에서 \\(Y\\)로 향하는 단사함수가 존재함을 보여라.\n\n따라서 \\(X \\subset Y\\) \\(\\Rightarrow\\) \\(|X|\\leq |Y|\\)"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-14-2wk-2.html",
    "href": "posts/1. 측도론/2023-03-14-2wk-2.html",
    "title": "02wk-2: 측도론 intro (2)",
    "section": "",
    "text": "강의영상\n\nhttps://youtube.com/playlist?list=PLQqh36zP38-zQoiFje77DtmGx03QS339J\n\n\n\n예비개념1: 귀류법\n- 귀류법: 니 논리 대로면… &lt;- 인터넷 댓글에 많음..\n님 논리대로면..\n- XXX가 문제 없으면 서울 전체가 문제가 없고 (애초에 서울은 문제도 아니라는데 왜 이소리는 하고 계신지 모르겠지만)\n- 수도권 모 대학이 문제가 없으면 전체가 문제가 없겠네요?\n- 지방도 1개 대학이 문제가 없으니 전체가 문제 없겠네요?\n와우! 모든 문제가 해결되었습니다! 출산율 감소로 인한 한국대학의 위기가 해결되었.. 아니 애초에 위기가 없었군요!.\n어휴.. ㅠㅠ\n\nref: 하이브레인넷\n\n\n\n예비개념2: 일반화\n- 연필의 정의: 필기도구의 하나. 흑연과 점토의 혼합물을 구워 만든 가느다란 심을 속에 넣고, 겉은 나무로 둘러싸서 만든다. 1565년에 영국에서 처음으로 만들었다.\n- 질문: 아래는 연필인가?\n\n\n\n애플펜슬!\n\n\n\n\ncardinality\n\nref: https://en.wikipedia.org/wiki/Cardinality\n\n- \\(A=\\{2,4,6\\}\\) \\(\\Rightarrow\\) \\(|A|=3\\), \\(A\\) has a cardinality of 3.\n- \\(A=\\{1,2,3,4,\\dots\\}=\\mathbb{N}\\) \\(\\Rightarrow\\) \\(|A|=?\\)\n\nCardinal number: 유한집합에서의 “갯수”라는 개념을 좀 더 일반화 하여 무한집합으로 적용하고 싶다.\n유한집합: 우리가 친숙한 size 와 그 뜻이 같음\n무한집합: 무한집합의 경우는 그 동작원리가 조금 더 복잡함\n\n- 질문: \\(|\\mathbb{Q}| &lt; |\\mathbb{Q}^c|\\) ??\nBijection, injection and surjection (예비학습)\n\nref: https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection\n\n\n- 용어 정리\n\nsurjective = onto = 전사 = 위로의 함수\ninjective = one-to-one = 단사 = 일대일 함수\nbijective = one-to-one and onto, one-to-one correspondence = 전단사 = 일대일 대응\n\n- 따지는 방법:\n\n단사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 단사함수이다. \\(\\Leftrightarrow\\) \\(\\forall x_1,x_2 \\in X\\): \\(x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n전사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전사함수이다. \\(\\Leftrightarrow\\) \\(\\forall y \\in Y ~\\exists x \\in X\\) such that \\(f(x)=y\\).\n\n- 성질1: 어떤함수가 전사함수 & 단사함수 \\(\\Rightarrow\\) 전단사함수\n- 성질2:\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 단사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\leq |Y|\\)\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\geq |Y|\\)\n\n(예비학습 끝)\n- 성질1~2로 유추하면 아래와 같은 사실을 주장 할 수 있지 않을까?\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 향하는 전단사함수가 존재한다 \\(\\Rightarrow\\) \\(|X|=|Y|\\)\n\n- 그렇다면 우리가 주장하고 싶은 것은 아래와 같이 된다.\n\n유리수집합의 무리수집합의 cardinality는 다르다.\n유리수집합과 무리수집합사이의 전단사함수는 존재할 수 없다.\n\n\n\n유리수집합의 카디널리티\n- 우리가 궁극적으로 궁금한 것\n\n유리수집합과 무리수집합의 카디널리티는 다를까?\n\n- 그냥 궁금한 것\n\n자연수의 집합, 비음인 정수의 집합, 음의 정수의 집합, 정수의 집합, 짝수의 집합, 홀수의 집합의 카디널리티는 어떠할까?\n\n- (예제1)\n집합 \\(X=\\{1,2,3\\}\\), \\(Y=\\{2,4,6\\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\n아래의 질문에 대답해보자.\n\n(단사) \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\)?\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사 함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다.\n- (예제2)\n집합 \\(X=\\{1,2,3,\\dots \\}\\), \\(Y=\\{2,4,6,\\dots \\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\\(\\dots\\)\n\n아래의 질문에 대답해보자.\n\n(단사) \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\)?\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다.\n- \\(\\aleph_0\\) (알레프 널, 혹은 알레프 제로라고 읽음)\n\n자연수집합 \\(\\mathbb{N}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{N}|=\\aleph_0\\).\n짝수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이고, 홀수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이다.\n정수집합 \\(\\mathbb{Z}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Z}|=\\aleph_0\\).\n\n- 느낌: \\(\\aleph_0\\)를 2배,3배,4배 하여도 \\(\\aleph_0\\)이다.\n\n즉 무한집합의 경우, 본인과 카디널넘버가 같은 진 부분집합이 존재할 수 있다. (유한집합에서는 불가능하겠지)\n무한집합의 정의: 집합 \\(A\\)가 무한집합이다. \\(\\Leftrightarrow\\) \\(A\\)와 동일한 카디널리티를 가지는 \\(A\\)의 진 부분집합이 존재한다.\n\n- (예제3)\n원소의 수가 \\(n\\)인 임의의 유한집합 \\(A\\)에 대하여 \\(|A|=n\\) 이다.\n- (예제4)\n유리수집합의 카디널리티는 얼마인가? (ref: https://en.wikipedia.org/wiki/Rational_number)\n집합 \\(X\\)를 자연수의 집합이라고 하자. 집합 \\(Y\\)를 아래그림에 있는 숫자들의 집합이라고 하자.1\n1 그래서 일단 집합 \\(Y\\)는 양의 유리수의 집합을 포함한다\n예를들어 집합 \\(X\\)와 집합 \\(Y\\)를 앞의 몇개만 써보면\n\n\\(X=\\{1,2,3,4,5,6,\\dots\\}\\)\n\\(Y=\\{1,\\frac{2}{1},\\frac{1}{2},\\frac{3}{1},\\frac{2}{2},\\frac{1}{3},\\dots \\}\\)\n\n함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=1\\)\n\\(f(2)=2/1\\)\n\\(f(3)=1/2\\)\n\\(f(4)=3/1\\)\n\\(f(5)=2/2\\)\n\\(f(6)=1/3\\)\n\\(\\dots\\)\n\n함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전단사함수이다. \\(\\Rightarrow\\) \\(|X|=\\aleph_0=|Y|\\)\n(관찰) 임의의 양의 유리수의 집합 \\(\\mathbb{Q}^+\\)는 모두 \\(Y\\)에 포함되어 있다. \\(\\Rightarrow\\) \\(X \\subset \\mathbb{Q}^+ \\subset Y\\) \\(\\Rightarrow\\) \\(|\\mathbb{Q}^+|=\\aleph_0\\)\n(생각) 그럼 음의 유리수의 집합 \\(\\mathbb{Q}^-\\)의 카디널넘버 역시 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Q}^-|=\\aleph_0\\).\n(결론) 그럼 유리수의 카디널넘버는 \\(\\aleph_0\\)이다.2 좀 더 자극적으로 말하면 “자연수의 갯수와 유리수의 갯수는 같다” 라고 말할 수 있다.\n2 \\(\\mathbb{Q} = \\mathbb{Q}^+ \\cup \\{0\\} \\cup \\mathbb{Q}^-\\)- 조금 무식하게 쓰면 아래와 같이 쓸 수 있다.\n\n\\(\\aleph_0 + 1 = \\aleph_0\\)\n\\(\\aleph_0 \\times 2 = \\aleph_0\\)\n\\(\\aleph_0 \\times \\aleph_0 = \\aleph_0^2 = \\aleph_0\\)"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-25-8wk-2.html",
    "href": "posts/2. 마코프체인/2023-04-25-8wk-2.html",
    "title": "08wk-2: 마코프체인 (5)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zHbA2xrF58wfGjzkhNzxnL"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-25-8wk-2.html#motivating-examples",
    "href": "posts/2. 마코프체인/2023-04-25-8wk-2.html#motivating-examples",
    "title": "08wk-2: 마코프체인 (5)",
    "section": "Motivating Examples",
    "text": "Motivating Examples\n\n예제1\n- 아래의 전이확률을 고려하자.\n\nP =np.array([0.5, 0.5, 0.0, 0.0, \n             0.5, 0.5, 0.0, 0.0,\n             0.0, 0.0, 0.5, 0.5,\n             0.0, 0.0, 0.5, 0.5]).reshape(4,4)\nP\n\narray([[0.5, 0.5, 0. , 0. ],\n       [0.5, 0.5, 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n- 특징1: \\({\\bf P}\\)는 수렴함\n\nP@P@P\n\narray([[0.5, 0.5, 0. , 0. ],\n       [0.5, 0.5, 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n- 특징2: 모든 row가 같은건 아님\n- 특징3: 정상분포는 유일하게 존재하지 않음\n\nπ = np.array([1/4, 1/4, 1/4, 1/4]).reshape(4,1)\nπ\n\narray([[0.25],\n       [0.25],\n       [0.25],\n       [0.25]])\n\n\n\nπ.T @ P, π.T \n\n(array([[0.25, 0.25, 0.25, 0.25]]), array([[0.25, 0.25, 0.25, 0.25]]))\n\n\n\nπ = np.array([1/2, 1/2, 0, 0]).reshape(4,1)\nπ\n\narray([[0.5],\n       [0.5],\n       [0. ],\n       [0. ]])\n\n\n\nπ.T @ P, π.T \n\n(array([[0.5, 0.5, 0. , 0. ]]), array([[0.5, 0.5, 0. , 0. ]]))\n\n\n\nπ = np.array([1/6, 1/6, 2/6, 2/6]).reshape(4,1)\nπ\n\narray([[0.16666667],\n       [0.16666667],\n       [0.33333333],\n       [0.33333333]])\n\n\n\nπ.T @ P, π.T \n\n(array([[0.16666667, 0.16666667, 0.33333333, 0.33333333]]),\n array([[0.16666667, 0.16666667, 0.33333333, 0.33333333]]))\n\n\n- 특징4: 초기분포가 정상분포라면 정상확률과정\n- 특징5: 상태공간 \\(E\\) 에 equivalence class 가 2개 있는 느낌\n\n\n예제2\n- 아래의 전이확률을 고려하자.\n\nP =np.array([1/4, 1/4, 0.0, 1/2, \n             1/4, 1/4, 0.0, 1/2,\n             0.0, 0.0, 1.0, 0.0,\n             1/2, 1/4, 0.0, 1/4]).reshape(4,4)\nP\n\narray([[0.25, 0.25, 0.  , 0.5 ],\n       [0.25, 0.25, 0.  , 0.5 ],\n       [0.  , 0.  , 1.  , 0.  ],\n       [0.5 , 0.25, 0.  , 0.25]])\n\n\n- 특징1: \\({\\bf P}\\)는 수렴함\n\nnp.matrix(P)**500\n\nmatrix([[0.35, 0.25, 0.  , 0.4 ],\n        [0.35, 0.25, 0.  , 0.4 ],\n        [0.  , 0.  , 1.  , 0.  ],\n        [0.35, 0.25, 0.  , 0.4 ]])\n\n\n- 특징2: 모든 row가 같지는 않음\n- 특징3: 유일한 정상분포를 가지는건 아님\n\nc1 = 0.2 # 상태 0,1,3 \nc2 = 0.8 # 상태 2 \nπ = np.array([0.35*c1, 0.25*c1, 1.0*c2 ,0.4*c1]).reshape(4,1)\nπ\n\narray([[0.07],\n       [0.05],\n       [0.8 ],\n       [0.08]])\n\n\n\nπ.T @ P, π.T \n\n(array([[0.07, 0.05, 0.8 , 0.08]]), array([[0.07, 0.05, 0.8 , 0.08]]))\n\n\n- 특징4: 초기분포가 정상분포라면 정상확률과정\n- 특징5: 상태공간 \\(E\\)에 equivalence class 가 2개 있는 느낌"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-25-8wk-2.html#정의-및-이론",
    "href": "posts/2. 마코프체인/2023-04-25-8wk-2.html#정의-및-이론",
    "title": "08wk-2: 마코프체인 (5)",
    "section": "정의 및 이론",
    "text": "정의 및 이론\n- 용어\n\nirreducible (기약) // reducible (비기약)\n(strongly) connected\n\n- 정의\n- 느낌\n\n연결되어있는 느낌. 즉 모든 \\(x,y \\in E\\)에 대하여 \\(x\\to \\cdots \\to y\\) 인 path 나 \\(y \\to \\cdots \\to x\\) 인 path 가 존재함\n겉도는 그룹이 없음 (상태공간 \\(E\\)에 단 하나의 equivalence class가 존재함)\n\n- Thm: HMC \\(\\{X_t\\}\\) 가 (1) finite state space 를 가지고 (2) irreducible 이라면 \\(\\{X_t\\}\\)의 유일한 정상분포 \\({\\boldsymbol \\pi}\\)가 존재하며 모든 state에 대한 확률은 양수이다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-25-8wk-2.html#motivating-examples-1",
    "href": "posts/2. 마코프체인/2023-04-25-8wk-2.html#motivating-examples-1",
    "title": "08wk-2: 마코프체인 (5)",
    "section": "Motivating Examples",
    "text": "Motivating Examples\n\n예제1\n- 아래와 같은 전이확률을 고려하자.\n\nP = np.array([0.0, 1.0, 0.0,\n              0.0, 0.0, 1.0,\n              1.0, 0.0, 0.0]).reshape(3,3)\nP\n\narray([[0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.]])\n\n\n- 다이어그램\n\n\n\n\nflowchart LR\n  0 --&gt;|1| 1\n  1 --&gt;|1| 2\n  2 --&gt;|1| 0\n\n\n\n\n\n- 특징1: \\({\\bf P}\\)는 수렴안함\n\nP@P@P\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n- 특징2:\n- 특징3: 정상분포는 유일하게 존재함.\n\nπ = np.array([1/3,1/3,1/3]).reshape(3,1)\nπ\n\narray([[0.33333333],\n       [0.33333333],\n       [0.33333333]])\n\n\n\nπ.T @ P, π.T\n\n(array([[0.33333333, 0.33333333, 0.33333333]]),\n array([[0.33333333, 0.33333333, 0.33333333]]))\n\n\n- 특징4: 초기분포가 정상분포라면 정상확률과정\n- 특징5: 상태공간 \\(E\\)에 equivalence class 가 1개\n- 특징6: 주기성을 가짐 (주기는 3)\n\n관찰: 어떠한 상태 \\(x \\in E\\) 에 있더라도 반드시 3번 안에는 원래 상태로 되돌아옴.\n\n\n\n예제2\n- 아래와 같은 전이확률을 고려하자.\n\nP = np.array([0.0, 1.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 1.0,\n              0.0, 1.0, 0.0, 0.0,\n              1/3, 0.0, 2/3, 0.0]).reshape(4,4)\nP\n\narray([[0.        , 1.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        ],\n       [0.        , 1.        , 0.        , 0.        ],\n       [0.33333333, 0.        , 0.66666667, 0.        ]])\n\n\n- 다이어그램\n\n\n\n\nflowchart LR\n  0 --&gt;|1| 1\n  1 --&gt;|1| 3\n  2 --&gt;|1| 1\n  3 --&gt;|1/3| 0 \n  3 --&gt;|2/3| 2\n\n\n\n\n\n- 특징1: \\({\\bf P}\\)는 수렴안함\n\nP@P@P@P\n\narray([[0.        , 1.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        ],\n       [0.        , 1.        , 0.        , 0.        ],\n       [0.33333333, 0.        , 0.66666667, 0.        ]])\n\n\n- 특징2: Pass\n- 특징3: 정상분포는 유일하게 존재함.\n\nπ = (np.array([1,3,2,3])/9).reshape(4,1)\nπ\n\narray([[0.11111111],\n       [0.33333333],\n       [0.22222222],\n       [0.33333333]])\n\n\n\nπ.T @ P, π.T \n\n(array([[0.11111111, 0.33333333, 0.22222222, 0.33333333]]),\n array([[0.11111111, 0.33333333, 0.22222222, 0.33333333]]))\n\n\n어떻게 찾음?\n\neig_value, eig_vector_matrix = np.linalg.eig(P.T)\n\n\neig_value[2]\n\n(1.000000000000001+0j)\n\n\n\nπ = abs(eig_vector_matrix[:,2])\nπ = π/π.sum()\nπ\n\narray([0.11111111, 0.33333333, 0.22222222, 0.33333333])\n\n\n- 특징4: 초기분포가 정상분포라면 정상확률과정\n- 특징5: irr\n- 특징6: 주기성을 가짐 (주기는3)\n\n\n\n\nflowchart LR\n  0 --&gt;|1| 1\n  1 --&gt;|1| 3\n  2 --&gt;|1| 1\n  3 --&gt;|1/3| 0 \n  3 --&gt;|2/3| 2\n\n\n\n\n\n0에서 시작한다면?\n\n\\(0 \\to 1 \\to 3 \\to 0\\)\n\\(0 \\to 1 \\to 3 \\to 2 \\to 1 \\to 3 \\to 0\\)\n\\(0 \\to 1 \\to 3 \\to 2 \\to 1 \\to 3 \\to 2 \\to \\cdots\\)\n\n\n3번만에 되돌아오거나, 6번만에 되돌아오거나, 9번만에 되돌아오거나 … \\(\\Rightarrow\\) 주기는 3 (3,6,9의 최대공약수는 3)\n\n1에서 시작한다면?\n\n\\(1 \\to 3 \\to 0 \\to 1\\)\n\\(1 \\to 3 \\to 2 \\to 1 \\to 3 \\to 0 \\to 1\\)\n\\(\\dots\\)\n\n2에서 시작한다면?\n3에서 시작한다면?\n\n꿀팁: HMC \\(\\{X_t\\}\\)가 irreducible 이라면 모든 \\(x \\in E\\) 는 같은 주기를 가진다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-25-13wk-1.html",
    "href": "posts/2. 마코프체인/2023-05-25-13wk-1.html",
    "title": "13wk-1: 마코프체인 (12)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-x4iyJGrSEk1pswE7dsNoke"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-25-13wk-1.html#지난시간",
    "href": "posts/2. 마코프체인/2023-05-25-13wk-1.html#지난시간",
    "title": "13wk-1: 마코프체인 (12)",
    "section": "지난시간",
    "text": "지난시간\n- 정의 \\(\\{X_t\\}\\)가 HMC라고 하자. 아래의 식을 만족하는\n\\[\\tilde{\\boldsymbol \\pi}^\\top {\\bf P} = \\tilde{\\boldsymbol \\pi}^\\top\\]\n\\(\\tilde{\\boldsymbol \\pi}^\\top\\) 를 invariant measure 라고 한다. 만약에 \\(\\tilde{\\boldsymbol \\pi}^\\top\\) 이 분포의 정의를 만족하면 stationary measure 혹은 stationary distribution 이라고 부른다.\n- 예시: “오른쪽으로만 갈래” 예제에서는\n\\[\\tilde{\\boldsymbol \\pi}^\\top = [1,1,1,\\dots]\\]\n이 수식\n\\[\\tilde{\\boldsymbol \\pi}^\\top {\\bf P} = \\tilde{\\boldsymbol \\pi}^\\top\\]\n을 만족한다. 따라서 이 예제에서 \\(\\tilde{\\boldsymbol \\pi}^\\top = [1,1,1,\\dots]\\) 은 invariant measure 이다.\n- \\(\\{X_t\\}\\)가 HMC라고 하자. 각각에 대하여 아래가 성립한다.\n\n\n\n\n\n\n\n\n\n\nIRR\nnature\n\\(\\exists! \\tilde{\\boldsymbol \\pi}\\) up to multiplier\n\\(\\exists! {\\boldsymbol \\pi}\\)\n에르고딕정리(\\(\\approx\\)LLN)\n\n\n\n\n\\(O\\)\nPR\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\\(O\\)\nNR\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(O\\)\nTR\n\\(\\Delta\\)\n\\(X\\)\n\\(X\\)\n\n\n\n- 이론: \\(\\{X_t\\}\\)가 IRR-HMC1 라고 하자. \\(\\{X_t\\}\\)가 정상분포를 가진다는 조건과 유일한 정상분포를 가질 조건은 동치이다.\n1 irreducible 한 homogeneous markov chain\n즉 \\(\\{X_t\\}\\)가 IRR-HMC 일때, 정상분포가 존재한다는 사실만 보이면 자동으로 유일성이 보장된다.\n\n- Thm: \\(\\{X_t\\}\\)가 IRR-HMC 라고 하자. 그러면 positvite recurrent 와 \\(\\exists! {\\boldsymbol \\pi}\\) 은 동치조건이다. 즉\n\nIRR-HMC \\(\\{X_t\\}\\) 가 positive recurrent 하다면 항상 \\(\\{X_t\\}\\) 는 유일한 정상분포를 가진다.\nIRR-HMC \\(\\{X_t\\}\\) 가 정상분포를 가지면 (그 분포는 유일해지고) \\(\\{X_t\\}\\)는 항상 positive recurrent 하다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-25-13wk-1.html#이번시간",
    "href": "posts/2. 마코프체인/2023-05-25-13wk-1.html#이번시간",
    "title": "13wk-1: 마코프체인 (12)",
    "section": "이번시간",
    "text": "이번시간\n- (정의) – 복습 \\(\\{X_t\\}\\)가 HMC라고 하자. 모든 \\(i \\in E\\) 는 아래의 조건중 하나를 만족하는데\n\n\\(\\mathbb{P}_i(T_i &lt;\\infty)= 1\\) and \\(\\mathbb{E}_i[T_i]&lt;\\infty\\),\n\\(\\mathbb{P}_i(T_i &lt;\\infty)= 1\\) and \\(\\mathbb{E}_i[T_i]=\\infty\\),\n\\(\\mathbb{P}_i(T_i &lt;\\infty)= 0\\).\n\n이중에서 3의 경우는 상태 \\(i\\)가 transient 하다고 표현하며, 1,2의 경우는 각각 potivite recurrent, null recurrent 하다고 표현한다.\n- 이걸 갑자기 복습하는 이유? 결국 TR, NR 모두 그 상태에 머물확률이 궁극적으로는 0이라는 느낌을 위해서! TR일 경우는 따질 필요 없이 확실하고 NR일 경우는 아래 식을 이용하여 판단할 수 있다.\n\\[\\frac{1}{\\mathbb{E}(T_i)} \\approx \\frac{1}{T}\\sum_{t=1}^{T}\\#\\mathbb{1}(X_t=i)\\]\n\n\\(T=100\\) 일때 21번 상태 0에 있었음.\n평균적으로 \\(\\frac{21}{100}\\approx 1/5\\) 비율로 상태 0에 있는듯\n현재 상태 0에 머물러 있다면, 평균 5번정도내로는 돌아올 듯 (그렇지 않다면 2가 성립하지 않는걸?)\n\n- 직관: 어떠한 상태가 PR이 아닌 경우는 그 상태에 머물 확률이 0이므로 당연히 정상분포를 가지지 않음.\n- Thm (에르고딕 thm): IRR-HMC \\(\\{X_t\\}\\)가 PR 조건을 만족한다고 하자. 그러면 \\(\\sum_{i\\in E}|f(i)|\\pi_i&lt;\\infty\\)를 만족하는 함수 \\(f:E \\to \\mathbb{R}\\)에 대하여 아래가 성립한다.\n\\[\\lim_{T\\to\\infty} \\frac{1}{T}\\sum_{t=0}^{T-1}f(X_t) = \\mathbb{E}_{\\boldsymbol \\pi}[f(X_0)]\\]\n여기에서 \\(\\boldsymbol \\pi\\)는 \\({\\boldsymbol \\pi}^\\top = {\\boldsymbol \\pi}^\\top{\\bf P}\\)를 만족하는 유일한 정상분포이고 \\({\\bf P}\\)는 \\(\\{X_t\\}\\)의 transition matrix 이다.\n\nFINITE 한 경우와 비교1: FINITE 조건이 PR 조건으로 바뀐느낌.\n\n\nFINITE 한 경우와 비교2: \\(\\sum_{i\\in E}|f(i)|\\pi_i&lt;\\infty\\) 이라는 조건은 없었는데 생김\n\n- 이론: (에르고딕 thm, ver2) IRR-HMC \\(\\{X_t\\}\\)가 PR이면 아래가 성립한다는 의미이다.\n\\[\\bar{\\boldsymbol \\pi} \\to {\\boldsymbol \\pi}\\]\n(증명?)\n이 이론이 성립하는 이유는 원래의 에르고딕 이론에서 \\(f\\)를 잘 해석하면 된다.\nSOME NOTES\n\nIRR 조건은 까다롭지 않다. (없다면 그냥 가정할 수 있음)\nPR 조건이 있는 이유? NR 이거나 TR 이면 애초에 수렴할 정상분포가 없는걸?\n\n- 이론: (에르고딕 thm, ver3) IRR-HMC \\(\\{X_t\\}\\)가 FINITE 이면 아래가 성립한다.\n\\[\\bar{\\boldsymbol \\pi} \\to {\\boldsymbol \\pi}\\]\n(증명?)\nIRR-HMC가 FINITE 할 경우 PR이 임플라이 되므로 자동성립\n\n에르고딕 정리는 결국 LLN의 upgrade 버전이며 (조건은 약화되었는데 결론도 강해요) “시간평균 \\(\\approx\\) 앙상블평균” 을 의미한다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-16-11wk-2.html",
    "href": "posts/2. 마코프체인/2023-05-16-11wk-2.html",
    "title": "11wk-2: 마코프체인 (9)",
    "section": "",
    "text": "- 수업시간 중 잘못 설명한 부분이 있어서 정정하고 촬영하였습니다. (두번째 영상이 재촬영한 부분임)\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wygptXg6WEfudbDb-ZjvRm"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-16-11wk-2.html#intro",
    "href": "posts/2. 마코프체인/2023-05-16-11wk-2.html#intro",
    "title": "11wk-2: 마코프체인 (9)",
    "section": "intro",
    "text": "intro\n- 같다(=) 라는 개념의 추상화\n- 예시1: 아래의 리스트에서 같은 원소끼리 묶어라.\n\n[1,1,2,2,3,3,3]\n\n[1, 1, 2, 2, 3, 3, 3]\n\n\n\n[[1,1], [2,2], [3,3,3]]\n\n[[1, 1], [2, 2], [3, 3, 3]]\n\n\n\n같은 원소들의 모임을 동치류 (equivalence class) 라고 한다. 이 예제에서는 3개의 동치류가 있는 셈.\n\n- 예시2: 아래의 리스트에서 같은 원소끼리 묶어라.\n\nlst = [1, 1.0, 2, 2, 3, 3, 3]\nlst \n\n[1, 1.0, 2, 2, 3, 3, 3]\n\n\n\n어떻게 할까? (수학적으로 볼까? 프로그래밍적으로 볼까?)\n같다라는건 뭐지?"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-16-11wk-2.html#같다의-개념을-추상화",
    "href": "posts/2. 마코프체인/2023-05-16-11wk-2.html#같다의-개념을-추상화",
    "title": "11wk-2: 마코프체인 (9)",
    "section": "“같다”의 개념을 추상화",
    "text": "“같다”의 개념을 추상화\n- “같다”라는 개념을 좀 일반화 해보자.\n\n같다라는 것은 “어떠한 기준으로 판단하였을 경우” 그 결과가 같은 집합으로 묶인다는 것을 의미\n\n- 아래의 예시를 다시 관찰하자.\n\nlst = [1, 1.0, 2, 2, 3, 3, 3]\nlst \n\n[1, 1.0, 2, 2, 3, 3, 3]\n\n\n(경우1)\n판단기준을 “수학적인 값이 같음”으로 설정한다면 lst[0]과 lst[1]은 같다.\n\nlst[0] == lst[1]\n\nTrue\n\n\n따라서 아래와 같은 분류가 합리적이다.\n\n[[1, 1.0], [2,2], [3,3,3]]\n\n[[1, 1.0], [2, 2], [3, 3, 3]]\n\n\n(경우2)\n판단기준을 “수학적인 값이 같음 & 파이썬에서의 자료형이 일치” 로 설정한다면 lst[0]과 lst[1]은 다르다.\n\ntype(lst[0]) == type(lst[1])\n\nFalse\n\n\n따라서 아래와 같은 분류가 합리적이다.\n\n[[1], [1.0], [2,2], [3,3,3]]\n\n[[1], [1.0], [2, 2], [3, 3, 3]]\n\n\n(경우3)\n판단기준을 “파이썬에서의 자료형이 일치” 로 설정한다면? 아래와 같은 분류도 합리적이다.\n\n[[1,2,2,3,3,3], [1.0]]\n\n[[1, 2, 2, 3, 3, 3], [1.0]]\n\n\n이것도 어떠한 의미에서는 같은원소들을 모아놓은 것임\n\n“같다”라는 것을 올바르게 지칭하려면 “어떠한 의미에서 같다”라는 것인지 명확하게 설명할 필요가 있다.\n\n- 예시2: \\(a\\)와 \\(b\\)가 “어떠한 의미에서 같다”라는 것을 기호로 \\(a \\sim b\\)라고 하자. ~의 의미를\n\n\\(a \\sim b\\) \\(\\overset{def}{\\Leftrightarrow}\\) a == b\n\n로 해석한다면, 아래와 같이 원소를 묶을 수 있다.\n\n[[1, 1.0], [2,2], [3,3,3]]\n\n[[1, 1.0], [2, 2], [3, 3, 3]]\n\n\n만약에 ~의 의미를\n\n\\(a \\sim b\\) \\(\\overset{def}{\\Leftrightarrow}\\) (a == b) & (type(a)==type(b))\n\n로 해석한다면, 아래와 같이 원소를 묶을 수 있다.\n\n[[1], [1.0], [2,2], [3,3,3]]\n\n[[1], [1.0], [2, 2], [3, 3, 3]]\n\n\n만약에 ~의 의미를\n\n\\(a \\sim b\\) \\(\\overset{def}{\\Leftrightarrow}\\) (type(a)==type(b))\n\n로 해석한다면, 아래와 같이 원소를 묶을 수 있다.\n\n[[1,2,2,3,3,3], [1.0]]\n\n[[1, 2, 2, 3, 3, 3], [1.0]]\n\n\n- “같음(=)”이라는 기호가 가지는 당연한 성질\n\n\\(a=a\\)\n\\(a=b \\Rightarrow b=a\\)\n\\(a=b, b=c \\Rightarrow a=c\\)\n\n성질 1,2,3은 원래 =라는 기호가 “두 원소의 같음”을 의미할때 가지는 당연한 성질이다.\n- 역으로 생각해보면 어떠한 기호 \\(\\sim\\)이 성질 1,2,3을 가진다면 기호 \\(\\sim\\)를 같음을 의미하는 기호로 “해석”할 수 있다.\n\n예시1: 합동\n예시2: 닮음\n\n- 정의: 어떠한 집합 \\(\\Omega\\)의 임의의 원소 \\(a,b,c\\) 에 대하여 \\(\\sim\\)이 아래와 같은 성질이 성립한다면 \\(\\sim\\)를 equivalence relation 이라고 부른다.\n\n\\(a\\sim~a\\)\n\\(a \\sim b \\Rightarrow b\\sim a\\)\n\\(a \\sim b, b \\sim c \\Rightarrow a \\sim c\\)\n\n여기에서 \\(\\sim\\)은 “같음”을 의미하는 기호 \\(=\\)의 일반화된 버전이다.\n- 정의: 어떠한 집합 \\(\\Omega\\)가 equivalence relation \\(\\sim\\)를 가진다면 그 집합은 \\(\\sim\\)를 기준으로 나눌 수 있다.\n(예시1) 아래와 같이 5명의 학생이 있다고 치자.\n\n23학번: 20살, 20살, 20살\n22학번: 21살, 21살\n\n구성원들의 나이나 학번이 같으면 반말을 한다고 치자. (그렇지 않으면 존대말을 한다고 가정하자) 이제 아래와 같은 기호를 정의하자.\n\n\\(a \\sim b\\) \\(\\overset{def}{\\Leftrightarrow}\\) \\(a\\)가 \\(b\\)에게 반말함\n\n그렇다면 \\(\\sim\\) equivalence relation 이다. 따라서 학생들을 \\(\\sim\\)를 기준으로 두개의 그룹으로 나눌 수 있다.\n(예시2) 아래와 같이 5명의 학생이 있다고 치자.\n\n23학번: 20살, 20살, 21살\n20학번: 23살, 23살\n\n여전히 \\(\\sim\\)는 equivalence relation 이다. 따라서 학생들을 \\(\\sim\\)를 기준으로 두개의 그룹으로 나눌 수 있다.\n(예시3) 아래와 같이 5명의 학생이 있다고 치자.\n\n23학번: 20살, 20살, 21살\n22학번: 21살, 21살\n\n이제 \\(\\sim\\)는 equivalence relation 이 아니다. 따라서 학생들을 \\(\\sim\\)를 기준으로 나눌 수 없다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-16-11wk-2.html#intro-1",
    "href": "posts/2. 마코프체인/2023-05-16-11wk-2.html#intro-1",
    "title": "11wk-2: 마코프체인 (9)",
    "section": "intro",
    "text": "intro\n- 질문: “오른쪽으로만 갈래요” 예제는 IRR HMC 인가?\n- 가짜정의: 어떠한 HMC \\(\\{X_t\\}\\)가 IRR이라는 것은 모든 상태공간이 “연결”되어있다는 의미이다.\n- 가짜정의의 보충설명 (1)\n\n여기에서 모든 상태공간이 연결되어있다는 의미는 상태공간 \\(E\\)에서 임의의 두 상태 \\(i,j\\)를 뽑았을때 \\(i \\to j\\) 이고, \\(j \\to i\\) 라는 의미이다.\n여기에서 \\(i\\to j\\) 라는 의미는 언젠가는 상태 \\(i\\)에서 출발한 체인이 상태 \\(j\\)에 도달할 수 있다는 의미이다.\n\n- 의문: 언젠가는에 대한 의미??\n\n\n\n\nflowchart LR\n  0 --&gt; 1\n  1 --&gt; 2\n  2 --&gt; 1 \n  2 --&gt; 3 \n  3 --&gt; 1\n  3 --&gt; 2 \n\n\n\n\n\n상태0에서 시작하면 3회 이후에는 상태3에 갈 확률이 있다. (3회시점에 꼭 상태3에 있겠다는 의미는 아님) 따라서 이 경우\n\\[0 \\to 3\\]\n이라고 쓸 수 있다. 이 예제의 경우\n\n\\(0 \\to 1\\), \\(0 \\to 2\\), \\(0 \\to 3\\), \\(0 \\to 4\\), \\(0\\to 0\\)1\n\\(1 \\to 1\\), \\(1 \\to 2\\), \\(1 \\to 3\\)\n\\(2 \\to 1\\), \\(2 \\to 2\\), \\(2 \\to 3\\)\n\\(3 \\to 1\\), \\(3 \\to 2\\), \\(3 \\to 3\\)\n\n1 0회도 포함시키면 \\(0\\to 0\\) 라고 볼 수 있음.와 같다.\n\n여기서 제가 설명잘못했는데요, 0회도 포함시킨다고 하면 \\(0 \\to 0\\) 입니다.\n\n- 다시 가짜정의의 보충설명 (2) – (1)을 이어서\n\n여기에서 모든 상태공간이 연결되어있다는 의미는 상태공간 \\(E\\)에서 임의의 두 상태 \\(i,j\\)를 뽑았을때 \\(i \\to j\\) 이고, \\(j \\to i\\) 라는 의미이다.\n여기에서 \\(i\\to j\\) 라는 의미는 언젠가는 상태 \\(i\\)에서 출발한 체인이 상태 \\(j\\)에 도달할 수 있다는 의미이다.\n즉 \\(i \\to j\\)라는 의미는 “(\\(i\\)에서 출발한다면 \\(T_0\\) 이후에 \\(j\\)에 도달해 있을 확률) &gt; \\(0\\)” 이라는 뜻이다.\n\n- 다시의문: \\(i\\)에서 출발했다고 가정할때 \\(T_0\\)이후에 \\(j\\)에 도달해 있을 확률을 어떻게 구체적으로 쓰지?\n- (예시)\n\n\n\n\nflowchart LR\n  0 --&gt; |1.0| 1\n  1 --&gt; |0.5| 1\n  1 --&gt; |0.5| 2\n  2 --&gt; |1.0| 1 \n\n\n\n\n\n질문: \\(p_{ij}^{(T_0)}\\)를 “\\(i\\)에서 출발했다고 가정할때 \\(T_0\\)이후에 \\(j\\)에 도달해 있을 확률이라고 하자.” \\(T_0=2\\)일 경우 아래를 구하라.\n\n\\(p_{00}^{(2)}=0\\)\n\\(p_{01}^{(2)}=0.5\\)\n\\(p_{02}^{(2)}=0.5\\)\n\\(p_{10}^{(2)}=0\\)\n\\(p_{11}^{(2)}=?\\)\n\\(p_{12}^{(2)}=?\\)\n\\(p_{20}^{(2)}=0\\)\n\\(p_{21}^{(2)}=0.5\\)\n\\(p_{22}^{(2)}=0.5\\)\n\n\nP = np.array([[0,1,0],\n              [0,1/2,1/2],\n              [0,1,0]])\nP@P\n\narray([[0.  , 0.5 , 0.5 ],\n       [0.  , 0.75, 0.25],\n       [0.  , 0.5 , 0.5 ]])\n\n\n- 다시 가짜정의의 보충설명 (3) – (2)를 이어서\n\n여기에서 모든 상태공간이 연결되어있다는 의미는 상태공간 \\(E\\)에서 임의의 두 상태 \\(i,j\\)를 뽑았을때 \\(i \\to j\\) 이고, \\(j \\to i\\) 라는 의미이다.\n여기에서 \\(i\\to j\\) 라는 의미는 언젠가는 상태 \\(i\\)에서 출발한 체인이 상태 \\(j\\)에 도달할 수 있다는 의미이다.\n즉 \\(i \\to j\\)라는 의미는 “(\\(i\\)에서 출발한다면 \\(T_0\\) 이후에 \\(j\\)에 도달해 있을 확률) &gt; \\(0\\)” 이라는 뜻이다.\n\n즉 \\(i \\to j\\)라는 의미는 “\\(\\exists T_0 \\in \\mathbb{N}_0\\) such that \\(p_{ij}^{(T_0)}&gt;0\\)” 이라는 뜻이다.\n\n\n이 부분도 제가 설명을 잘못했는데 여기에서 \\(T_0=0\\) 인 경우는 \\({\\bf P}^{0}={\\bf I}\\) 와 같이 해석합니다. 따라서 모든 \\(\\forall x \\in E:~ x\\leftrightarrow x\\) 입니다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-16-11wk-2.html#정의-irreducible-irr",
    "href": "posts/2. 마코프체인/2023-05-16-11wk-2.html#정의-irreducible-irr",
    "title": "11wk-2: 마코프체인 (9)",
    "section": "정의: irreducible (IRR)",
    "text": "정의: irreducible (IRR)\n- 정의: \\(\\{X_t\\}\\)를 상태공간 \\(E\\)에 정의된 HMC라고 하고 \\({\\bf P}\\)를 \\(\\{X_t\\}\\)의 transition matrix (혹은 그 비슷한 것) 라고 하자. 임의의 \\(i,j \\in S\\)에 대하여 상태 \\(i\\)에서 상태 \\(j\\)로 도달가능(accessible)하다는 의미는\n\n\\(\\exists T_0 \\in \\mathbb{N}_0\\) such that \\(p_{ij}^{(T_0)}&gt;0\\)\n\n를 의미하며 이를 기호로는 \\(i\\to j\\)와 같이 표현한다. 참고로 여기에서 \\(p_{ij}^{(T_0)}\\)는 \\({\\bf P}^{T_0}\\)의 \\((i,j)\\)-th element이다.\n- 따라서 아래는 모두 같은 의미임\n\n\\(\\exists T_0 \\in \\mathbb{N}_0 \\textsf{ such that } p_{ij}^{(T_0)}&gt;0\\)\n\\(i \\to j\\)\n\\(j\\) is accessible from \\(i\\)\n\n- 정의: \\(\\{X_t\\}\\)를 상태공간 \\(E\\)에 정의된 HMC라고 하고 \\({\\bf P}\\)를 \\(\\{X_t\\}\\)의 transition matrix (혹은 그 비슷한 것) 라고 하자. 임의의 \\(i,j \\in S\\)에 대하여 상태 \\(i,j\\)가 상호도달가능 (communicate) 하다는 의미는\n\n\\(i \\to j\\) and \\(j \\to i\\)\n\n임을 의미한다. \\(i,j\\)가 상호도달할 경우 기호로는 \\(i \\leftrightarrow j\\) 와 같이 표현한다.\n- 이론: 아래가 성립한다. (굳이 증명할 필요없음. 결과만 기억해도 OK)\n\n\\(i \\leftrightarrow i\\)\n\\(i \\leftrightarrow j\\) \\(\\Rightarrow\\) \\(j \\leftrightarrow i\\)\n\\(i \\leftrightarrow j\\), \\(j \\leftrightarrow k\\) \\(\\Rightarrow\\) \\(i \\leftrightarrow k\\)\n\n따라서 \\(\\leftrightarrow\\) 는 equivalence relation 이다. 따라서 상태공간 \\(E\\)는 \\(\\leftrightarrow\\)를 기준으로 “나눌 수” 있다.\n- 정의: \\(\\{X_t\\}\\)를 상태공간 \\(E\\)에 정의된 HMC라고 하자. 상태공간 \\(E\\)는 equivalence relation \\(\\leftrightarrow\\)를 기준으로\n\\[E = E_1 \\uplus E_2 \\uplus \\dots\\]\n와 같이 “나눌 수” 있는데 이때 나누어진 집합 \\(E_1,E_2,\\dots\\) 를 communication class라고 부른다.\n- 예시1: 상태공간을 \\(\\{0\\}\\)와 \\(\\{1,2\\}\\)로 나눌 수 있다. 따라서 \\(E\\)는 2개의 communication class 를 가진다.\n\n\n\n\nflowchart LR\n  0 --&gt; |1.0| 1\n  1 --&gt; |0.5| 1\n  1 --&gt; |0.5| 2\n  2 --&gt; |1.0| 1 \n\n\n\n\n\n- 예시2: 아래와 같은 transition matrix를 가지는 마코프체인의 경우\n\nP = np.array([[1,0],\n              [0,1]])\nP\n\narray([[1, 0],\n       [0, 1]])\n\n\n상태공간을 \\(\\{0\\},\\{1\\}\\)로 나눌 수 있다. 따라서 \\(E\\)는 2개의 communication class 를 가진다.\n- 정의 \\(\\{X_t\\}\\)를 상태공간 \\(E\\)에 정의된 HMC라고 하고 \\({\\bf P}\\)를 \\(\\{X_t\\}\\)의 transition matrix (혹은 그 비슷한 것) 라고 하자. 상태공간 \\(E\\)가 오직 하나의 communication class를 가지는 경우 아래와 같이 말한다.\n\n\\(\\{X_t\\}\\) 가 irreducible 한 마코프체인이다.\n\\({\\bf P}\\) 가 irreducible 한 transition matrix 이다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-20-8wk-1.html",
    "href": "posts/2. 마코프체인/2023-04-20-8wk-1.html",
    "title": "08wk-1: 마코프체인 (4)",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yapWz131weSlgUlgS-0T98\n\n\n영상2는 추후 재촬영예정임\n\n\n\nimports\n\nimport numpy as np\n\n\n\nMarkovchain, Transition Matrix\n- 정의: 카운터블한 상태공간 \\(E\\)을 가지는 이산시간 확률과정 \\(\\{X_t\\}_{t\\geq 0}\\)을 고려하자. 아래가 성립한다면 확률과정 \\(\\{X_t\\}\\)을 마코프체인(Markov chain, MC)라고 한다.\n\n\\(\\forall t\\geq 0, \\forall i_0,i_1,\\dots,i_{n-1},i,j \\in E\\):\n\n\\[\\mathbb{P}(X_{t+1}=j | X_t=i, X_{t-1}=i_{t-1}, \\dots, X_0=i_0) = \\mathbb{P}(X_{t+1}=j|X_t=i)\\]\n만약에 \\(P(X_{t+1} =j | X_t=i)\\)가 모든 \\(t\\)에 대하여 일정하다면 \\(\\{X_t\\}\\)를 균질마코프체인1(homogeneous Markov chain, HMC) 라고 한다.\n1 진짜 억지로 변형한것, 마땅한 한글용어가 없음- 정의: 아래의 수식을 마코프성질 (Markov property) 이라고 한다.\n\n\\(\\forall t\\geq 0, \\forall i_0,i_1,\\dots,i_{n-1},i,j \\in E\\):\n\n\\[\\mathbb{P}(X_{t+1}=j | X_t=i, X_{t-1}=i_{t-1}, \\dots, X_0=i_0) = \\mathbb{P}(X_{t+1}=j|X_t=i)\\]\n- 정의: 카운터블한 상태공간 \\(E\\)를 가지는 HMC \\(\\{X_t\\}_{t\\geq 0}\\)를 고려하자. 상태 \\(i\\)에서 상태 \\(j\\)로 바뀌는 조건부 확률\n\\[p_{ij}=\\mathbb{P}(X_{t+1}=j | X_t=0)\\]\n를 \\(\\{X_t\\}_{t\\geq 0}\\)의 전이확률(transition probability)라고 한다.\n- 전이확률의 특징: 이때 전이확률은 아래의 특징을 가진다.\n\n\\(p_{ij} \\geq 0\\)\n\n\\(\\sum_{j\\in E}p_{ij}=1\\)\n\n첫번째 식은 확률이 양수이어야 한다는 내용이고2 두번째 식은 임의의 시점에서 상태 \\(i\\)에 존재할 경우, 그 다음시점에서 상태집합 \\(V\\) 중 어딘가로는 이동해야한다는 의미이다.\n2 쓸모없는 내용- 정의: 카운터블한 상태공간 \\(V\\)를 가지는 HMC \\(\\{X_t\\}_{t \\geq 0}\\)를 고려하자. \\(p_{ij}\\)를 \\(\\{X_t\\}_{t\\geq 0}\\)의 전이확률이라고 하자. \\((i,j)\\)-th 원소를 \\(p_{ij}\\)로 가지는 행렬 \\({\\bf P}\\)를 전이확률행렬 (transition probability matrix) 혹은 줄여서 전이행렬 (transition matrix) 이라고 한다.\n- 참고(\\(\\star\\)): 상태공간 \\(V\\)의 원소수가 무한일 수도 있으므로, 원래 \\({\\bf P}\\)를 행렬이라고 하기에는 무리가 있다. 하지만 행렬의 덧셈, 행렬의 곱셈과 같은 연산들은 일반적으로 잘 정의되므로 \\({\\bf P}\\)를 행렬로 생각할 수 있다. 이러한 \\({\\bf P}\\)는 row와 col이 무한대로 있다고 생각하면 된다.\n\n\\(|E|=\\infty\\) 인 경우 \\({\\bf P}\\)의 예시: \\({\\bf P}=\\begin{bmatrix} p_{00} & p_{01} & \\cdots \\\\ p_{10} & p_{11} & \\cdots \\\\ \\cdots & \\cdots & \\cdots \\end{bmatrix}\\)\n\n- 전이행렬의 특징: 모든 row의 합이 1이다.\n\n\\(\\sum_{j \\in E}p_{ij} = 1\\) 이어야 하므로\n\n\n\nDistribution, Distribution Function\n- 예제1: 동전예제\n선언1: \\((\\Omega, 2^{\\Omega}, \\mathbb{P})\\) 를 확률공간이라고 하자. 여기에서 확률 \\(\\mathbb{P}\\)은 아래와 같이 정의되는 set function 이다.\n\n\\(\\mathbb{P}(\\emptyset) = 0\\)\n\\(\\mathbb{P}(\\{H\\}) = 1/2\\)\n\\(\\mathbb{P}(\\{T\\}) = 1/2\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\n\n선언2: 확률변수 \\(X: (\\Omega, 2^\\Omega) \\to (V,2^V)\\)를 아래와 같이 선언하자. (단, \\(V=\\{0,1\\}\\))\n\n\\(X(H) = 0\\)\n\\(X(T) = 1\\)\n\n생각: 이제 \\(B \\in 2^V\\) 에 대하여 아래와 같은 표현들을 고려하자.\n\n표현1: \\(\\mathbb{P}(X \\in B)\\) // 고등학교 부터 쓰던 그 표현\n표현2: \\(\\mathbb{P}(\\{\\omega: X(\\omega) \\in B\\})\\) // 이번에 배운 표현, 표현1의 정확한 버전\n표현3: \\(\\mathbb{P}(X^{-1}(B))\\) // 표현2의 다른 버전, inverse image의 느낌이 확 살아 있음\n표현4: \\((\\mathbb{P} \\circ X^{-1})(B)\\) // 생각해보니까 이것도 가능함. \\(\\mathbb{P}\\), \\(X\\) 모두 함수였잖아?\n\n새로운 함수 \\(\\mu:= \\mathbb{P}\\circ X^{-1}\\)는 이 경우 어떻게 정의할 수 있을까?\n\n\\(\\mu(\\emptyset) = 0\\)\n\\(\\mu(\\{0\\}) = \\frac{1}{2}\\)\n\\(\\mu(\\{1\\}) = \\frac{1}{2}\\)\n\\(\\mu(\\{0,1\\}) = 1\\)\n\n표현1과 4만 모아서 살펴보면 아래와 같다.\n\n\\(\\mu(\\emptyset) = 0\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X \\notin \\{0,1\\})=0\\)\n\\(\\mu(\\{0\\}) = \\frac{1}{2}\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X=0)\\)\n\n\\(\\mu(\\{1\\}) = \\frac{1}{2}\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X=1\\})\\)\n\\(\\mu(\\{0,1\\}) = \\frac{1}{2}\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X\\in \\{0,1\\})\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X\\leq 1)\\)\n\n- 예제2: 동전예제(2)\n\\((V,2^V)\\) 대신에 \\((\\mathbb{R},{\\cal R})\\) 으로 바꾸어도 위의 동전예제는 잘 정의된다.\n선언1: \\((\\Omega, 2^{\\Omega}, \\mathbb{P})\\) 를 확률공간이라고 하자. 여기에서 확률 \\(\\mathbb{P}\\)은 아래와 같이 정의되는 set function 이다.\n\n\\(\\mathbb{P}(\\emptyset) = 0\\)\n\\(\\mathbb{P}(\\{H\\}) = 1/2\\)\n\\(\\mathbb{P}(\\{T\\}) = 1/2\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\n\n선언2: 확률변수 \\(X: (\\Omega, 2^\\Omega) \\to (\\mathbb{R},{\\cal R})\\)를 아래와 같이 선언하자.\n\n\\(X(H) = 0\\)\n\\(X(T) = 1\\)\n\n생각: 이제 \\(B \\in {\\cal R}\\) 에 대한 표현들. 편의상 \\(B=\\{b: b\\leq 0.5\\}\\) 라고 가정하자.\n\n표현1: \\(\\mathbb{P}(X \\in B)=\\mathbb{P}(X\\leq 0.5)=\\mathbb{P}(X=0)=\\frac{1}{2}\\)\n표현2: 생략\n표현3: 생략\n표현4: \\((\\mathbb{P} \\circ X^{-1})((-\\infty,0.5])\\)\n\n표현1과 4만 모아서 살펴보면 아래와 같다.\n\n\\(\\mu((-\\infty,x])\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X \\leq x)\\)\n\\(\\mu(A)\\) \\(\\Leftrightarrow\\) \\(\\mathbb{P}(X\\in A)\\)\n\n- 생각의 시간\n\\((\\Omega,{\\cal F}, \\mathbb{P})\\)가 확률공간이고 \\(X \\to \\mathbb{R}\\)이 확률변수라면, \\(\\mu\\)는 언제나 잘 정의된다.\n\n모든 \\(B \\in {\\cal R}\\)에 대하여 \\(X^{-1}(B)\\)가 시그마필드의 원소가 아닐 수 없다. (만약 그렇다면 \\(X\\)는 확률변수가 아닌걸?)\n모든 \\(B \\in {\\cal R}\\)에 대하여 \\(\\mathbb{P}(X^{-1}(B))\\)의 값을 모순되게 정의할 수 없다. (만약 그렇다면 \\((\\Omega, {\\cal F}, \\mathbb{P})\\)는 확률공간이 아닌걸?)\n\n결론: \\(\\mu\\)는 안전해!\n- \\(\\mu\\)도 메져의 조건을 만족한다.\n\n정의역이 시그마필드임\n\\(\\forall B \\in {\\cal R}:~ \\mu(B)\\geq 0\\).\n\\(\\forall B_1,B_2,\\dots \\in {\\cal R}\\) such that \\(B_1,B_2 \\dots\\) are disjoint: \\(\\sum_{i=1}^{n}\\mu(B_i) = \\mu(\\uplus_{i=1}^{\\infty}B_i)\\)\n\n- \\(\\mu\\)를 부르는 용어 (\\(\\star\\star\\star\\)): \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, \\mathbb{P})\\)에서 정의된 확률변수라고 하자. 이때 \\(X^{-1}\\circ \\mathbb{P}\\)로 정의가능한 함수 \\(\\mu: {\\cal R} \\to [0,1]\\) 를 \\(X\\)의 distribution 이라고 부른다.\n- \\(F(x)\\)의 정의: \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, \\mathbb{P})\\)에서 정의된 확률변수라고 하자. \\(F: \\mathbb{R} \\to [0,1]\\) 인 함수를 아래와 같이 정의하자.\n\\[F(x) = \\mu((-\\infty, x])\\]\n이러한 함수 \\(F\\)는 아래와 같이 표현할 수 있다.\n\\[F(x) = \\mathbb{P}(X \\leq x)\\]\n함수 \\(F\\)를 확률변수 \\(X\\)의 distribution function 이라고 한다.\n- 참고사항 (그냥 교양임, 시험에 안냄):\n\n\\(\\mu\\)가 언제나 잘 정의되므로 \\(F(x)\\)도 언제나 잘 정의된다.\n\\(F(x)\\)는 어떠한 성질들을 가진다. (비감소함수, 오른쪽연속 등..)\n\\(F(x)\\)는 \\(F(x)= F_c(x) + F_s(x) + F_d(x)\\) 와 같이 분해가능하다.\n\\(F(x)=F_c(x)\\)라면 \\(F(x)\\)는 연속형확률변수의 cdf가 된다. \\(F(x)=F_d(x)\\)라면, \\(F(x)\\)는 이산형확률변수의 cdf가 된다.\n\\(F(x)=F_c(x)+F_d(X)\\)라면 혼합형확률변수의 cdf가 된다.\n\\(F(x)=F_s(x)\\)인 경우는 pdf, pmf가 존재하지 않는다.\n\n- Borel sets (어떤 학생이 헷갈려해서.. 제가 헷갈리게 설명해서..)\n\n\\(\\Omega=\\mathbb{R}\\) 일때 \\(2^{\\mathbb{R}}\\) 역시 시그마필드임.\n따라서 적당한 메져가 존재하여 \\(2^\\mathbb{R}\\)의 모든 집합을 잴 수 있음. (모든 원소를 0으로 측정하는 메져라든가..)\n하지만 르벡메져는 \\(2^{\\mathbb{R}}\\)의 모든 원소를 잴 수 없음. 따라서 \\(2^{\\mathbb{R}}\\)의 모든 원소에서 확률을 정의하는 것이 불가능함.\n그러나 \\(\\Omega=\\mathbb{R}\\)일때 \\({\\cal R}\\)이라는 시그마필드는 모든 원소에서 확률을 정의할 수 있음.\n\\({\\cal R}\\)을 Borel sets 이라고 부름.\n\n- \\(\\mathbb{R}\\)을 포함하는 Borel sets 은 \\({\\cal B}(\\mathbb{R})\\)로 표현하기도 함. 즉 \\({\\cal R} = {\\cal B}(\\mathbb{R})\\) 이다.\n\n\nThe Stationary Distribution of an HMC\n- 정의: stationary distribution (정확한 버전)\n\\((E,{\\cal B}(E))\\)를 잴 수 있는 공간이라고 하고 \\(\\mu\\)를 \\((E,{\\cal B}(E))\\)에서의 distribution 이라고 하자. 만약에 아래식을 만족하면 \\(\\mu\\) 를 stationary distribution 이라고 한다.\n\\[\\mu p = \\mu\\]\n여기에서 \\(\\mu p(\\{x\\}):= \\sum_{y \\in E} \\mu(\\{y\\})p_{yx}\\) 를 의미한다.\n- 정의: stationary distribution (쉬운버전)\n아래식을 만족하는 distribution \\({\\boldsymbol \\mu}\\) 를 stationary distribution 이라고 한다.\n\\[{\\boldsymbol \\mu}^\\top{\\bf P} = {\\boldsymbol \\mu}^\\top\\]\n- 예시1: 아래와 같은 transition matrix를 고려하자.\n\nP = np.array([[0.2,0.8],\n              [0.3,0.7]])\nP\n\narray([[0.2, 0.8],\n       [0.3, 0.7]])\n\n\n수렴할까?\n\nnp.linalg.matrix_power(P,50)\n\narray([[0.27272727, 0.72727273],\n       [0.27272727, 0.72727273]])\n\n\n결과분석\n\n특징1: \\({\\bf P}^{\\star}\\)로 수렴한다.\n특징2: 수렴한 매트릭스를 세로로 읽으면 값이 같다. \\(\\Rightarrow\\) … \\(\\Rightarrow\\) \\({\\bf P}^{\\star}\\)의 아무 row나 가져오면 정상분포가 된다.\n특징3: \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\) \\(\\Leftarrow\\) \\(({\\boldsymbol \\mu}^\\top{\\bf P}^\\star) {\\bf P} ={\\boldsymbol \\pi}^\\top\\)\n특징4: 초기분포에 \\({\\boldsymbol \\pi}^\\top\\)을 대입하면 \\(\\{X_t\\}\\)는 동일한 분포를 가진다.\n\n- 예시2: 아래와 같은 transition matrix를 고려하자.\n\nP = np.array([[0.4,0.6],\n              [0.9,0.1]])\nP\n\narray([[0.4, 0.6],\n       [0.9, 0.1]])\n\n\n수렴할까?\n\nnp.linalg.matrix_power(P,50)\n\narray([[0.6, 0.4],\n       [0.6, 0.4]])\n\n\n결과분석\n\n특징1: \\({\\bf P}^{\\star}\\)로 수렴한다.\n특징2: 수렴한 매트릭스를 세로로 읽으면 값이 같다. \\(\\Rightarrow\\) … \\(\\Rightarrow\\) \\({\\bf P}^{\\star}\\)의 아무 row나 가져오면 정상분포가 된다.\n특징3: \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\) \\(\\Leftarrow\\) \\(({\\boldsymbol \\mu}^\\top{\\bf P}^\\star) {\\bf P} ={\\boldsymbol \\pi}^\\top\\)\n특징4: 초기분포에 \\({\\boldsymbol \\pi}^\\top\\)을 대입하면 \\(\\{X_t\\}\\)는 동일한 분포를 가진다.\n\n- 예시3: 어지간하면 다 수렴할 것 같으니까 아래와 같이 특이한 transition matrix를 고려하자.\n\nP = np.array([[1.0, 0.0],\n              [0.05,0.95]])\nP\n\narray([[1.  , 0.  ],\n       [0.05, 0.95]])\n\n\n\nnp.linalg.matrix_power(P,50)\n\narray([[1.        , 0.        ],\n       [0.92305502, 0.07694498]])\n\n\n수렴안하나?\n\nnp.linalg.matrix_power(P,100)\n\narray([[1.        , 0.        ],\n       [0.99407947, 0.00592053]])\n\n\n\nnp.linalg.matrix_power(P,500)\n\narray([[1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.27449156e-12]])\n\n\n결국에는 한다.\n결과분석\n\n특징1: \\({\\bf P}^{\\star}\\)로 수렴한다.\n특징2: 수렴한 매트릭스를 세로로 읽으면 값이 같다. \\(\\Rightarrow\\) … \\(\\Rightarrow\\) \\({\\bf P}^{\\star}\\)의 아무 row나 가져오면 정상분포가 된다.\n특징3: \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\) \\(\\Leftarrow\\) \\(({\\boldsymbol \\mu}^\\top{\\bf P}^\\star) {\\bf P} ={\\boldsymbol \\pi}^\\top\\)\n특징4: 초기분포에 \\({\\boldsymbol \\pi}^\\top\\)을 대입하면 \\(\\{X_t\\}\\)는 동일한 분포를 가진다.\n\n- 공식 (쓸모없는): transition matrix 가 아래와 같은 (2,2)-matrix이라고 하자.\n\n\\({\\bf P} = \\begin{bmatrix} 1-a & a \\\\ b & 1-b \\end{bmatrix}\\)\n\n그러면 대응하는 정상확률분포는 아래와 같다.\n\n\\(\\pi_0= \\frac{b}{a+b}\\)\n\\(\\pi_1= \\frac{a}{a+b}\\)\n\n예시1의 경우를 이 공식에 넣으면\n\n0.3/(0.8+0.3),0.8/(0.8+0.3)\n\n(0.2727272727272727, 0.7272727272727273)\n\n\n예시2의 경우를 이 공식에 넣으면\n\n0.9/(0.6+0.9), 0.6/(0.6+0.9)\n\n(0.6, 0.39999999999999997)\n\n\n예시3의 경우를 이 공식에 넣으면\n\n0.05/(0+0.05) , 0/(0+0.05)\n\n(1.0, 0.0)\n\n\n- 예시4: \\(a+b=0\\) 이라면?\n\nP = np.array([[1.0, 0.0],\n              [0.0, 1.0]])\nP\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n수렴은 할텐데..\n결과분석\n\n특징1: \\({\\bf P}^{\\star}\\)로 수렴한다.\n특징2: 수렴한 매트릭스를 세로로 읽으면 값이 다르다?\n특징3: 어?\n특징4: 어????? (이건 그냥 되는데?)\n\n특징3: 정상분포\n일단 모든 \\({\\boldsymbol \\mu}\\)에 대하여 아래가 성립하긴한다.\n\\[{\\boldsymbol \\mu}^\\top {\\bf P} =  {\\boldsymbol \\mu}^\\top\\]\n따라서 이 경우 모든 확률측도 \\({\\boldsymbol \\mu}\\)는 정상분포가 된다. 유일한 정상분포를 가지지 않는다!!\n특징4: 정상확률과정\n\\({\\bf P}= {\\bf I}\\) 이므로 당연히 \\(\\{X_t\\}\\)는 모든 \\(t\\geq 0\\)에 대하여 동일한 분포를 가진다.\n- 예시5 (\\(\\star\\star\\star\\))\n\nP = np.array([[0.0, 1.0],\n              [1.0, 0.0]])\nP\n\narray([[0., 1.],\n       [1., 0.]])\n\n\n\nP@P\n\narray([[1., 0.],\n       [0., 1.]])\n\n\n\nP@P@P\n\narray([[0., 1.],\n       [1., 0.]])\n\n\n결과분석\n\n특징1: 수렴을 안하는데?\n특징2:\n특징3:\n특징4:\n\n특징3: 정상분포\n만약에 \\({\\boldsymbol \\pi}=\\begin{bmatrix} 1/2 \\\\ 1/2 \\end{bmatrix}\\) 로 설정한다면 아래가 성립한다.\n\\[{\\boldsymbol \\pi}^\\top {\\bf P} =  {\\boldsymbol \\pi}^\\top\\]\n따라서 \\({\\boldsymbol \\pi}\\)는 정상분포가 된다.\n특징4: 정상확률과정\n만약에 \\({\\boldsymbol \\pi}=\\begin{bmatrix} 1/2 \\\\ 1/2 \\end{bmatrix}\\) 로 설정한다면 \\(\\{X_t\\}\\)는 모든 \\(t\\geq 0\\)에 대하여 동일한 분포를 가진다.\n- 생각의 시간\n\n\n\n\n특징1(수렴)\n특징2(동일row)\n특징3(정상분포)\n특징4(정상과정)\n\n\n\n\n예시1(나이스)\nO\nO\n존재O, 유일O\nO\n\n\n예시2(나이스)\nO\nO\n존재O, 유일O\nO\n\n\n예시3(흡수)\nO\nO\n존재O, 유일O\nO\n\n\n예시4(단위행렬)\nO\nX\n존재O, 유일X\nO\n\n\n예시5(주기)\nX\nNA\n존재O, 유일O\nO\n\n\n\n특징3에서 정상분포가 존재하면 특징4는 그냥 성립한다. 지금까지 살펴본 예제에서는 모두 정상분포가 존재했다. 혹시 정상분포가 존재하지 않을 수도 있을까?\n- Thm: finite state를 가지는 HMC는 정상분포가 최소한 1개는 존재한다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-w6PeAXdc4YcGTb7M_67Wog"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html#나그네",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html#나그네",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "나그네",
    "text": "나그네\n- 나그네 (박목월)\n강나루 건너서\n밀밭 길을\n\n구름에 달 가듯이\n가는 나그네\n\n길은 외줄기\n南道 삼백리\n\n술 익는 마을마다\n타는 저녁놀\n\n구름에 달 가듯이\n가는 나그네\n- 나그네\n\n정착 X\n모든 장소에 일시적(transient)으로만 머뭄\n다시 돌아올 수는 있는데 금방 다시 감.\n\n- 편의상 아래와 같이 생각하자.\n\n\\(E\\): 마을의 집합\n\\(X_t=i\\): \\(t\\)시점에 나그네가 마을 \\(i\\)에 머무는 event"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html#급수의-수렴",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html#급수의-수렴",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "급수의 수렴",
    "text": "급수의 수렴\n- \\(a_n \\to 0\\) 이라고 해서 \\(\\lim_{n\\to\\infty} S_n &lt;\\infty\\) 인건 아니다.\n- 예시1: \\(a_n=\\frac{1}{2^n}\\), 수렴하는 경우\n\nsum([1/2**i for i in range(1,10000)])\n\n1.0\n\n\n- 예시2: \\(a_n = \\frac{1}{n}\\), 수렴안하는 경우\n\nsum([1/i for i in range(1,10000)])\n\n9.787506036044348"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html#예제1-오른쪽으로만-갈래",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html#예제1-오른쪽으로만-갈래",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "예제1: 오른쪽으로만 갈래",
    "text": "예제1: 오른쪽으로만 갈래\n확률변수열 \\(\\{X_t\\}\\)가 HMC라고 하고, 그 transition matrix \\({\\bf P}\\) (혹은 그 비슷한 것) 가 아래와 같다고 하자.\n\\[{\\bf P} = \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 & \\dots \\\\\n0 & 0 & 1 & 0 & 0 & \\dots \\\\\n0 & 0 & 0 & 1 & 0 & \\dots \\\\\n0 & 0 & 0 & 0 & 1 & \\dots \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\n\\end{bmatrix}\\]\n- 체크: 이 예제의 마코프체인은 IRR 하지 않다.\n- 나이스케이스: \\(\\bar{\\boldsymbol \\pi}^\\top \\overset{T \\to \\infty}{\\longrightarrow} {\\bf p}_{\\star}^\\top = {\\boldsymbol \\pi}^\\top\\)\n- 이 예제는 나이스하지 않음 왜? IRR이 아니라서?\n\nIRR이 아니라서 나이스하지 않다는 것은 핑계임.\n오른쪽으로 갈 확률을 0.99로 수정한다면 IRR 마코프체인이 된다. 그렇지만 이게 나이스하게 바뀔 것 같지는 않음.\n\n- 나이스하지 않은 본질적인 이유\n\n상태 \\(i\\)에 일시적(transient)으로 머무는 느낌. 거의 나그네 수준임.\n\\(\\bar{\\boldsymbol \\pi}^\\top \\overset{T \\to \\infty}{\\longrightarrow} {\\bf p}_{\\star}^\\top = {\\boldsymbol \\pi}^\\top\\) 이와 같은 논리전개를 쓰려면 일단 \\(\\{X_t\\}\\)가 특정상태를 무한번 방문해야 가능\n\n- FINITE case\n\nIRR은 가정할 수 있음.\nIRR을 가정한다면, 모든 마을에 대해서 나그네가 반복적으로 돌아오는 느낌이 있음.\n\n- 깨달음.\n\nFINITE 인 경우는 IRR 이기만 하면 “반복적으로 마을방문” 이 보장되었다.\n그런데 INFINITE 한 경우는 IRR 이어도 “반복적으로 마을방문” 이 보장되지 않는다.\n\n\nIRR 조건이 엄청 대단한 조건인줄 알았는데, 사실 그런게 아니고 (수틀리면 그냥 IRR 이라고 가정해도 무방한) 실제로 대단한 조건은 숨어있는 “반복적으로 마을방문” 이라는 조건임.\n\n- 가짜정의: HMC \\(\\{X_t\\}\\)가 (1) IRR (2) PR (3) AP 조건을 만족한다면 \\(\\{X_t\\}\\)를 에르고딕 마코프체인이라고 부른다. 여기에서 PR은 positive recurrent의 약자이며 “반복적으로 마을을 방문한다”의 의미를 가지고 있다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html#reccurent-transient",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html#reccurent-transient",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "Reccurent, Transient",
    "text": "Reccurent, Transient\n- 대안정의: \\(\\{X_t\\}\\)가 상태공간 \\(E\\)에서 정의된 HMC 라고 하자. 만약에 상태 \\(i \\in E\\) 가 아래의 식을 만족한다면\n\\[\\sum_{t=0}^{\\infty} p_{ii}^{(t)}= \\infty\\]\n\\(i\\)는 recurrent 하다고 표현하고, 그렇지 않으면 \\(i\\)는 transient 하다고 표현한다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-18-12wk-1.html#예제2-reflecting-random-walk",
    "href": "posts/2. 마코프체인/2023-05-18-12wk-1.html#예제2-reflecting-random-walk",
    "title": "12wk-1: 마코프체인 (10)",
    "section": "예제2: reflecting random walk",
    "text": "예제2: reflecting random walk\n확률변수열 \\(\\{X_t\\}\\)가 HMC라고 하고, 그 transition matrix \\({\\bf P}\\) (혹은 그 비슷한 것) 가 아래와 같다고 하자.\n\\[{\\bf P} = \\begin{bmatrix}\n1-p & p & 0 & 0 & 0 & \\dots \\\\\n1-p & 0 & p & 0 & 0 & \\dots \\\\\n0 & 1-p & 0 & p & 0 & \\dots \\\\\n0 & 0 & 1-p & 0 & p & \\dots \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\n\\end{bmatrix}\\]\n- 체크: 이 마코프체인은 IRR하다.\n- case1: \\(p=0.99\\) 라고 하자.\n\np=0.99\nP1 = np.array([[i-j == 1 for i in range(1000)] for j in range(1000)])*p\nP2 = np.array([[j-i == 1 for i in range(1000)] for j in range(1000)])*(1-p)\nP = P1+P2\nP[0,0]= 1-p \nP\n\narray([[0.01, 0.99, 0.  , ..., 0.  , 0.  , 0.  ],\n       [0.01, 0.  , 0.99, ..., 0.  , 0.  , 0.  ],\n       [0.  , 0.01, 0.  , ..., 0.  , 0.  , 0.  ],\n       ...,\n       [0.  , 0.  , 0.  , ..., 0.  , 0.99, 0.  ],\n       [0.  , 0.  , 0.  , ..., 0.01, 0.  , 0.99],\n       [0.  , 0.  , 0.  , ..., 0.  , 0.01, 0.  ]])\n\n\n\n(np.matrix(P)**10).round(5)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n(np.matrix(P)**100).round(5)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n관찰결과\n\n\\(p_{00}^{(t)} \\to 0\\).\n\\(p_{00}^{(t)} \\to 0\\) 이라고 해서 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} &lt; \\infty\\) 이라고 주장할 순 없음. \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\) 역시 주장할 수 없음.\n\\(p_{00}^{(t)}\\)이 0으로 수렴하는 속도가 매우 빠르다면 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} &lt; \\infty\\) 일 것이고 그렇지 않다면 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\) 일 것임.\n이 경우는 \\(p_{00}^{(t)}\\)이 빠르게 0으로 수렴하는듯 보이므로 왠지 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} &lt; \\infty\\) 일 것으로 예상가능\n상태0은 transient 인 듯 하다. (확신 X)\n\n- case2: \\(p=0.1\\) 이라고 하자.\n\np=0.1\nP1 = np.array([[i-j == 1 for i in range(1000)] for j in range(1000)])*p\nP2 = np.array([[j-i == 1 for i in range(1000)] for j in range(1000)])*(1-p)\nP = P1+P2\nP[0,0]= 1-p \nP\n\narray([[0.9, 0.1, 0. , ..., 0. , 0. , 0. ],\n       [0.9, 0. , 0.1, ..., 0. , 0. , 0. ],\n       [0. , 0.9, 0. , ..., 0. , 0. , 0. ],\n       ...,\n       [0. , 0. , 0. , ..., 0. , 0.1, 0. ],\n       [0. , 0. , 0. , ..., 0.9, 0. , 0.1],\n       [0. , 0. , 0. , ..., 0. , 0.9, 0. ]])\n\n\n\n(np.matrix(P)**100).round(5)\n\narray([[0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       [0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       [0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       ...,\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ]])\n\n\n\n(np.matrix(P)**1000).round(5)\n\narray([[0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       [0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       [0.88889, 0.09877, 0.01097, ..., 0.     , 0.     , 0.     ],\n       ...,\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\n       [0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ]])\n\n\n관찰결과\n\n\\(p_{00}^{(t)} \\to 0.88889\\).\n\\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\)\n따라서 상태0은 recurrent!\n\n\\(p_{00}^{(t)} \\to 0.88889\\) 이므로 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\) 이다. 따라서 상태 0은 recurrent!\n- case3: \\(p=0.5\\) 이라고 하자.\n\np=0.5\nP1 = np.array([[i-j == 1 for i in range(1000)] for j in range(1000)])*p\nP2 = np.array([[j-i == 1 for i in range(1000)] for j in range(1000)])*(1-p)\nP = P1+P2\nP[0,0]= 1-p \nP\n\narray([[0.5, 0.5, 0. , ..., 0. , 0. , 0. ],\n       [0.5, 0. , 0.5, ..., 0. , 0. , 0. ],\n       [0. , 0.5, 0. , ..., 0. , 0. , 0. ],\n       ...,\n       [0. , 0. , 0. , ..., 0. , 0.5, 0. ],\n       [0. , 0. , 0. , ..., 0.5, 0. , 0.5],\n       [0. , 0. , 0. , ..., 0. , 0.5, 0. ]])\n\n\n\n(np.matrix(P)**1000).round(3)\n\narray([[0.025, 0.025, 0.025, ..., 0.   , 0.   , 0.   ],\n       [0.025, 0.025, 0.025, ..., 0.   , 0.   , 0.   ],\n       [0.025, 0.025, 0.025, ..., 0.   , 0.   , 0.   ],\n       ...,\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ]])\n\n\n\n(np.matrix(P)**100000).round(3)\n\narray([[0.003, 0.003, 0.003, ..., 0.   , 0.   , 0.   ],\n       [0.003, 0.003, 0.003, ..., 0.   , 0.   , 0.   ],\n       [0.003, 0.003, 0.003, ..., 0.   , 0.   , 0.   ],\n       ...,\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ]])\n\n\n\n(np.matrix(P)**10000000).round(3)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n관찰결과\n\n\\(p_{00}^{(t)} \\to 0\\).\n그런데 엄청 천천히 0으로 수렴함.\n\\(p_{00}^{(t)}\\)이 0으로 수렴하는 속도가 매우 빠르다면 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} &lt; \\infty\\) 일 것이고 그렇지 않다면 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\) 일 것임.\n이 경우는 왠지 \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)} = \\infty\\) 일 것 같음.\n그래서 상태 0은 recurrent 인 것 같음.\n실제로 그런지 실험해볼까?\n\n확인: \\(\\sum_{t=0}^{\\infty}p_{00}^{(t)}=\\infty\\) 임을 프로그래밍을 이용하여 근사적으로 체크해보자.\n\nPstar = P.copy()\npT = list()\npT.append(Pstar[0,0])\nT = 10000\nfor t in range(T):\n    Pstar = Pstar@P\n    pT.append(Pstar[0,0])    \nnp.array(pT).cumsum()\n\narray([  0.5       ,   1.        ,   1.375     , ..., 157.58090143,\n       157.58888008, 157.59685793])\n\n\n여기에서\n\npT = \\([p_{00}^{(0)},p_{00}^{(1)},\\dots,p_{00}^{(T)}]\\)\nnp.array(pT).cumsum() = \\([\\sum_{t=0}^{0} p_{00}^{(t)},\\sum_{t=0}^{1} p_{00}^{(t)},\\dots,\\sum_{t=0}^{T} p_{00}^{(t)}]\\)\n\n이다. 마지막의 np.array(pT).cumsum()을 시각화하면 아래와 같다.\n\nplt.plot(np.array(pT).cumsum())\n\n\n\n\n그래프를 보니 발산할 것 같음\n- 결론: 계산해보면 (이론적으로든, 시뮬레이션을 이용하든) 이 예제의 경우 아래와 같이 됨을 알 수 있다.\n\n경우1: \\(p&gt;1/2\\) \\(\\Rightarrow\\) \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)}&lt;\\infty\\). // state 0 is transient\n경우2: \\(p&lt;1/2\\) \\(\\Rightarrow\\) \\(\\sum_{t=0}^{\\infty}p_{00}^{(t)}=\\infty\\) with \\(p_{00}^{(t)} \\rightarrow c\\), \\(c&gt;0\\). // state 0 is positive recurrent\n경우3: \\(p=1/2\\) \\(\\Rightarrow\\) \\(\\sum_{t=0}^{\\infty} p_{00}^{(t)}=\\infty\\) with \\(p_{00}^{(t)} \\rightarrow 0\\). // state 0 is null recurrent\n\n- \\(p=0.45, p=0.5, p=0.55\\) 일 경우 \\(\\sum_{t=0}^{\\infty}p_{00}^{(t)}\\)의 값을 시각화\n\ndef calculate_pT(p): \n    P1 = np.array([[i-j == 1 for i in range(1000)] for j in range(1000)])*p\n    P2 = np.array([[j-i == 1 for i in range(1000)] for j in range(1000)])*(1-p)\n    P = P1+P2\n    P[0,0]= 1-p \n    \n    Pstar = P.copy()\n    pT = list()\n    pT.append(Pstar[0,0])\n    for t in range(1000):\n        Pstar = Pstar@P\n        pT.append(Pstar[0,0])    \n    return np.array(pT)\n\n\ncase1 = calculate_pT(0.55)\ncase2 = calculate_pT(0.45)\ncase3 = calculate_pT(0.50)\n\n\nfig = plt.figure(figsize=(8,6))\nplt.plot(case1.cumsum(),label=r'$p=0.55$ $\\Rightarrow$ transient', color='C0')\nplt.plot(case2.cumsum(),label=r'$p=0.45$ $\\Rightarrow$ positive recurrent', color='C1')\nplt.plot(case3.cumsum(),label=r'$p=0.50$ $\\Rightarrow$ null recurrent', color='C2')\nplt.title('reflecting random walk with $p$',size=15)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fa316ab5e50&gt;"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xbWjXgaQNqqqZDzuV1QsgL"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#motivating-examples-cont",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#motivating-examples-cont",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "Motivating Examples (cont)",
    "text": "Motivating Examples (cont)\n\n예제3\n- 아래의 전이확률을 고려하자.\n\nP =np.array([0.0, 1.0, 0.0, 0.0, \n             1/2, 0.0, 1/2, 0.0,\n             0.0, 0.0, 0.0, 1.0,\n             0.0, 1.0, 0.0, 0.0]).reshape(4,4)\nP\n\narray([[0. , 1. , 0. , 0. ],\n       [0.5, 0. , 0.5, 0. ],\n       [0. , 0. , 0. , 1. ],\n       [0. , 1. , 0. , 0. ]])\n\n\n- 다이어그램\n\n\n\n\nflowchart LR\n  0 --&gt;|1| 1\n  1 --&gt;|1/2| 0\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3\n  3 --&gt;|1| 1\n\n\n\n\n\n- 특징1,2:\n\nnp.matrix(P)**500\n\nmatrix([[0.2, 0.4, 0.2, 0.2],\n        [0.2, 0.4, 0.2, 0.2],\n        [0.2, 0.4, 0.2, 0.2],\n        [0.2, 0.4, 0.2, 0.2]])\n\n\n- 특징3: 정상분포를 가짐\n- 특징4: 초기분포가 정상분포라면 정상확률과정\n- 특징5: irr\n- 특징6: 주기가 없음\n\n\n\n\nflowchart LR\n  0 --&gt;|1| 1\n  1 --&gt;|1/2| 0\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3\n  3 --&gt;|1| 1\n\n\n\n\n\n1에서 시작한다면?\n\n\\(1 \\to 0 \\to 1\\), 2번만에 리턴\n\\(1 \\to 2 \\to 3 \\to 1\\), 3번만에 리턴\n\n이 경우 2와 3의 최대공약수는 1이므로 주기는 1이다. 그리고 finite state space를 가지는 HMC는 모든 state가 항상 같은 주기를 가지므로 이 마코프체인의 모든 주기는 1이다.\n\n주기가 1인 경우는 aperiodic 하다고 표현한다. (언제 올지 몰라)\n\n\n꿀팁: 자기자신으로 1턴만에 되돌아올 확률이 있다면 항상 aperiodic 하다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#정의-및-이론",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#정의-및-이론",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "정의 및 이론",
    "text": "정의 및 이론\n- 정의:\n- 느낌: 상태 \\(i\\)에서 \\(i\\)로 되돌아오는 횟수들의 최대공약수를 HMC \\(\\{X_t\\}\\)의 period라고 하고, period=1인 경우를 aperiodic 이라고 한다.\n- 이론: HMC \\(\\{X_t\\}\\)이 IRR이면, 모든 상태가 항상 같은 주기를 가진다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#정의-및-이론-1",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#정의-및-이론-1",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "정의 및 이론",
    "text": "정의 및 이론\n- Thm: HMC \\(\\{X_t\\}\\)가 (1) finite state space를 가지고 (2) irreduciable 하고 (3) aperiodic 이라면, \\({\\bf P}\\)가 수렴하고 수렴한 matrix의 모든 row는 같다. 따라서 임의의 초기분포 \\({\\boldsymbol \\mu}\\) 에 대하여\n\\[\\lim_{t\\to \\infty}{\\boldsymbol \\mu}^\\top{\\bf P}^t = {\\boldsymbol \\pi}^\\top \\]\n이 성립한다. 여기에서 \\({\\boldsymbol \\pi}\\)는 \\(\\{X_t\\}\\)의 정상분포이다.\n- 정의: 아래의 식을 만족하는 HMC \\(\\{X_t\\}\\)을 에르고딕하다고 말한다.\n\\[\\lim_{t\\to \\infty}{\\boldsymbol \\mu}^\\top{\\bf P}^t = {\\boldsymbol \\pi}^\\top \\]"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#intro",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#intro",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "intro",
    "text": "intro\n- Google\n\nGoogle은 사용자의 검색어와 일치하는 검색 결과를 제공\nGoogle은 웹사이트들 사이에서 “더 나은” 또는 “더 중요한” 웹사이트가 검색 결과 상위에 나타나도록 순위를 유지\n이 순위는 전체 문제를 한 번에 해결하는 것이 아니라 먼저 전체적으로 수립되고(검색어와는 독립적으로), 그 후에 검색어와 일치하는 웹사이트들만 해당 순위에 따라 정렬된다고 함\n\n- 이 강의에서는 순위 매기기에 초점을 맞추어 생각해보자. (이는 마코프체인과 관련이 있음)\n\nref: https://en.wikipedia.org/wiki/PageRank\n\n- 페이지랭크\n\n페이지랭크(PageRank)는 구글 검색에서 웹 페이지의 순위를 결정하는 알고리즘으로 이는 “웹 페이지”와 구글 공동 창업자인 라리 페이지(Larry Page)의 이름을 따서 지어졌음\n페이지랭크는 웹사이트 페이지의 중요성을 측정하는 방법이며 기본적으로 더 중요한 웹사이일수록 다른 웹사이트에서 더 많은 링크를 받을 가능성이 높다는 점에 착안함\n페이지랭크는 구글이 검색 결과를 정렬하는 데 사용하는 유일한 알고리즘이 아니지만, 구글에서 사용한 최초의 알고리즘이며, 가장 잘 알려진 알고리즘임"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#toy-example",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#toy-example",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "toy example",
    "text": "toy example\n- 아래는 7개의 website에 대한 web graph이다.\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  1 --&gt;|1/2| 0\n  0 --&gt;|1/2| 2\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n  3 --&gt;|1| 5 \n  6 --&gt;|1| 5\n\n\n\n\n\n- 여기에서 가장 중요한 웹사이트는 무엇일까?\n\n구글의 아이디어는 기본적으로 더 많은 화살표를 받는 쪽이 더 중요한 웹사이트이다 라는 것이었다.\n이 논리대로라면 노드 2,3,5가 똑같이 중요해보인다.\n좀 더 생각해보니까 노드2보다 노드3과 노드5가 더 중요해보인다. 왜냐하면 노드2는 확률 1/2 짜리 화살표 2개이지만 노드3과 노드5는 확률 1짜리 화살표가 2개임\n그렇지만 또 노드3보다는 노드5가 더 중요해보인다. 왜냐하면 노드3을 방문한 사람은 결국은 노드5로 갈테니까 노드3보다 노드5가 더 중요한 사이트라고 볼 수 있다.\n그럼 노드3의 중요도가 1일때 노드5의 중요도는 얼마정도 될까?\n\n- 구글의 아이디어: random surfer\n\n무작위로 웹사이트를 방문하는 가상의 유저를 만들자.\n그리고 이 유저가 많이 방문하게 되는 웹사이트를 기록하자.\n\n- 구글의 아이디어는 결국 위의 다이어그램을 토대로 transition matrix \\({\\bf P}\\)를 만들고 임의의 초기상태 \\({\\boldsymbol \\mu}\\)에 대하여\n\\[\\lim_{t\\to\\infty}{\\boldsymbol \\mu}^\\top{\\bf P}^{t}\\]\n를 계산하겠다는 의미이다.\n- 문제점1: 이 상황은 transition matrix를 만들 수 없는걸?\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0, 0.0, 0.0,\n              1/2, 0.0, 1/2, 0.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ### 이 부분은 다 0이다. \n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]).reshape(7,7)\nP\n\narray([[0. , 0.5, 0.5, 0. , 0. , 0. , 0. ],\n       [0.5, 0. , 0.5, 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 1. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 1. , 0. ],\n       [0. , 0. , 0. , 1. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 1. , 0. ]])\n\n\n- 문제점1의 해결: 이러한 경우 상태5에서 다른상태로 갈 확률은 랜덤으로 다시 뿌린다.\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0, 0.0, 0.0,\n              1/2, 0.0, 1/2, 0.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, ### 이렇게 고쳐버리자~\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]).reshape(7,7)\nP\n\narray([[0.        , 0.5       , 0.5       , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        1.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        ],\n       [0.14285714, 0.14285714, 0.14285714, 0.14285714, 0.14285714,\n        0.14285714, 0.14285714],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        1.        , 0.        ]])\n\n\n- 문제점2: \\(\\lim_{t\\to\\infty}{\\boldsymbol \\mu}^\\top{\\bf P}^{t}\\)이게 수렴한다는 보장이 어디있지?"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#수렴의-트릭",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#수렴의-트릭",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "수렴의 트릭",
    "text": "수렴의 트릭\n- 생각: HMC \\(\\{X_t\\}\\)가 에르고딕이려면 (1) finite state space를 가지고 (2) irreducible (3) aperiodic 해야한다.\n- 그런데 (N,N) 차원을 가지는 임의의 transition matrix \\({\\bf P}\\)를 아래와 같이 \\(\\tilde{\\bf P}\\)로 변형한다면 이 transition matrix는 aperiodic하고 irreducible하게 된다.\n\\[\\tilde{\\bf P} = 0.99 \\cdot {\\bf P} + 0.01 \\cdot \\frac{1}{N}{\\bf J}\\]\n여기에서 \\({\\bf J}\\)는 \\({\\bf P}\\)와 차원이 같고 모든 원소가 1인 매트릭스이다. 즉\n\\[{\\bf J} = \\begin{bmatrix} 1 & 1 & \\dots & 1 \\\\ 1 & 1 & \\dots & 1 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & 1 & \\dots & 1 \\end{bmatrix}\\]\n이다.\n- 위의 수식에서 \\(\\tilde{\\bf P}\\)는 \\({\\bf P}\\)와 매우 비슷하지만 에르고딕한 마코프체인이다.\n- 이러한 \\(\\tilde{\\bf P}\\)를 구글매트릭스라고 부르자. 위의 식을 좀 더 간결하게 쓰면\n\\[{\\bf GoogleMatrix}:= \\alpha\\cdot {\\bf P} + (1-\\alpha)\\cdot\\frac{1}{N}{\\bf J}\\]\n와 같이 된다. 여기에서 \\(\\alpha \\in (0,1)\\) 이다.\n- 여기에서 \\(\\alpha\\)는 수렴의 속도를 결정한다.\n\n\\({\\bf P}\\)가 원래 수렴안하는 조건이었다면 \\(\\alpha \\approx 1\\) 일수록 구글매트릭스는 매우 느리게 수렴할 것이다.\n\\(\\alpha=0\\) 이라면 구글매트릭스는 이미 수렴되어 있다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#구현",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#구현",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "구현",
    "text": "구현\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0, 0.0, 0.0,\n              1/2, 0.0, 1/2, 0.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]).reshape(7,7)\nP\n\narray([[0.        , 0.5       , 0.5       , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        1.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        ],\n       [0.14285714, 0.14285714, 0.14285714, 0.14285714, 0.14285714,\n        0.14285714, 0.14285714],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        1.        , 0.        ]])\n\n\n\nalpha= 0.85 \nJ = np.ones(49).reshape(7,7)\nGoogleMatrix = alpha*P + (1-alpha)/7 \n\n\nnp.linalg.matrix_power(GoogleMatrix,100)[0].round(3).tolist()\n\n[0.102, 0.102, 0.145, 0.231, 0.058, 0.304, 0.058]\n\n\n\nimport pandas as pd \npd.DataFrame({'website':['state'+i for i in '0123456'], \n             'pagerank': np.linalg.matrix_power(GoogleMatrix,100)[0].round(3).tolist()})\n\n\n\n\n\n\n\n\nwebsite\npagerank\n\n\n\n\n0\nstate0\n0.102\n\n\n1\nstate1\n0.102\n\n\n2\nstate2\n0.145\n\n\n3\nstate3\n0.231\n\n\n4\nstate4\n0.058\n\n\n5\nstate5\n0.304\n\n\n6\nstate6\n0.058"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#다른풀이",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#다른풀이",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "다른풀이",
    "text": "다른풀이\n\n_, eigen_vector_matrix = np.linalg.eig(GoogleMatrix.T)\n\n\nabs(eigen_vector_matrix[:,0])/ abs(eigen_vector_matrix[:,0]).sum()\n\narray([0.10154862, 0.10154862, 0.14470678, 0.2310231 , 0.05839045,\n       0.30439198, 0.05839045])"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-27-9wk-1.html#페이지랭크의-약점",
    "href": "posts/2. 마코프체인/2023-04-27-9wk-1.html#페이지랭크의-약점",
    "title": "09wk-1: 마코프체인 (6)",
    "section": "페이지랭크의 약점",
    "text": "페이지랭크의 약점\n- 아래와 같은 상황을 고려하자.\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  1 --&gt;|1/2| 0\n  0 --&gt;|1/2| 2\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n\n\n\n\n\n\nP = np.arrfffq/5, 1/5,\n              0.0, 0.0, 0.0, 1.0, 0.0]).reshape(5,5)\nP\n\narray([[0. , 0.5, 0.5, 0. , 0. ],\n       [0.5, 0. , 0.5, 0. , 0. ],\n       [0. , 0. , 0. , 1. , 0. ],\n       [0.2, 0.2, 0.2, 0.2, 0.2],\n       [0. , 0. , 0. , 1. , 0. ]])\n\n\n\nGoogleMatrix = P*0.85 + 0.15/5 \n\n\nnp.linalg.matrix_power(GoogleMatrix,100)\n\narray([[0.15936255, 0.15936255, 0.22709163, 0.3625498 , 0.09163347],\n       [0.15936255, 0.15936255, 0.22709163, 0.3625498 , 0.09163347],\n       [0.15936255, 0.15936255, 0.22709163, 0.3625498 , 0.09163347],\n       [0.15936255, 0.15936255, 0.22709163, 0.3625498 , 0.09163347],\n       [0.15936255, 0.15936255, 0.22709163, 0.3625498 , 0.09163347]])\n\n\n- 우리는 여기에서 1번네트워크의 page rank를 올리고 싶다고 가정하자. (현재는 5개중 0.15936255)\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  1 --&gt;|1/2| 0\n  0 --&gt;|1/2| 2\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n\n\n\n\n\nStep1: 먼저 1번에서 다른쪽으로 가는 모든 링크를 끊는다. (다른 웹사이트의 page rank를 올려줄 이유가 없음)\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  0 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n\n\n\n\n\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0,\n              1/5, 1/5, 1/5, 1/5, 1/5,\n              0.0, 0.0, 0.0, 1.0, 0.0,\n              1/5, 1/5, 1/5, 1/5, 1/5,\n              0.0, 0.0, 0.0, 1.0, 0.0]).reshape(5,5)\n\n\nGoogleMatrix = P*0.85 + 0.15/5 \nnp.linalg.matrix_power(GoogleMatrix,100)\n\narray([[0.12640228, 0.18012324, 0.18012324, 0.38694897, 0.12640228],\n       [0.12640228, 0.18012324, 0.18012324, 0.38694897, 0.12640228],\n       [0.12640228, 0.18012324, 0.18012324, 0.38694897, 0.12640228],\n       [0.12640228, 0.18012324, 0.18012324, 0.38694897, 0.12640228],\n       [0.12640228, 0.18012324, 0.18012324, 0.38694897, 0.12640228]])\n\n\nStep2: 3개의 더미사이트 5,6,7을 만들어서 1번네트워크와 서로 연결시킨다.\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  0 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n  1 --&gt;|1/3| 5\n  5 --&gt;|1| 1\n  1 --&gt;|1/3| 6\n  6 --&gt;|1| 1\n  1 --&gt;|1/3| 7 \n  7 --&gt;|1| 1 \n\n\n\n\n\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1/3, 1/3, 1/3,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n              1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n              0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n              0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n              0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]).reshape(8,8)\n\n\nGoogleMatrix = P*0.85 + 0.15/8\npagerank =np.linalg.matrix_power(GoogleMatrix,100)[0]\npagerank\n\narray([0.02778839, 0.39804992, 0.03959846, 0.08506721, 0.02778839,\n       0.14056921, 0.14056921, 0.14056921])\n\n\n\nwebsite = ['state'+i for i in '01234567']\nwebsite\n\n['state0',\n 'state1',\n 'state2',\n 'state3',\n 'state4',\n 'state5',\n 'state6',\n 'state7']\n\n\n\npd.DataFrame({'pagerank':pagerank,'website':website})\n\n\n\n\n\n\n\n\npagerank\nwebsite\n\n\n\n\n0\n0.027788\nstate0\n\n\n1\n0.398050\nstate1\n\n\n2\n0.039598\nstate2\n\n\n3\n0.085067\nstate3\n\n\n4\n0.027788\nstate4\n\n\n5\n0.140569\nstate5\n\n\n6\n0.140569\nstate6\n\n\n7\n0.140569\nstate7\n\n\n\n\n\n\n\n- 약점을 극복한 구글의 아이디어: 저도 몰라용.."
  },
  {
    "objectID": "posts/4. 강화학습/A1.html",
    "href": "posts/4. 강화학습/A1.html",
    "title": "A1: 강화학습 (1) – bandit",
    "section": "",
    "text": "강의영상\n\n\n\n환경셋팅\n- 설치 (코랩)\n!pip install -q swig\n!pip install gymnasium\n!pip install gymnasium[box2d]\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nref: https://gymnasium.farama.org/index.html\n\n\n\nintro\n- 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를 학습하는 과업\n- 딥마인드: breakout \\(\\to\\) 알파고\n\nhttps://www.youtube.com/watch?v=TmPfTpjtdgg\n\n- 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n- 선행 (강화학습)\n\n프로그래밍 지식: 파이썬, 클래스에 대한 이해 // https://guebin.github.io/PP2023/ 10wk-2 이후\n딥러닝 기본지식: DNN // https://guebin.github.io/DL2022/ 3wk-02 ~ 4wk-02\n수학적인 지식: 마코프과정\n\n\n\nGame1: bandit\n- 문제설명: 두 개의 버튼이 있다. 버튼0을 누르면 1의 보상을, 버튼1을 누르면 100의 보상을 준다고 가정\n- 처음에 어떤 행동을 해야 하는가? —&gt; ??? 처음에는 아는게 없음 —&gt; 일단 “아무거나” 눌러보자.\n- 버튼을 아무거나 누르는 함수를 구현해보자.\n\naction_space = ['button0', 'button1'] \naction = np.random.choice(action_space)\naction\n\n'button0'\n\n\n- 보상을 주는 함수를 구현해보자.\n\nif action == 'button0': # button0을 눌렀다면 \n    reward = 1 \nelse: # button1을 눌렀다면 \n    reward = 100 \n\n\nreward\n\n1\n\n\n- 아무버튼이나 10번정도 눌러보면서 데이터를 쌓아보자.\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 'button0': \n        reward = 1 \n    else: \n        reward = 100     \n    print(action,reward) \n\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton0 1\nbutton1 100\nbutton1 100\nbutton0 1\n\n\n- 깨달았음: button0을 누르면 1점을 받고, button1을 누르면 100점을 받는 “환경”이구나? \\(\\to\\) button1을 누르는 “동작”을 해야하는 상황이구나?\n\n여기에서 \\(\\to\\)의 과정을 체계화 시킨 학문이 강화학습\n\n\nfor _ in range(10):\n    action = action_space[1]\n    if action == 'button0': \n        reward = 1 \n    else: \n        reward = 100     \n    print(action,reward) \n\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\n\n\n\n게임 클리어\n\n- 강화학습: 환경을 이해 \\(\\to\\) 행동을 결정\n위의 과정이 잘 되었다는 의미로 사용하는 문장들\n\n강화학습이 성공적으로 잘 되었다.\n에이전트가 환경의 과제를 완료했다.\n에이전트가 환경에서 성공적으로 학습했다.\n에이전트가 올바른 행동을 학습했다.\n게임 클리어 (비공식)\n\n- 게임이 클리어 되었다는 것을 의미하는 지표를 정하고 싶다.\n\n첫 생각: button1을 누르는 순간 게임클리어로 보면 되지 않나?\n두번째 생각: 아니지? 우연히 누를수도 있잖아?\n게임클리어조건: 최근 20번의 보상이 1900점 이상이면 게임이 클리어 되었다고 생각하자.1\n\n1 button1을 눌러야 하는건 맞지만 20번에 한번정도의 실수는 눈감아 주는 조건- 무지한자 – 게임을 클리어할 수 없다.\n\naction_space = [0,1]\nrewards = [] \nfor t in range(50): # 10000번을 해도 못깸 \n    action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 0   reward= 1   reward20= 101   \nn_try = 3   action= 0   reward= 1   reward20= 102   \nn_try = 4   action= 0   reward= 1   reward20= 103   \nn_try = 5   action= 1   reward= 100 reward20= 203   \nn_try = 6   action= 1   reward= 100 reward20= 303   \nn_try = 7   action= 1   reward= 100 reward20= 403   \nn_try = 8   action= 0   reward= 1   reward20= 404   \nn_try = 9   action= 1   reward= 100 reward20= 504   \nn_try = 10  action= 1   reward= 100 reward20= 604   \nn_try = 11  action= 0   reward= 1   reward20= 605   \nn_try = 12  action= 0   reward= 1   reward20= 606   \nn_try = 13  action= 1   reward= 100 reward20= 706   \nn_try = 14  action= 0   reward= 1   reward20= 707   \nn_try = 15  action= 0   reward= 1   reward20= 708   \nn_try = 16  action= 0   reward= 1   reward20= 709   \nn_try = 17  action= 1   reward= 100 reward20= 809   \nn_try = 18  action= 1   reward= 100 reward20= 909   \nn_try = 19  action= 0   reward= 1   reward20= 910   \nn_try = 20  action= 1   reward= 100 reward20= 1010  \nn_try = 21  action= 1   reward= 100 reward20= 1010  \nn_try = 22  action= 0   reward= 1   reward20= 1010  \nn_try = 23  action= 0   reward= 1   reward20= 1010  \nn_try = 24  action= 1   reward= 100 reward20= 1109  \nn_try = 25  action= 1   reward= 100 reward20= 1109  \nn_try = 26  action= 0   reward= 1   reward20= 1010  \nn_try = 27  action= 1   reward= 100 reward20= 1010  \nn_try = 28  action= 1   reward= 100 reward20= 1109  \nn_try = 29  action= 1   reward= 100 reward20= 1109  \nn_try = 30  action= 1   reward= 100 reward20= 1109  \nn_try = 31  action= 0   reward= 1   reward20= 1109  \nn_try = 32  action= 0   reward= 1   reward20= 1109  \nn_try = 33  action= 0   reward= 1   reward20= 1010  \nn_try = 34  action= 0   reward= 1   reward20= 1010  \nn_try = 35  action= 1   reward= 100 reward20= 1109  \nn_try = 36  action= 0   reward= 1   reward20= 1109  \nn_try = 37  action= 1   reward= 100 reward20= 1109  \nn_try = 38  action= 1   reward= 100 reward20= 1109  \nn_try = 39  action= 0   reward= 1   reward20= 1109  \nn_try = 40  action= 1   reward= 100 reward20= 1109  \nn_try = 41  action= 1   reward= 100 reward20= 1109  \nn_try = 42  action= 1   reward= 100 reward20= 1208  \nn_try = 43  action= 1   reward= 100 reward20= 1307  \nn_try = 44  action= 1   reward= 100 reward20= 1307  \nn_try = 45  action= 1   reward= 100 reward20= 1307  \nn_try = 46  action= 1   reward= 100 reward20= 1406  \nn_try = 47  action= 1   reward= 100 reward20= 1406  \nn_try = 48  action= 1   reward= 100 reward20= 1406  \nn_try = 49  action= 1   reward= 100 reward20= 1406  \nn_try = 50  action= 1   reward= 100 reward20= 1406  \n\n\n- 깨달은자 – 게임클리어\n\naction_space = [0,1]\nrewards = [] \nfor t in range(50): # 10000번을 해도 못깸 \n    #action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n    action = 1\n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 1   reward= 100 reward20= 200   \nn_try = 3   action= 1   reward= 100 reward20= 300   \nn_try = 4   action= 1   reward= 100 reward20= 400   \nn_try = 5   action= 1   reward= 100 reward20= 500   \nn_try = 6   action= 1   reward= 100 reward20= 600   \nn_try = 7   action= 1   reward= 100 reward20= 700   \nn_try = 8   action= 1   reward= 100 reward20= 800   \nn_try = 9   action= 1   reward= 100 reward20= 900   \nn_try = 10  action= 1   reward= 100 reward20= 1000  \nn_try = 11  action= 1   reward= 100 reward20= 1100  \nn_try = 12  action= 1   reward= 100 reward20= 1200  \nn_try = 13  action= 1   reward= 100 reward20= 1300  \nn_try = 14  action= 1   reward= 100 reward20= 1400  \nn_try = 15  action= 1   reward= 100 reward20= 1500  \nn_try = 16  action= 1   reward= 100 reward20= 1600  \nn_try = 17  action= 1   reward= 100 reward20= 1700  \nn_try = 18  action= 1   reward= 100 reward20= 1800  \nn_try = 19  action= 1   reward= 100 reward20= 1900  \n\n\n\n\n수정1: action_space의 수정\n\naction_space = gym.spaces.Discrete(2)\naction_space\n\nDiscrete(2)\n\n\n- 좋은점1: sample\n\nfor _ in range(10):\n    print(action_space.sample())\n\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\n- 좋은점2: in\n\n0 in action_space # 유효한 액션을 검사 -- 0은 유효한 액션\n\nTrue\n\n\n\n1 in action_space # 유효한 액션을 검사 -- 1은 유효한 액션 \n\nTrue\n\n\n\n2 in action_space # 유효한 액션을 검사 -- 2는 유효하지 않은 액션 \n\nFalse\n\n\n- 코드 1차수정\n\naction_space = gym.spaces.Discrete(2) \nrewards = [] \nfor t in range(50): \n    action = action_space.sample()\n    #action = 1\n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 \nn_try = 2   action= 0   reward= 1   reward20= 2 \nn_try = 3   action= 1   reward= 100 reward20= 102   \nn_try = 4   action= 1   reward= 100 reward20= 202   \nn_try = 5   action= 1   reward= 100 reward20= 302   \nn_try = 6   action= 0   reward= 1   reward20= 303   \nn_try = 7   action= 0   reward= 1   reward20= 304   \nn_try = 8   action= 0   reward= 1   reward20= 305   \nn_try = 9   action= 0   reward= 1   reward20= 306   \nn_try = 10  action= 1   reward= 100 reward20= 406   \nn_try = 11  action= 0   reward= 1   reward20= 407   \nn_try = 12  action= 0   reward= 1   reward20= 408   \nn_try = 13  action= 0   reward= 1   reward20= 409   \nn_try = 14  action= 1   reward= 100 reward20= 509   \nn_try = 15  action= 1   reward= 100 reward20= 609   \nn_try = 16  action= 0   reward= 1   reward20= 610   \nn_try = 17  action= 1   reward= 100 reward20= 710   \nn_try = 18  action= 0   reward= 1   reward20= 711   \nn_try = 19  action= 0   reward= 1   reward20= 712   \nn_try = 20  action= 1   reward= 100 reward20= 812   \nn_try = 21  action= 1   reward= 100 reward20= 911   \nn_try = 22  action= 0   reward= 1   reward20= 911   \nn_try = 23  action= 0   reward= 1   reward20= 812   \nn_try = 24  action= 0   reward= 1   reward20= 713   \nn_try = 25  action= 0   reward= 1   reward20= 614   \nn_try = 26  action= 0   reward= 1   reward20= 614   \nn_try = 27  action= 0   reward= 1   reward20= 614   \nn_try = 28  action= 0   reward= 1   reward20= 614   \nn_try = 29  action= 0   reward= 1   reward20= 614   \nn_try = 30  action= 0   reward= 1   reward20= 515   \nn_try = 31  action= 1   reward= 100 reward20= 614   \nn_try = 32  action= 1   reward= 100 reward20= 713   \nn_try = 33  action= 0   reward= 1   reward20= 713   \nn_try = 34  action= 1   reward= 100 reward20= 713   \nn_try = 35  action= 1   reward= 100 reward20= 713   \nn_try = 36  action= 0   reward= 1   reward20= 713   \nn_try = 37  action= 1   reward= 100 reward20= 713   \nn_try = 38  action= 0   reward= 1   reward20= 713   \nn_try = 39  action= 1   reward= 100 reward20= 812   \nn_try = 40  action= 0   reward= 1   reward20= 713   \nn_try = 41  action= 1   reward= 100 reward20= 713   \nn_try = 42  action= 0   reward= 1   reward20= 713   \nn_try = 43  action= 1   reward= 100 reward20= 812   \nn_try = 44  action= 1   reward= 100 reward20= 911   \nn_try = 45  action= 1   reward= 100 reward20= 1010  \nn_try = 46  action= 1   reward= 100 reward20= 1109  \nn_try = 47  action= 1   reward= 100 reward20= 1208  \nn_try = 48  action= 0   reward= 1   reward20= 1208  \nn_try = 49  action= 0   reward= 1   reward20= 1208  \nn_try = 50  action= 0   reward= 1   reward20= 1208  \n\n\n\n\n수정2: Env 클래스\n- env 클래스 선언\n\nclass Bandit: \n    def step(self, action):\n        if action == 0:\n            return 1 \n        else: \n            return 100 \n\n\naction_space = gym.spaces.Discrete(2) \nenv = Bandit()\nrewards = []\nfor t in range(50): \n    #action = action_space.sample()\n    action = 1\n    reward = env.step(action)\n    rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 1   reward= 100 reward20= 200   \nn_try = 3   action= 1   reward= 100 reward20= 300   \nn_try = 4   action= 1   reward= 100 reward20= 400   \nn_try = 5   action= 1   reward= 100 reward20= 500   \nn_try = 6   action= 1   reward= 100 reward20= 600   \nn_try = 7   action= 1   reward= 100 reward20= 700   \nn_try = 8   action= 1   reward= 100 reward20= 800   \nn_try = 9   action= 1   reward= 100 reward20= 900   \nn_try = 10  action= 1   reward= 100 reward20= 1000  \nn_try = 11  action= 1   reward= 100 reward20= 1100  \nn_try = 12  action= 1   reward= 100 reward20= 1200  \nn_try = 13  action= 1   reward= 100 reward20= 1300  \nn_try = 14  action= 1   reward= 100 reward20= 1400  \nn_try = 15  action= 1   reward= 100 reward20= 1500  \nn_try = 16  action= 1   reward= 100 reward20= 1600  \nn_try = 17  action= 1   reward= 100 reward20= 1700  \nn_try = 18  action= 1   reward= 100 reward20= 1800  \nn_try = 19  action= 1   reward= 100 reward20= 1900  \n\n\n\n\n수정3: Agnet 클래스\n- Agent 클래스를 만들자. (액션을 하고, 환경에서 받은 reward를 간직)\n\nclass Agent1:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2) \n        self.action = None \n        self.reward = None \n        self.actions = [] \n        self.rewards = []\n    def act(self):\n        self.action = self.action_space.sample() # 무지한자 \n        #self.action = 1 # 깨달은 자\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n\n— 대충 아래와 같은 느낌으로 코드가 돌아가요 —\n시점0: init\n\nenv = Bandit()\nagent = Agent1() \n\n\nagent.action, agent.reward\n\n(None, None)\n\n\n시점1: agent &gt;&gt; env\n\nagent.act()\n\n\nagent.action, agent.reward\n\n(0, None)\n\n\n\nenv.agent_action = agent.action\n\n시점2: agent &lt;&lt; env\n\nagent.reward = env.step(env.agent_action)\n\n\nagent.action, agent.reward, env.agent_action\n\n(0, 1, 0)\n\n\n\nagent.actions,agent.rewards\n\n([], [])\n\n\n\nagent.save_experience()\n\n\nagent.actions,agent.rewards\n\n([0], [1])\n\n\n– 전체코드 –\n\nenv = Bandit() \nagent = Agent1()\nfor t in range(50): \n    ## 1. main 코드 \n    # step1: agent &gt;&gt; env \n    agent.act() \n    env.agent_action = agent.action\n    # step2: agent &lt;&lt; env \n    agent.reward = env.step(env.agent_action)\n    agent.save_experience() \n\n    ## 2. 비본질적 코드 \n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {agent.action}\\t\"\n        f\"reward= {agent.reward}\\t\"\n        f\"reward20= {sum(agent.rewards[-20:])}\\t\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 \nn_try = 2   action= 1   reward= 100 reward20= 101   \nn_try = 3   action= 1   reward= 100 reward20= 201   \nn_try = 4   action= 0   reward= 1   reward20= 202   \nn_try = 5   action= 1   reward= 100 reward20= 302   \nn_try = 6   action= 0   reward= 1   reward20= 303   \nn_try = 7   action= 0   reward= 1   reward20= 304   \nn_try = 8   action= 1   reward= 100 reward20= 404   \nn_try = 9   action= 0   reward= 1   reward20= 405   \nn_try = 10  action= 0   reward= 1   reward20= 406   \nn_try = 11  action= 1   reward= 100 reward20= 506   \nn_try = 12  action= 0   reward= 1   reward20= 507   \nn_try = 13  action= 1   reward= 100 reward20= 607   \nn_try = 14  action= 1   reward= 100 reward20= 707   \nn_try = 15  action= 1   reward= 100 reward20= 807   \nn_try = 16  action= 1   reward= 100 reward20= 907   \nn_try = 17  action= 1   reward= 100 reward20= 1007  \nn_try = 18  action= 1   reward= 100 reward20= 1107  \nn_try = 19  action= 0   reward= 1   reward20= 1108  \nn_try = 20  action= 1   reward= 100 reward20= 1208  \nn_try = 21  action= 0   reward= 1   reward20= 1208  \nn_try = 22  action= 0   reward= 1   reward20= 1109  \nn_try = 23  action= 0   reward= 1   reward20= 1010  \nn_try = 24  action= 1   reward= 100 reward20= 1109  \nn_try = 25  action= 0   reward= 1   reward20= 1010  \nn_try = 26  action= 0   reward= 1   reward20= 1010  \nn_try = 27  action= 1   reward= 100 reward20= 1109  \nn_try = 28  action= 0   reward= 1   reward20= 1010  \nn_try = 29  action= 0   reward= 1   reward20= 1010  \nn_try = 30  action= 1   reward= 100 reward20= 1109  \nn_try = 31  action= 1   reward= 100 reward20= 1109  \nn_try = 32  action= 1   reward= 100 reward20= 1208  \nn_try = 33  action= 1   reward= 100 reward20= 1208  \nn_try = 34  action= 1   reward= 100 reward20= 1208  \nn_try = 35  action= 0   reward= 1   reward20= 1109  \nn_try = 36  action= 0   reward= 1   reward20= 1010  \nn_try = 37  action= 1   reward= 100 reward20= 1010  \nn_try = 38  action= 1   reward= 100 reward20= 1010  \nn_try = 39  action= 0   reward= 1   reward20= 1010  \nn_try = 40  action= 1   reward= 100 reward20= 1010  \nn_try = 41  action= 0   reward= 1   reward20= 1010  \nn_try = 42  action= 0   reward= 1   reward20= 1010  \nn_try = 43  action= 1   reward= 100 reward20= 1109  \nn_try = 44  action= 1   reward= 100 reward20= 1109  \nn_try = 45  action= 0   reward= 1   reward20= 1109  \nn_try = 46  action= 0   reward= 1   reward20= 1109  \nn_try = 47  action= 1   reward= 100 reward20= 1109  \nn_try = 48  action= 0   reward= 1   reward20= 1109  \nn_try = 49  action= 0   reward= 1   reward20= 1109  \nn_try = 50  action= 1   reward= 100 reward20= 1109  \n\n\n\n\n수정4: 학습과정을 포함\n- Game1에 대한 생각:\n\n사실 강화학습은 “환경을 이해 \\(\\to\\) 행동을 결정” 의 과정에서 \\(\\to\\)의 과정을 수식화 한 것이다.\n그런데 지금까지 했던 코드는 환경(env)를 이해하는 순간 에이전트가 최적의 행동(action)2을 직관적으로 결정하였으므로 기계가 스스로 학습을 했다고 볼 수 없다.\n\n2 button1을 누른다- 지금까지의 코드 복습\n\n클래스를 선언하는 부분\n\nEnv 클래스의 선언\nAgent 클래스의 선언\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하여 게임을 진행\n\n메인코드: (1) agent \\(\\to\\) env (2) agent \\(\\leftarrow\\) env\n비본질적코드: 학습과정을 display, 학습의 종료조건체크\n\n\n- 앞으로 구성할 코드의 형태: 에이전트가 데이터를 보고 스스로 button1을 눌러야 한다는 생각을 했으면 좋겠음.\n\n클래스를 선언하는 부분\n\nEnv 클래스의 선언\nAgent 클래스의 선언 // &lt;—- 학습의 과정이 포함되어야 한다, act함수의 수정, learn함수의 추가\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하여 게임을 진행\n\n메인코드 (1) agent \\(\\to\\) env (2) agent \\(\\leftarrow\\) env // &lt;—- agent가 데이터를 분석하고 학습하는 과정이 추가\n비본질적코드: 학습과정을 display, 학습의 종료조건체크\n\n\n- 에이전트가 학습을 어떻게 하는가? 아래와 같이 버튼을 누르도록 한다면\n\n버튼0을 누를 확률: \\(\\frac{q_0}{q_0+q_1}\\)\n버튼1을 누를 확률: \\(\\frac{q_1}{q_0+q_1}\\)\n\n시간이 지날수록 버튼1을 주로 누를 것이다.\n- 걱정: \\(t=0\\) 이면 어쩌지? \\(t=1\\)이면 어쩌지?… \\(\\to\\) 해결책: 일정시간동안 랜덤액션을 하면서 데이터를 쌓고 그 뒤에 \\(q_0,q_1\\)을 계산\n- 쌓은 데이터를 바탕으로 환경을 이해하고 action을 뽑는 코드\n\nagent.actions = [0,1,1,0,1,0,0] \nagent.rewards = [1,101,102,1,99,1,1.2] \nactions = np.array(agent.actions)\nrewards = np.array(agent.rewards)\n\n\nq0 = rewards[actions == 0].mean()\nq1 = rewards[actions == 1].mean()\n\n\nagent.q = np.array([q0,q1]) \nagent.q\n\narray([  1.05      , 100.66666667])\n\n\n\nprob = agent.q / agent.q.sum()\nprob \n\narray([0.01032279, 0.98967721])\n\n\n\naction = np.random.choice([0,1], p= agent.q / agent.q.sum())\naction\n\n1\n\n\n- 최종코드정리\n\nclass Bandit: \n    def step(self, action):\n        if action == 0:\n            return 1 \n        else: \n            return 100 \nclass Agent:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2) \n        self.action = None \n        self.reward = None \n        self.actions = [] \n        self.rewards = []\n        self.q = np.array([0,0]) \n        self.n_experience = 0 \n    def act(self):\n        if self.n_experience&lt;30: \n            self.action = self.action_space.sample() \n        else: \n            self.action = np.random.choice([0,1], p= self.q / self.q.sum())\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience += 1 \n    def learn(self):\n        if self.n_experience&lt;30: \n            pass \n        else: \n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0 = rewards[actions == 0].mean()\n            q1 = rewards[actions == 1].mean()\n            self.q = np.array([q0,q1]) \n\n\nenv = Bandit() \nagent = Agent()\nfor t in range(50): \n    ## 1. main 코드 \n    # step1: agent &gt;&gt; env \n    agent.act() \n    env.agent_action = agent.action\n    # step2: agent &lt;&lt; env \n    agent.reward = env.step(env.agent_action)\n    agent.save_experience() \n    # step3: learn \n    agent.learn()\n    ## 2. 비본질적 코드 \n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {agent.action}\\t\"\n        f\"reward= {agent.reward}\\t\"\n        f\"reward20= {sum(agent.rewards[-20:])}\\t\"\n        f\"q = {agent.q}\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 q = [0 0]\nn_try = 2   action= 0   reward= 1   reward20= 2 q = [0 0]\nn_try = 3   action= 1   reward= 100 reward20= 102   q = [0 0]\nn_try = 4   action= 0   reward= 1   reward20= 103   q = [0 0]\nn_try = 5   action= 0   reward= 1   reward20= 104   q = [0 0]\nn_try = 6   action= 0   reward= 1   reward20= 105   q = [0 0]\nn_try = 7   action= 0   reward= 1   reward20= 106   q = [0 0]\nn_try = 8   action= 0   reward= 1   reward20= 107   q = [0 0]\nn_try = 9   action= 1   reward= 100 reward20= 207   q = [0 0]\nn_try = 10  action= 0   reward= 1   reward20= 208   q = [0 0]\nn_try = 11  action= 1   reward= 100 reward20= 308   q = [0 0]\nn_try = 12  action= 1   reward= 100 reward20= 408   q = [0 0]\nn_try = 13  action= 1   reward= 100 reward20= 508   q = [0 0]\nn_try = 14  action= 0   reward= 1   reward20= 509   q = [0 0]\nn_try = 15  action= 1   reward= 100 reward20= 609   q = [0 0]\nn_try = 16  action= 0   reward= 1   reward20= 610   q = [0 0]\nn_try = 17  action= 0   reward= 1   reward20= 611   q = [0 0]\nn_try = 18  action= 0   reward= 1   reward20= 612   q = [0 0]\nn_try = 19  action= 1   reward= 100 reward20= 712   q = [0 0]\nn_try = 20  action= 1   reward= 100 reward20= 812   q = [0 0]\nn_try = 21  action= 0   reward= 1   reward20= 812   q = [0 0]\nn_try = 22  action= 1   reward= 100 reward20= 911   q = [0 0]\nn_try = 23  action= 0   reward= 1   reward20= 812   q = [0 0]\nn_try = 24  action= 1   reward= 100 reward20= 911   q = [0 0]\nn_try = 25  action= 1   reward= 100 reward20= 1010  q = [0 0]\nn_try = 26  action= 1   reward= 100 reward20= 1109  q = [0 0]\nn_try = 27  action= 1   reward= 100 reward20= 1208  q = [0 0]\nn_try = 28  action= 1   reward= 100 reward20= 1307  q = [0 0]\nn_try = 29  action= 1   reward= 100 reward20= 1307  q = [0 0]\nn_try = 30  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 31  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 32  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 33  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 34  action= 1   reward= 100 reward20= 1505  q = [  1. 100.]\nn_try = 35  action= 1   reward= 100 reward20= 1505  q = [  1. 100.]\nn_try = 36  action= 1   reward= 100 reward20= 1604  q = [  1. 100.]\nn_try = 37  action= 1   reward= 100 reward20= 1703  q = [  1. 100.]\nn_try = 38  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 39  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 40  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 41  action= 1   reward= 100 reward20= 1901  q = [  1. 100.]"
  },
  {
    "objectID": "posts/4. 강화학습/A3.html",
    "href": "posts/4. 강화학습/A3.html",
    "title": "A3: 강화학습 (3) – LunarLander",
    "section": "",
    "text": "강의영상\n\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch\nimport collections\nimport IPython\n\n\n\n예비학습\n- collections.deque 의 기능\n\na = collections.deque([1,2,3], maxlen = 5 )\na\n\ndeque([1, 2, 3], maxlen=5)\n\n\n\na.append(4)\na\n\ndeque([1, 2, 3, 4], maxlen=5)\n\n\n\na.append(5)\na\n\ndeque([1, 2, 3, 4, 5], maxlen=5)\n\n\n\na.append(6)\na\n\ndeque([2, 3, 4, 5, 6], maxlen=5)\n\n\n- 단점? numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\na + 1\n\nTypeError: can only concatenate deque (not \"int\") to deque\n\n\n- 그렇지만 필요하다면 np.array 화 시킬 수 있음.\n\nnp.array(a) + 1\n\narray([3, 4, 5, 6, 7])\n\n\n- collection.deque 는 리플레이 버퍼를 구현할때 유용한 자료구조이다.\n\n(우리가 했던) 기존방식: 모든 데이터를 저장하며 하나의 경험씩 학습함\n리플레이버퍼: 최근 \\(N\\)개의 데이터를 저장하여 여러경험을 샘플링하여 학습하는 방식\n리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을 저장하고 무작위로 재사용하여 학습이 안정적으로 된다, “저장 -&gt; 학습 -&gt; 저장” 순으로 반드시 실시간으로 학습할 필요가 없어서 병렬처리에 용이하다, 강화학습에서 연속된 경험은 상관관계가 있을 수 있는데 무작위 샘플로 이러한 상관관계를 제거할 수 있음\n\n\n\nGame3: LunarLander\n- 환경생성\n\nenv = gym.make('LunarLander-v2', render_mode = 'rgb_array') \nenv \n\n&lt;TimeLimit&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;LunarLander&lt;LunarLander-v2&gt;&gt;&gt;&gt;&gt;\n\n\n- state_space\n\nenv.observation_space\n\nBox([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n 1.       ], (8,), float32)\n\n\n- action_space\n\nenv.action_space\n\nDiscrete(4)\n\n\n- env.reset()\n\nstate, _ = env.reset()\nstate \n\narray([-0.004881  ,  1.4137907 , -0.49441272,  0.12757756,  0.00566272,\n        0.11199194,  0.        ,  0.        ], dtype=float32)\n\n\n- env.render()\n\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5477bb10&gt;\n\n\n\n\n\n- env.step\n\nnext_state, reward, terminated, _, _ = env.step(0)\nnext_state, reward, terminated\n\n(array([-0.00976257,  1.4160839 , -0.4937438 ,  0.10189002,  0.01119673,\n         0.11069117,  0.        ,  0.        ], dtype=float32),\n -0.13923681373518093,\n False)\n\n\n- play\n\nenv.reset()\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5349a410&gt;\n\n\n\n\n\n\nfor _ in range(7):\n    env.step(3)\n    env.step(2)\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a534ea410&gt;\n\n\n\n\n\n\n0 : 아무행동도 하지 않음\n1 : 왼쪽\n2 : 위\n3 : 오른쪽\n\n\n\n시각화\n\ndef show(ims,jump=10):\n    ims = ims[::jump]\n    fig = plt.Figure()\n    ax = fig.subplots()\n    def update(i):\n       ax.imshow(ims[i])\n    ani = FuncAnimation(fig,update,frames=len(ims))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\ncurrent_state, _ = env.reset()\nims = [] \nfor t in range(500): \n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n    im = env.render()\n    ims.append(im) \n    current_state = next_state \n    if terminated: break \n\n\nshow(ims) \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nq_net\n- 원래는 agent.q 에 해당하는 것인데, 이전에서는 agent.q를 (4,4,4) shape의 numpy array 를 사용했는데 여기서는 불가능\n\n4x4 grid: 상태공간의 차원은 2차원이며 가질수 있는 값은 16개, 각 상태공간에서 할수 있는 행동이 4개 -&gt; 총 16*4의 경우의 수에 대한 reward만 조사하면 되었음\nLunarLander: 상태공간의 차원은 8차원이지만 가질수 있는 값의 범위는 무한대 -&gt; 무수히 많은 경우에 대한 reward 값을 조사하는건 현실적으로 불가능\n\n- 데이터를 모아보자.\n\ncurrent_states = collections.deque(maxlen=50) \nactions = collections.deque(maxlen=50) \nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50) \n\ncurrent_state, _ = env.reset()\nfor t in range(500): \n    ## step1: agent &gt;&gt; env \n    action = env.action_space.sample()\n    ## step2:agent &lt;&lt; env \n    next_state, reward, terminated, _, _ = env.step(action)\n    current_states.append(current_state)\n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward)\n    terminations.append(terminated) \n    ## step3: learn \n    ## step4: update state     \n    current_state = next_state \n    ## step5: 종료조건체크 \n    if terminated: break \n\n- 이전코드에서 아래에 대응하는 부분을 구현하면 된다.\n## 1. q[x,y,a]를 초기화: q(s)를 넣으면 action에 대한 q값을 알려주는 기능 \nagent.q = np.zeros([4,4,4]) \n\n## 2. q_estimated 를 계산 \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = agent.reward + 0.99 * q_future 를 수행하는 과정 \nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff \n1. agent.q 에 대응하는 과정\n\nq_net = torch.nn.Sequential(\n    torch.nn.Linear(8,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,4)\n)\n\n\nq_net # &lt;- 8개의 숫자가 입력으로 오면 4개의 숫자를 리턴하는 함수 \n\nSequential(\n  (0): Linear(in_features=8, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=32, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=32, out_features=4, bias=True)\n)\n\n\n\nq_net(torch.tensor(current_state))\n\ntensor([-0.0863, -0.0824, -0.1490,  0.0031], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nq_net은 8개의 숫자가 입력으로 오면 4개의 숫자가 리턴되는 함수이다.\n해석을 하면 8개의 숫자는 state를 나타내는 숫자로 이해할 수 있고 4개의 숫자는 각 action에 대한 q값으로 해석할 수 있다.\n하지만 이 숫자가 합리적인건 아님 (아무숫자임)\nq_net의 특징: 고정된 함수가 아니고 데이터를 이용하여 점점 더 그럴듯한 숫자를 뱉어내도록 학습할 수 있는 함수이다. (뉴럴네트워크)\n\n1. agent.q 에 대응하는 과정 (배치버전)\n– get batch –\n\nbatch_size = 4 \nidx = np.random.randint(0,50,size=batch_size)\n\ncurrent_states_batch = torch.tensor(np.array(current_states))[idx].float()\nactions_batch = torch.tensor(np.array(actions))[idx].reshape(batch_size,-1) \nrewards_batch = torch.tensor(np.array(rewards))[idx].reshape(batch_size,-1).float()\nnext_states_batch = torch.tensor(np.array(next_states))[idx].float()\nterminations_batch = torch.tensor(np.array(terminations))[idx].reshape(batch_size,-1)\n\n– q_net –\n\ncurrent_states_batch\n\ntensor([[-0.5863,  0.7144, -0.7831, -1.1050,  0.0357, -0.0844,  0.0000,  0.0000],\n        [-0.4805,  1.0306, -0.7311, -0.8693,  0.1304, -0.1544,  0.0000,  0.0000],\n        [-0.6180,  0.6100, -0.7990, -1.1882,  0.0206, -0.0456,  0.0000,  0.0000],\n        [-0.6100,  0.6367, -0.7883, -1.1610,  0.0229, -0.0884,  0.0000,  0.0000]])\n\n\n\nq_net(current_states_batch)\n\ntensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n        [-0.1009, -0.1039, -0.0828,  0.0529],\n        [-0.0953, -0.0925, -0.0947,  0.0437],\n        [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n2. q_estimated\n\nq_net(current_states_batch), actions_batch\n\n(tensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n         [-0.1009, -0.1039, -0.0828,  0.0529],\n         [-0.0953, -0.0925, -0.0947,  0.0437],\n         [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[0],\n         [1],\n         [3],\n         [1]]))\n\n\n\nq_net(current_states_batch).gather(1,actions_batch)\n\ntensor([[-0.0974],\n        [-0.1039],\n        [ 0.0437],\n        [-0.0925]], grad_fn=&lt;GatherBackward0&gt;)\n\n\n3. q_realistic = agent.reward + 0.99 * q_future\n– q_future –\n\nq_future = q_net(next_states_batch).max(axis=1)[0].reshape(batch_size,1)\nq_future\n\ntensor([[0.0461],\n        [0.0538],\n        [0.0421],\n        [0.0437]], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\n\nq_realistic = rewards_batch + 0.99 * q_future * (~terminations_batch)\n\n4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야합니다.. \noptimizer = torch.optim.Adam(q_net.parameters(),lr=0.0001) \nfor _ in range(2000):\n    ~~~\n    ~~~\n    q_estimated = ~~~ \n    q_realistic = ~~~ \n    loss = torch.nn.functional.mse_loss(q_estimated,q_realistic)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\npolicy\n\neps = 0.5 \nif np.random.rand() &lt; eps:\n    action = env.action_space.sample() \nelse:\n    action = q_net(torch.tensor(current_state)).argmax().item()\n\n\naction\n\n3\n\n\n\n\nAgent 클래스 + run\n\nclass Agent():\n    def __init__(self,env):\n        self.eps = 0\n        self.n_experiences = 0\n        self.n_episode = 0\n        self.score = 0\n        self.scores = []\n        self.playtimes = []\n        self.batch_size = 64\n        self.buffer_size = 5000 \n        self.action_space = env.action_space\n        #self.state_space = env.observation_space\n\n        # Q-Network\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,128), \n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,4)\n        ) \n        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n\n        # ReplayBuffer\n        self.current_states = collections.deque(maxlen=self.buffer_size)\n        self.actions = collections.deque(maxlen=self.buffer_size)\n        self.rewards = collections.deque(maxlen=self.buffer_size)\n        self.next_states = collections.deque(maxlen=self.buffer_size)\n        self.terminations = collections.deque(maxlen=self.buffer_size)\n       \n    def save_experience(self):\n        \"\"\"Add a new experience to memory.\"\"\"\n        self.current_states.append(self.current_state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences = self.n_experiences+1\n        self.score += self.reward\n    \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else:\n            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n            \n    def get_batch(self):\n        idx = np.random.randint(0,self.buffer_size,size=self.batch_size) \n        self.current_states_batch = torch.tensor(np.array(self.current_states))[idx].float()\n        self.actions_batch = torch.tensor(np.array(self.actions))[idx].reshape(self.batch_size,1)\n        self.rewards_batch = torch.tensor(np.array(self.rewards))[idx].reshape(self.batch_size,-1).float()\n        self.next_states_batch = torch.tensor(np.array(self.next_states))[idx].float()\n        self.terminations_batch = torch.tensor(np.array(self.terminations))[idx].reshape(self.batch_size,-1) \n    \n    def learn(self):\n        if self.n_experiences &lt; self.buffer_size:\n            pass\n        else: \n            self.get_batch()\n            q_estimated = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n            q_future = self.q_net(self.next_states_batch).detach().max(1)[0].reshape(self.batch_size,1)\n            q_realistic = self.rewards_batch + 0.99 * q_future * (~self.terminations_batch)\n\n            loss = torch.nn.functional.mse_loss(q_estimated, q_realistic)\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\nagent = Agent(env)\nagent.eps = 1.0 \nfor _ in range(2000):\n    ### 1. 본질적인 코드\n    agent.current_state, _  = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(500):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated, _,_ = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps*0.995\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 10) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_eps: {agent.eps}\\t'\n            f'n_experiences: {agent.n_experiences}\\t'\n        )\n    if np.mean(agent.scores[-100:])&gt;=200.0:\n        break\n\nEpisode 10  Score: -213.18  Playtime:  92.70    n_eps: 0.9511101304657719   n_experiences: 927  \nEpisode 20  Score: -204.70  Playtime:  99.50    n_eps: 0.9046104802746175   n_experiences: 1990 \nEpisode 30  Score: -211.72  Playtime:  104.50   n_eps: 0.8603841919146962   n_experiences: 3135 \nEpisode 40  Score: -226.75  Playtime:  105.53   n_eps: 0.8183201210226743   n_experiences: 4221 \nEpisode 50  Score: -208.68  Playtime:  106.34   n_eps: 0.778312557068642    n_experiences: 5317 \nEpisode 60  Score: -197.43  Playtime:  108.47   n_eps: 0.7402609576967045   n_experiences: 6508 \nEpisode 70  Score: -208.33  Playtime:  115.60   n_eps: 0.7040696960536299   n_experiences: 8092 \nEpisode 80  Score: -212.00  Playtime:  117.41   n_eps: 0.6696478204705644   n_experiences: 9393 \nEpisode 90  Score: -208.73  Playtime:  118.74   n_eps: 0.6369088258938781   n_experiences: 10687    \nEpisode 100 Score: -206.49  Playtime:  119.74   n_eps: 0.6057704364907278   n_experiences: 11974    \nEpisode 110 Score: -196.32  Playtime:  123.45   n_eps: 0.5761543988830038   n_experiences: 13272    \nEpisode 120 Score: -184.32  Playtime:  129.69   n_eps: 0.547986285490042    n_experiences: 14959    \nEpisode 130 Score: -172.21  Playtime:  130.03   n_eps: 0.5211953074858876   n_experiences: 16138    \nEpisode 140 Score: -152.05  Playtime:  142.30   n_eps: 0.49571413690105054  n_experiences: 18451    \nEpisode 150 Score: -143.29  Playtime:  146.41   n_eps: 0.47147873742168567  n_experiences: 19958    \nEpisode 160 Score: -132.48  Playtime:  154.47   n_eps: 0.4484282034609769   n_experiences: 21955    \nEpisode 170 Score: -106.66  Playtime:  163.24   n_eps: 0.42650460709830135  n_experiences: 24416    \nEpisode 180 Score: -85.85   Playtime:  180.09   n_eps: 0.40565285250151817  n_experiences: 27402    \nEpisode 190 Score: -73.39   Playtime:  201.63   n_eps: 0.3858205374665315   n_experiences: 30850    \nEpisode 200 Score: -49.93   Playtime:  230.35   n_eps: 0.3669578217261671   n_experiences: 35009    \nEpisode 210 Score: -40.46   Playtime:  263.12   n_eps: 0.34901730169741024  n_experiences: 39584    \nEpisode 220 Score: -31.11   Playtime:  280.75   n_eps: 0.33195389135223546  n_experiences: 43034    \nEpisode 230 Score: -15.80   Playtime:  314.08   n_eps: 0.3157247089126454   n_experiences: 47546    \nEpisode 240 Score: -5.43    Playtime:  333.12   n_eps: 0.30028896908517405  n_experiences: 51763    \nEpisode 250 Score:  4.33    Playtime:  363.03   n_eps: 0.285607880564032    n_experiences: 56261    \nEpisode 260 Score:  10.80   Playtime:  391.83   n_eps: 0.27164454854530906  n_experiences: 61138    \nEpisode 270 Score:  14.88   Playtime:  413.84   n_eps: 0.2583638820072446   n_experiences: 65800    \nEpisode 280 Score:  21.58   Playtime:  432.86   n_eps: 0.2457325055235537   n_experiences: 70688    \nEpisode 290 Score:  31.61   Playtime:  443.43   n_eps: 0.23371867538818816  n_experiences: 75193    \nEpisode 300 Score:  29.04   Playtime:  439.61   n_eps: 0.22229219984074702  n_experiences: 78970    \nEpisode 310 Score:  37.79   Playtime:  443.86   n_eps: 0.21142436319205632  n_experiences: 83970    \nEpisode 320 Score:  43.76   Playtime:  456.21   n_eps: 0.2010878536592394   n_experiences: 88655    \nEpisode 330 Score:  43.98   Playtime:  461.09   n_eps: 0.1912566947289212   n_experiences: 93655    \nEpisode 340 Score:  45.86   Playtime:  468.92   n_eps: 0.18190617987607657  n_experiences: 98655    \nEpisode 350 Score:  50.36   Playtime:  473.94   n_eps: 0.1730128104744653   n_experiences: 103655   \nEpisode 360 Score:  51.94   Playtime:  467.59   n_eps: 0.16455423674261854  n_experiences: 107897   \nEpisode 370 Score:  51.95   Playtime:  467.90   n_eps: 0.15650920157696743  n_experiences: 112590   \nEpisode 380 Score:  55.56   Playtime:  469.02   n_eps: 0.14885748713096328  n_experiences: 117590   \nEpisode 390 Score:  62.16   Playtime:  473.97   n_eps: 0.14157986400593744  n_experiences: 122590   \nEpisode 400 Score:  71.21   Playtime:  485.75   n_eps: 0.1346580429260134   n_experiences: 127545   \nEpisode 410 Score:  82.20   Playtime:  479.03   n_eps: 0.12807462877562611  n_experiences: 131873   \nEpisode 420 Score:  96.33   Playtime:  473.30   n_eps: 0.12181307688414106  n_experiences: 135985   \nEpisode 430 Score:  115.43  Playtime:  466.74   n_eps: 0.11585765144771248  n_experiences: 140329   \nEpisode 440 Score:  118.52  Playtime:  463.05   n_eps: 0.11019338598389174  n_experiences: 144960   \nEpisode 450 Score:  117.20  Playtime:  463.05   n_eps: 0.10480604571960442  n_experiences: 149960   \nEpisode 460 Score:  135.12  Playtime:  460.61   n_eps: 0.0996820918179746   n_experiences: 153958   \nEpisode 470 Score:  156.37  Playtime:  447.12   n_eps: 0.09480864735409487  n_experiences: 157302   \nEpisode 480 Score:  175.40  Playtime:  433.66   n_eps: 0.09017346495423652  n_experiences: 160956   \nEpisode 490 Score:  191.98  Playtime:  417.28   n_eps: 0.08576489601717459  n_experiences: 164318   \n\n\n- 시각화를 위한코드\n\nagent2 = Agent(env) \nagent2.q_net = agent.q_net\n\nagent2.current_state, _ = env.reset()\nagent2.terminated = False \nims = [] \nims.append(env.render())\nfor t in range(500):\n    agent2.act() \n    agent2.next_state, agent2.reward, agent2.terminated, _, _  = env.step(agent2.action)\n    im = env.render()\n    ims.append(im)\n    agent2.current_state = agent2.next_state\n    if agent2.terminated: break \n\n\nshow(ims)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html",
    "title": "15wk-1: MCMC (3)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport copy"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#단계1-아무값이나-넣어서-boldsymbol-theta_0를-초기화한다.",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#단계1-아무값이나-넣어서-boldsymbol-theta_0를-초기화한다.",
    "title": "15wk-1: MCMC (3)",
    "section": "단계1: 아무값이나 넣어서 \\({\\boldsymbol \\theta}_0\\)를 초기화한다.",
    "text": "단계1: 아무값이나 넣어서 \\({\\boldsymbol \\theta}_0\\)를 초기화한다.\n- \\({\\boldsymbol \\theta}_0=(\\theta_0[0],\\dots,\\theta_0[5])\\)를 아무값이나 셋팅\n\nθ = {\n    'doc1':[2,1],\n    'doc2':[1,2],\n    'doc3':[2,1]\n}\n\n\n\\({\\boldsymbol \\theta}_0 = [2,1,1,2,2,1]\\) 이라고 생각하자.\n\n- 임의의 초기값이 셋팅된 상황은 아래와 같다.\n\n\nTable 3: 초기상태요약\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 임의의 초기값으로 설정된 토픽\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n2 (=\\(\\theta_0[0]\\))\n1 (=\\(\\theta_0[2]\\))\n2 (=\\(\\theta_0[4]\\))\n\n\n1 (=\\(\\theta_0[1]\\))\n2 (=\\(\\theta_0[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 골, 데이터과학\n\n\n2\n손흥민, 확률, 확률"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#단계2-boldsymbol-theta_00-boldsymbol-theta_01-cdots-boldsymbol-theta_05-을-순서대로-샘플링",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#단계2-boldsymbol-theta_00-boldsymbol-theta_01-cdots-boldsymbol-theta_05-을-순서대로-샘플링",
    "title": "15wk-1: MCMC (3)",
    "section": "단계2: \\({\\boldsymbol \\theta}_0[0], {\\boldsymbol \\theta}_0[1] \\cdots, {\\boldsymbol \\theta}_0[5]\\) 을 순서대로 샘플링",
    "text": "단계2: \\({\\boldsymbol \\theta}_0[0], {\\boldsymbol \\theta}_0[1] \\cdots, {\\boldsymbol \\theta}_0[5]\\) 을 순서대로 샘플링\n- 예비개념: MCMC에서 베타분포를 뽑았던 예제로 돌아가보자\n\n임의의 초기값 \\(x\\) 생성\n새로운 값으로 \\(y\\)를 고려 (추천받을 수도 있고 그냥 고려할수도 있음)\n\\(x\\)가 그럴듯한지, \\(y\\)가 그럴듯한지 판단하고 \\(x'\\)의 값은 어떠한확률로 \\(x,y\\) 중에서 선택\n\n- 전략: 그동안 pmf 혹은 pdf의 정보가 필요했던 것은 “그럴듯한 정도” 를 판단할 기준을 얻기 위해서였다. 그런데 만약, “그럴듯한 정도”를 판단하는 기준을 pmf, pdf 로 정하지 않는다면? pmf 혹은 pdf 가 필요하지 않다.\n\nstage0: \\({\\boldsymbol \\theta}_0[0]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n\n\n\nTable 4: \\(t=0\\), \\(d={\\tt doc1}\\), \\(w={\\tt 손흥민}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 2 (=\\(\\theta_0[0]\\))\n1 (=\\(\\theta_0[2]\\))\n2 (=\\(\\theta_0[4]\\))\n\n\n1 (=\\(\\theta_0[1]\\))\n2 (=\\(\\theta_0[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 골, 데이터과학\n\n\n2\n손흥민, 확률, 확률\n\n\n\n\n\n\n(d) \\(\\theta_0[0]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽1으로 분류되어있음\n나도 토픽1인듯\n\n\n토픽눈치\n토픽1에도, 토픽2에도 나랑 같은 단어는 없음\n난 토픽1도 2도 아닌듯\n\n\n\n\n\n\n- \\(\\theta_0[0]\\)이 토픽1에서 뽑혓다고 보는게 타당한지 토픽1에서 뽑혔다고 보는게 타당하지 아래와 같이 따져보자.\n\n토픽1의 타당성: (doc1에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 손흥민이라는 단어가 포함된 비율) = 1 \\(\\times\\) 0\n토픽2의 타당성: (doc1에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 손흥민이라는 단어가 포함된 비율) = 0 \\(\\times\\) 0\n\n\n둘 다 \\(0\\) 이라서 비긴거야?? 그런데 그래도 토픽1가 그나마 타당한거아냐?\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc1에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 손흥민이라는 단어가 포함된 비율) = 1 \\(\\times\\) 0.001\n토픽2의 타당성: (doc1에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 손흥민이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 0.001\n\n\n\\(\\theta_0[0]\\)의 생각: 나는 토픽1인듯해\n\n- 업데이트\n\nθ['doc1'][0]\n\n2\n\n\n\nθ['doc1'][0] = 1\n\n\n\nstage1: \\({\\boldsymbol \\theta}_0[1]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n파란색/볼드: 과거 / 업데이트O\n\n\n\nTable 5: \\(t=0\\), \\(d={\\tt doc1}\\), \\(w={\\tt 골}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 1 (=\\(\\theta_1[0]\\))1\n1 (=\\(\\theta_0[2]\\))\n2 (=\\(\\theta_0[4]\\))\n\n\n 1 (=\\(\\theta_0[1]\\))\n2 (=\\(\\theta_0[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n1 원래 2였는데 업데이트되었음\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 골, 데이터과학, 손흥민2\n\n\n2\n확률, 확률\n\n\n\n2 원래 2에 있었은데 1로 옮겼음\n\n\n(d) \\(\\theta_0[1]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽1으로 분류되어있음\n나도 토픽1인듯\n\n\n토픽눈치\n토픽1에는 나랑 같은 단어가 있는데 토픽2에는 없음\n나도 토픽1인듯\n\n\n\n\n\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc1에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 골이라는 단어가 포함된 비율) = 1 \\(\\times\\) 1/3\n토픽2의 타당성: (doc1에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 골이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 0.001\n\n\n\\(\\theta_0[1]\\)의 생각: 나는 토픽1인듯해\n\n- 업데이트: 안함.. 난 토픽1이 맞는것 같음\n\n\nstage2: \\({\\boldsymbol \\theta}_0[2]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n파란색/볼드: 과거 / 업데이트O\n파란색/볼드X: 과거 / 업데이트X\n\n\n\nTable 6: \\(t=0\\), \\(d={\\tt doc2}\\), \\(w={\\tt 골}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 1 (=\\(\\theta_1[0]\\))3\n 1 (=\\(\\theta_0[2]\\)) \n2 (=\\(\\theta_0[4]\\))\n\n\n 1 (=\\(\\theta_1[1]\\))\n2 (=\\(\\theta_0[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n3 원래 2였는데 업데이트되었음\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 골, 데이터과학, 손흥민4\n\n\n2\n확률, 확률\n\n\n\n4 원래 2에 있었은데 1로 옮겼음\n\n\n(d) \\(\\theta_0[2]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽2로 분류되어있음\n나도 토픽2인듯\n\n\n토픽눈치\n토픽1에는 나랑 같은 단어가 있는데 토픽2에는 없음\n나도 토픽1인듯\n\n\n\n\n\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc2에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 골이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 1/3\n토픽2의 타당성: (doc2에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 골이라는 단어가 포함된 비율) = 1 \\(\\times\\) 0.001\n\n\n\\(\\theta_0[2]\\)의 생각: 나는 토픽2인듯함 (그런데 아닐 수도 있음)\n\n- 업데이트\n\nθ['doc2'][0]\n\n1\n\n\n\nθ['doc2'][0] = 2\n\n\n\nstage3: \\({\\boldsymbol \\theta}_0[3]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n파란색/볼드: 과거 / 업데이트O\n파란색/볼드X: 과거 / 업데이트X\n\n\n\nTable 7: \\(t=0\\), \\(d={\\tt doc2}\\), \\(w={\\tt 확률}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 1 (=\\(\\theta_1[0]\\))5\n 2 (=\\(\\theta_1[2]\\)) 6\n2 (=\\(\\theta_0[4]\\))\n\n\n 1 (=\\(\\theta_1[1]\\))\n 2 (=\\(\\theta_0[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n5 원래 2였는데 업데이트되었음6 원래 1이었는데 업데이트되었음, 업데이트 안될수도 있었음.\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 데이터과학, 손흥민7\n\n\n2\n확률, 확률, 골8\n\n\n\n7 원래 2에 있었은데 1로 옮겼음8 원래 1에 있었은데 2로 옮겼음, 안바뀔수도 있었음.\n\n\n(d) \\(\\theta_0[3]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽2로 분류되어있음\n나도 토픽2인듯\n\n\n토픽눈치\n토픽1에는 나랑 같은 단어가 없는데 토픽2에는 있음\n나도 토픽2인듯\n\n\n\n\n\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc2에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 확률이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 0.001\n토픽2의 타당성: (doc2에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 확률이라는 단어가 포함된 비율) = 1 \\(\\times\\) 1/2\n\n\n\\(\\theta_0[3]\\)의 생각: 나는 토픽2인듯함\n\n- 업데이트: 안함. 난 토픽2가 확실한듯\n\n\nstage4: \\({\\boldsymbol \\theta}_0[4]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n파란색/볼드: 과거 / 업데이트O\n파란색/볼드X: 과거 / 업데이트X\n\n\n\nTable 8: \\(t=0\\), \\(d={\\tt doc3}\\), \\(w={\\tt 확률}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 1 (=\\(\\theta_1[0]\\))9\n 2 (=\\(\\theta_1[2]\\)) 10\n2 (=\\(\\theta_0[4]\\))\n\n\n 1 (=\\(\\theta_1[1]\\))\n 2 (=\\(\\theta_1[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n9 원래 2였는데 업데이트되었음10 원래 1이었는데 업데이트되었음, 업데이트 안될수도 있었음.\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 데이터과학, 손흥민11\n\n\n2\n확률, 확률, 골12\n\n\n\n11 원래 2에 있었은데 1로 옮겼음12 원래 1에 있었은데 2로 옮겼음, 안바뀔수도 있었음.\n\n\n(d) \\(\\theta_0[4]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽1로 분류되어있음\n나도 토픽1인듯\n\n\n토픽눈치\n토픽1에는 나랑 같은 단어가 없는데 토픽2에는 있음\n나도 토픽2인듯\n\n\n\n\n\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc3에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 확률이라는 단어가 포함된 비율) = 1 \\(\\times\\) 0.001\n토픽2의 타당성: (doc3에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 확률이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 1/2\n\n\n\\(\\theta_0[4]\\)의 생각: 나는 토픽1인듯함 (그런데 아닐수도 있음)\n\n- 업데이트: 안함. 난 토픽1인것 같긴한데, 확실하지 않아서 그냥 토픽2에 머무르겠음.\n\n\nstage5: \\({\\boldsymbol \\theta}_0[5]\\)을 sampling\n- 현재상태\n\n빨간색/볼드: 지금 focus하는 것\n파란색/볼드: 과거 / 업데이트O\n파란색/볼드X: 과거 / 업데이트X\n\n\n\nTable 9: \\(t=0\\), \\(d={\\tt doc3}\\), \\(w={\\tt 데이터과학}\\)\n\n\n\n\n(a) \\(D\\): 코퍼스(=관찰한자료)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n손흥민\n골\n확률\n\n\n골\n확률\n데이터과학\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_t\\): 추정된 토픽\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n 1 (=\\(\\theta_1[0]\\))13\n 2 (=\\(\\theta_1[2]\\)) 14\n2 (=\\(\\theta_1[4]\\))15\n\n\n 1 (=\\(\\theta_1[1]\\))\n 2 (=\\(\\theta_1[3]\\))\n1 (=\\(\\theta_0[5]\\))\n\n\n\n13 원래 2였는데 업데이트되었음14 원래 1이었는데 업데이트되었음, 업데이트 안될수도 있었음.15 업데이트 할수도 있었는데 안했음\n\n\n\n\n(c) 토픽별로 등장하는 단어\n\n\n\n\n\n\ntopic\nwords\n\n\n\n\n1\n골, 데이터과학, 손흥민16\n\n\n2\n확률, 확률17, 골18\n\n\n\n16 원래 2에 있었은데 1로 옮겼음17 1로 바뀔수도 있었는데 안바꾸었음18 원래 1에 있었은데 2로 옮겼음, 안바뀔수도 있었음.\n\n\n(d) \\(\\theta_0[5]\\)의 고민 = 나는 토픽1인가 토픽2인가?\n\n\n\n\n\n\n\n\n관찰\n결론\n\n\n\n\n문서눈치\n문서안에서 나말고 다른 단어는 토픽2로 분류되어있음\n나도 토픽2인듯\n\n\n토픽눈치\n토픽1에도, 토픽2에도 나랑 같은 단어는 없음\n나는 토픽1도 토픽2도 아닌듯\n\n\n\n\n\n\n- 수정된 타당성\n\n토픽1의 타당성: (doc3에 토픽1이 포함된 비율) \\(\\times\\) (토픽1에서 데이터과학이라는 단어가 포함된 비율) = 0.001 \\(\\times\\) 0.001\n토픽2의 타당성: (doc3에 토픽2가 포함된 비율) \\(\\times\\) (토픽2에서 데이터과학이라는 단어가 포함된 비율) = 1 \\(\\times\\) 0.001\n\n\n\\(\\theta_0[5]\\)의 생각: 나는 토픽1인듯함 (그런데 아닐수도 있음)\n\n- 업데이트: 난 토픽2가 확실한듯\n\nθ['doc3'][1]\n\n1\n\n\n\nθ['doc3'][1] = 2"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#단계3-t1234dots-에-대하여-단계2를-반복",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#단계3-t1234dots-에-대하여-단계2를-반복",
    "title": "15wk-1: MCMC (3)",
    "section": "단계3: \\(t=1,2,3,4,\\dots\\) 에 대하여 단계2를 반복",
    "text": "단계3: \\(t=1,2,3,4,\\dots\\) 에 대하여 단계2를 반복\n\nθ\n\n{'doc1': [1, 1], 'doc2': [2, 2], 'doc3': [2, 2]}\n\n\n- 초기상태와 지금을 비교하면 아래와 같다.\n\n\nTable 10: 수정된 상태\n\n\n\n\n(a) \\({\\boldsymbol \\pi}\\)\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n\\(\\begin{bmatrix}0.9\\\\0.1\\end{bmatrix}\\)\n\\(\\begin{bmatrix}0.8\\\\0.2\\end{bmatrix}\\)\n\\(\\begin{bmatrix}0.3\\\\0.7\\end{bmatrix}\\)\n\n\n\\(\\begin{bmatrix}0.8\\\\0.2\\end{bmatrix}\\)\n\\(\\begin{bmatrix}0.3\\\\0.7\\end{bmatrix}\\)\n\\(\\begin{bmatrix}0.05\\\\0.95\\end{bmatrix}\\)\n\n\n\n\n\n\n(b) \\({\\boldsymbol \\theta}_0\\)\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n2\n1\n2\n\n\n1\n2\n1\n\n\n\n\n\n\n(c) \\({\\boldsymbol \\theta}_1\\)\n\n\ndoc1\ndoc2\ndoc3\n\n\n\n\n1\n219\n220\n\n\n1\n2\n2\n\n\n\n19 1일수도 있었음20 1일 수도 있었음\n\n\n- \\(t=1,2,3,4\\dots\\)로 진행하다보면 서로 눈치를 보면서 아래와 같은 원리로 이동한다.\n\n문서눈치: 내가 토픽k 라면, 내가 속한 문서에는 토픽k로 분류된 단어가 많을거야.\n토픽눈치: 내가 토픽k 라면, 토픽k에는 나랑 같은 단어가 많을거야."
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#구현에-필요한-예비학습",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#구현에-필요한-예비학습",
    "title": "15wk-1: MCMC (3)",
    "section": "구현에 필요한 예비학습",
    "text": "구현에 필요한 예비학습\n\nenumerate\n\nfor i in 'abc':\n    print(i)\n\na\nb\nc\n\n\n\nfor i in enumerate('abc'):\n    print(i)\n\n(0, 'a')\n(1, 'b')\n(2, 'c')\n\n\n\nfor i,s in enumerate('abc'):\n    print(i,s)\n\n0 a\n1 b\n2 c\n\n\n\nfor i,s in enumerate('abc'):\n    print(s*(i+1))\n\na\nbb\nccc\n\n\n\n\nnp.random.choice\n\nnp.random.choice([10,100,1000],size=50)\n\narray([1000, 1000,   10,  100,  100, 1000,  100,  100,   10, 1000,   10,\n       1000,  100,   10, 1000,  100,   10,  100,  100, 1000, 1000,   10,\n       1000, 1000,   10, 1000, 1000,  100, 1000, 1000, 1000, 1000, 1000,\n        100,   10, 1000,   10, 1000,   10,   10, 1000,  100,   10, 1000,\n        100,  100, 1000,   10,  100,  100])\n\n\n\nnp.random.choice([10,100,1000],size=50,p=[0.8,0.1,0.1])\n\narray([  10,   10,   10,   10,   10,   10,   10,   10,  100,   10, 1000,\n         10,   10,  100,   10,   10,   10,   10,   10,   10,   10,   10,\n         10,   10,   10,   10,   10,   10,   10,   10,   10,   10,   10,\n         10,   10,   10,   10,   10,   10, 1000,   10,   10,   10,   10,\n         10,   10,   10,   10,   10,   10])\n\n\n\n\n딕셔너리의 해체\n- 아래와 같은 딕셔너리를 고려하자.\n\nD = {'doc1':['손흥민','골'],\n     'doc2':['골','확률'],\n     'doc3':['확률','데이터과학']}\nθ = {'doc1':[2,1],\n     'doc2':[1,2],\n     'doc3':[2,1]}\n\n- 이러한 딕셔너리에를 해체하고 싶다면?\n\n[wrd for doc in D for wrd in D[doc]]\n\n['손흥민', '골', '골', '확률', '확률', '데이터과학']\n\n\n\n[tpc for doc in θ for tpc in θ[doc]]\n\n[2, 1, 1, 2, 2, 1]\n\n\n\n\n리스트의 count 메소드\n\nlst = list('asdfsdasdfasdfasdfasdfas')\nlst.count('a')\n\n6\n\n\n\n[wrd for doc in D for wrd in D[doc]]\n\n['손흥민', '골', '골', '확률', '확률', '데이터과학']\n\n\n\n[wrd for doc in D for wrd in D[doc]].count('골')\n\n2\n\n\n\n\n조건부 컴프리헨션\n\nlst = [-1,0,1,2]\n[l for l in lst if l&lt;=0]\n\n[-1, 0]\n\n\n\n\n딕셔너리의 원소삭제\n\ndct = {'doc1':['손흥민']*5, 'doc2':['골']*5}\ndct\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민', '손흥민'],\n 'doc2': ['골', '골', '골', '골', '골']}\n\n\n\ndel dct['doc1'][0]\n\n\ndct\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민'], 'doc2': ['골', '골', '골', '골', '골']}\n\n\n\n\n깊은복사\n- 특정원소가 삭제된 dct와 삭제되지 않은 dct를 동시에 가지고 있으려면?\n\ndct = {'doc1':['손흥민']*5, 'doc2':['골']*5}\ndct2 = dct \n\n\ndel dct['doc1'][0]\n\n\ndct\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민'], 'doc2': ['골', '골', '골', '골', '골']}\n\n\n\ndct2 # 잉 왜 같이 삭제되는거야?\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민'], 'doc2': ['골', '골', '골', '골', '골']}\n\n\n- 해결책\n\nimport copy\n\n\ndct = {'doc1':['손흥민']*5, 'doc2':['골']*5}\ndct2 = copy.deepcopy(dct)\n\n\ndel dct['doc1'][0]\n\n\ndct\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민'], 'doc2': ['골', '골', '골', '골', '골']}\n\n\n\ndct2 # 이제야 제대로 돌아가네\n\n{'doc1': ['손흥민', '손흥민', '손흥민', '손흥민', '손흥민'],\n 'doc2': ['골', '골', '골', '골', '골']}\n\n\n\n\n연습문제\n아래와 같은 자료가 있다고 하자.\n\nD = {'doc1':['손흥민','골'],\n     'doc2':['골','확률'],\n     'doc3':['확률','데이터과학']}\nθ = {'doc1':[2,1],\n     'doc2':[1,2],\n     'doc3':[2,1]}\n\n(1) \\(D\\)에는 총 몇개의 단어가 있는가?\n\nlen(set([wrd for doc in D for wrd in D[doc]]))\n\n4\n\n\n(2) 문서2에 토픽1은 몇개나 있는가?\n\nθ['doc2']\n\n[1, 2]\n\n\n\nθ['doc2'].count(1)\n\n1\n\n\n(3) 토픽1에 들어있는 단어들의 목록을 구하라.\n\nwrdlst = [wrd for doc in D for wrd in D[doc]]\nwrdlst\n\n['손흥민', '골', '골', '확률', '확률', '데이터과학']\n\n\n\ntpclst = [tpc for doc in θ for tpc in θ[doc]]\ntpclst \n\n[2, 1, 1, 2, 2, 1]\n\n\n\n[wrd for i,wrd in enumerate(wrdlst) if tpclst[i]==1 ]\n\n['골', '골', '데이터과학']\n\n\n(4) 토픽1에서 ’골’이라는 단어는 몇번 등장하는가?\n\n[wrd for i,wrd in enumerate(wrdlst) if tpclst[i]==1 ].count('골')\n\n2"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#데이터-d와-theta의-설정",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#데이터-d와-theta의-설정",
    "title": "15wk-1: MCMC (3)",
    "section": "데이터: \\(D\\)와 \\(\\theta\\)의 설정",
    "text": "데이터: \\(D\\)와 \\(\\theta\\)의 설정\n\nD = {'doc1': ['심판', '헤딩', '선수', '골', '리그', '골', '선수', '공격', '헤딩', '슈팅', '공', '패스', '공격수', '페널티킥', '공'], 'doc2': ['수비', '골', '챔피언스리그', '헤딩', '경기장', '골키퍼', '챔피언스리그', '헤딩', '경기장', '수비수', '수비수', '패스', '드리블', '선수', '월드컵'], 'doc3': ['헤딩', '리그', '드리블', '골키퍼', '공격수', '공격수', '월드컵', '선수', '공', '헤딩', '중앙미드필더', '공격', '선수', '수비수', '드리블'], 'doc4': ['클럽', '선수', '챔피언스리그', '슈팅', '리그', '수비', '리그', '중앙미드필더', '공격', '공', '중앙미드필더', '골', '패스', '중앙미드필더', '클럽'], 'doc5': ['선수', '경기장', '수비', '골키퍼', '월드컵', '리그', '드리블', '공격수', '슈팅', '선수', '선수', '월드컵', '드리블', '월드컵', '골키퍼'], 'doc6': ['수비수', '심판', '공', '공격', '표준편차', '표본', '상관관계', '모집단', '딥러닝', '클러스터링', '인공지능', '챔피언스리그', '공', '심판', '챔피언스리그'], 'doc7': ['페널티킥', '중앙미드필더', '챔피언스리그', '선수', '표본', '평균', '분류', '로지스틱 회귀', '머신러닝', '클러스터링', '평균', '수비수', '중앙미드필더', '페널티킥', '심판'], 'doc8': ['공격', '경기장', '패스', '수비수', '신뢰구간', '데이터과학', '확률', '통계', '분류', '인공지능', '머신러닝', '수비수', '수비수', '페널티킥', '수비수'], 'doc9': ['페널티킥', '패스', '골키퍼', '공', '신뢰구간', '딥러닝', '평균', '인공지능', '딥러닝', '분산', '딥러닝', '월드컵', '월드컵', '슈팅', '골키퍼'], 'doc10': ['리그', '슈팅', '드리블', '선수', '평균', '데이터분석', '데이터과학', '신뢰구간', '평균', '분류', '딥러닝', '심판', '슈팅', '패스', '선수'], 'doc11': ['평균', '데이터분석', '클러스터링', '데이터과학', '신경망', '데이터분석', '상관관계', '인공지능', '상관관계', '확률', '회귀분석', '로지스틱 회귀', '평균', '표준편차', '딥러닝'], 'doc12': ['신뢰구간', '딥러닝', '확률', '평균', '데이터분석', '상관관계', '회귀분석', '통계', '신경망', '상관관계', '회귀분석', '확률', '로지스틱 회귀', '상관관계', '데이터과학'], 'doc13': ['확률', '로지스틱 회귀', '통계', '딥러닝', '모집단', '머신러닝', '인공지능', '표준편차', '상관관계', '확률', '확률', '클러스터링', '신경망', '분류', '데이터분석'], 'doc14': ['데이터분석', '데이터과학', '분류', '통계적 가설검정', '머신러닝', '로지스틱 회귀', '회귀분석', '분류', '표본', '모집단', '통계적 가설검정', '상관관계', '표본', '클러스터링', '표본'], 'doc15': ['딥러닝', '인공지능', '표본', '표준편차', '신경망', '분류', '모집단', '데이터분석', '통계', '통계적 가설검정', '통계적 가설검정', '머신러닝', '머신러닝', '상관관계', '딥러닝']}\npd.DataFrame(D)\n\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\ndoc4\ndoc5\ndoc6\ndoc7\ndoc8\ndoc9\ndoc10\ndoc11\ndoc12\ndoc13\ndoc14\ndoc15\n\n\n\n\n0\n심판\n수비\n헤딩\n클럽\n선수\n수비수\n페널티킥\n공격\n페널티킥\n리그\n평균\n신뢰구간\n확률\n데이터분석\n딥러닝\n\n\n1\n헤딩\n골\n리그\n선수\n경기장\n심판\n중앙미드필더\n경기장\n패스\n슈팅\n데이터분석\n딥러닝\n로지스틱 회귀\n데이터과학\n인공지능\n\n\n2\n선수\n챔피언스리그\n드리블\n챔피언스리그\n수비\n공\n챔피언스리그\n패스\n골키퍼\n드리블\n클러스터링\n확률\n통계\n분류\n표본\n\n\n3\n골\n헤딩\n골키퍼\n슈팅\n골키퍼\n공격\n선수\n수비수\n공\n선수\n데이터과학\n평균\n딥러닝\n통계적 가설검정\n표준편차\n\n\n4\n리그\n경기장\n공격수\n리그\n월드컵\n표준편차\n표본\n신뢰구간\n신뢰구간\n평균\n신경망\n데이터분석\n모집단\n머신러닝\n신경망\n\n\n5\n골\n골키퍼\n공격수\n수비\n리그\n표본\n평균\n데이터과학\n딥러닝\n데이터분석\n데이터분석\n상관관계\n머신러닝\n로지스틱 회귀\n분류\n\n\n6\n선수\n챔피언스리그\n월드컵\n리그\n드리블\n상관관계\n분류\n확률\n평균\n데이터과학\n상관관계\n회귀분석\n인공지능\n회귀분석\n모집단\n\n\n7\n공격\n헤딩\n선수\n중앙미드필더\n공격수\n모집단\n로지스틱 회귀\n통계\n인공지능\n신뢰구간\n인공지능\n통계\n표준편차\n분류\n데이터분석\n\n\n8\n헤딩\n경기장\n공\n공격\n슈팅\n딥러닝\n머신러닝\n분류\n딥러닝\n평균\n상관관계\n신경망\n상관관계\n표본\n통계\n\n\n9\n슈팅\n수비수\n헤딩\n공\n선수\n클러스터링\n클러스터링\n인공지능\n분산\n분류\n확률\n상관관계\n확률\n모집단\n통계적 가설검정\n\n\n10\n공\n수비수\n중앙미드필더\n중앙미드필더\n선수\n인공지능\n평균\n머신러닝\n딥러닝\n딥러닝\n회귀분석\n회귀분석\n확률\n통계적 가설검정\n통계적 가설검정\n\n\n11\n패스\n패스\n공격\n골\n월드컵\n챔피언스리그\n수비수\n수비수\n월드컵\n심판\n로지스틱 회귀\n확률\n클러스터링\n상관관계\n머신러닝\n\n\n12\n공격수\n드리블\n선수\n패스\n드리블\n공\n중앙미드필더\n수비수\n월드컵\n슈팅\n평균\n로지스틱 회귀\n신경망\n표본\n머신러닝\n\n\n13\n페널티킥\n선수\n수비수\n중앙미드필더\n월드컵\n심판\n페널티킥\n페널티킥\n슈팅\n패스\n표준편차\n상관관계\n분류\n클러스터링\n상관관계\n\n\n14\n공\n월드컵\n드리블\n클럽\n골키퍼\n챔피언스리그\n심판\n수비수\n골키퍼\n선수\n딥러닝\n데이터과학\n데이터분석\n표본\n딥러닝\n\n\n\n\n\n\n\n- 간단한 데이터 조사\n\n데이터는 총 15개의 문서로 이루어져 있으며 처음5개의 문서는 축구관련, 이후 5개는 축구와 통계 관련, 이후 5개는 통계관련이다.\n축구와 통계가 섞인 doc6~doc11은 축구관련4단어, 통계관련7단어, 축구관련4단어의 조합으로 이루어져 있다.\n\n- 초기값\n\nθ = {doc:np.random.choice([0,1],size=15).tolist() for doc in D}\nθ\n\n{'doc1': [0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n 'doc2': [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n 'doc3': [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0],\n 'doc4': [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1],\n 'doc5': [1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0],\n 'doc6': [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n 'doc7': [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n 'doc8': [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1],\n 'doc9': [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n 'doc10': [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0],\n 'doc11': [1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n 'doc12': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n 'doc13': [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n 'doc14': [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1],\n 'doc15': [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1]}\n\n\n- D와 \\(\\theta\\)를 묶어서 하나의 dict를 만들자.\n\ndata = {'D':D, 'θ':θ}"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#하이퍼파라메터",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#하이퍼파라메터",
    "title": "15wk-1: MCMC (3)",
    "section": "하이퍼파라메터",
    "text": "하이퍼파라메터\n\nK = 2 # 토픽의수 &lt;-- 유저가 설정함"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#필요한-함수",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#필요한-함수",
    "title": "15wk-1: MCMC (3)",
    "section": "필요한 함수",
    "text": "필요한 함수\n- \\(p_1\\): \\(d\\)-th document에 포함된 토픽 \\(k\\)의 비율을 리턴하는 함수, 즉 아래를 계산한다.\n\\[p_1=:\\frac{\\#({\\tt topic == k, document == d})+0.1}{\\#({\\tt document == d})+0.1\\times K}\\]\n여기에서 \\(K\\)는 토픽의 수.\n\ndef p1(topic,doc,data):\n    θ = data['θ']\n    a = θ[doc].count(topic) +0.1\n    b = len(θ[doc]) + 0.1 *K  \n    return a/b\n\n- \\(p_2\\): \\(k\\)-th topic에 포함된 단어 \\(w\\)의 비율을 리턴하는 함수, 즉 아래를 계산한다.\n\\[p_2=:\\frac{\\#({\\tt word ==w, topic == k})+0.1}{\\#({\\tt topic == k})+0.1\\times W}\\]\n여기에서 \\(W\\)는 전체단어의 수. (이 예제의 경우 40개의 단어로 이루어짐)\n\ndef p2(word,topic,data):\n    D=data['D']\n    θ=data['θ']\n    tpclst = [tpc for doc in θ for tpc in θ[doc]]\n    wrdlst = [wrd for doc in D for wrd in D[doc]]\n    a = [wrd for i,wrd in enumerate(wrdlst) if tpclst[i]==topic].count(word) + 0.1 \n    b = tpclst.count(topic) + 0.1 * len(set(wrdlst))\n    return a/b"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#알고리즘",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#알고리즘",
    "title": "15wk-1: MCMC (3)",
    "section": "알고리즘",
    "text": "알고리즘\n\n1. 초기값: \\({\\boldsymbol \\theta}_0\\)\n\nθ = {doc:np.random.choice([0,1],size=15).tolist() for doc in D}\nplt.matshow(list(θ.values()))\n\n&lt;matplotlib.image.AxesImage at 0x7fbe31030610&gt;\n\n\n\n\n\n\n\n2. 반복: \\({\\boldsymbol \\theta}_0 \\to {\\boldsymbol \\theta}_1 \\to {\\boldsymbol \\theta}_2 \\to \\dots\\)\n\nfor t in range(5):\n    for doc in D:\n        for i, wrd in enumerate(D[doc]):\n            # 임시의 data를 만들고 현재 포커싱되어있는 자료를 삭제함 \n            data = {'D':copy.deepcopy(D), 'θ': copy.deepcopy(θ)} \n            del data['D'][doc][i]\n            del data['θ'][doc][i]\n            \n            # 토픽의 타당성조사, msr는 타당성을 나타내는 측도, prob는 msr의 총합을 1로 맞춤\n            msr0 = p1(topic=0, doc=doc, data=data) * p2(word=wrd, topic=0, data=data) # 토픽0의 타당성\n            msr1 = p1(topic=1, doc=doc, data=data) * p2(word=wrd, topic=1, data=data) # 토픽1의 타당성\n            prob = [msr0/(msr0 + msr1), msr1/(msr0 + msr1)] \n            \n            # update θ|\n            θ[doc][i] = np.random.choice([0,1], p=prob)"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-08-15wk-1.html#시각화",
    "href": "posts/3. MCMC/2023-06-08-15wk-1.html#시각화",
    "title": "15wk-1: MCMC (3)",
    "section": "시각화",
    "text": "시각화\n\nplt.matshow(list(θ.values()))\n\n&lt;matplotlib.image.AxesImage at 0x7fbe30f627f0&gt;"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html",
    "title": "13wk-2: MCMC (1)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zyk8psVKy2OaZs3zV4ExWn"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#균등분포",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#균등분포",
    "title": "13wk-2: MCMC (1)",
    "section": "균등분포",
    "text": "균등분포\n- 가정: 균등분포에서는 뽑을 수 있다고 가정한다. (제가 사실 여기는 잘 몰라요)\n\n\n\n그림1: 균등분포를 생성하는 원리에 대하여 chatGPT에게 물어봄\n\n\n- 균등분포 이외의 난수는 어떻게?… (어려워요)"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#베르누이-이항분포-포아송-지수분포",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#베르누이-이항분포-포아송-지수분포",
    "title": "13wk-2: MCMC (1)",
    "section": "베르누이, 이항분포, 포아송, 지수분포",
    "text": "베르누이, 이항분포, 포아송, 지수분포\n- 베르누이\n\nx = np.random.rand(1000)\nplt.hist(x);\n\n\n\n\n\nfig, ax = plt.subplots(1,2)\nax[0].hist((x &gt; 0.5)*1.0,color='C0');\nax[1].hist(np.random.binomial(1,0.5,size=1000),color='C1');\n\n\n\n\n\n\\(X \\sim Ber(p)\\), \\(p=0.5\\)\n\n- 이항분포: 베르누이의 합으로!\n\nx = np.random.rand(10*100000).reshape(10,100000)\nfig, ax = plt.subplots(1,2)\nax[0].hist((x&gt;0.5).sum(axis=0),bins=9);\nax[1].hist(np.random.binomial(10,0.5,size=100000),color='C1',bins=9);\n\n\n\n\n- 포아송: 이항분포의 근사로\n\nref: https://guebin.github.io/SC2022/0324.html\n\n- 지수분포: 포아송 프로세스를 이용하여!\n\nref: https://guebin.github.io/SC2022/0324.html"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#inverse-cdf-박스뮬러변환",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#inverse-cdf-박스뮬러변환",
    "title": "13wk-2: MCMC (1)",
    "section": "inverse cdf, 박스뮬러변환",
    "text": "inverse cdf, 박스뮬러변환\n- inverse cdf: 지수분포를 뽑는 또 다른 테크닉: (지수분포가 아니더라도 CDF를 알면 뽑을 수 있음)\n\nref: https://guebin.github.io/SC2022/0329.html\n\n- 박스뮬러변환: 지수분포 + uniform으로 정규분포를 뽑는 테크닉\n\nref: https://guebin.github.io/SC2022/0329.html\n\n- 카이제곱분포, 감마분포: 지수분포를 이용하면 샘플링가능\n\nref: https://guebin.github.io/SC2022/0419.html"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#pdf-정의",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#pdf-정의",
    "title": "13wk-2: MCMC (1)",
    "section": "pdf 정의",
    "text": "pdf 정의\n- 모티브: 그냥 pdf를 입력하면 알아서 샘플링되도록 할 수 없나?\n- 예비학습: 감마함수\n\nscipy.special.gamma(5)\n\n24.0\n\n\n\ng = scipy.special.gamma\n\n- 베타분포의 pdf\n\\[f_X(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\]\n\ndef f(x): \n    return g(2+6)/(g(2)*g(6)) * x**(2-1) * (1-x)**(6-1)\n\n\n_x = np.linspace(0,1,10000)\nplt.plot(_x,f(_x))"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#시도1-망했음",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#시도1-망했음",
    "title": "13wk-2: MCMC (1)",
    "section": "시도1: 망했음",
    "text": "시도1: 망했음\n\nstep1: 초기화\n- 베타분포에서 10만개의 샘플을 뽑아보자.\n- 일단 \\({\\bf xx}=(x_0,x_1,\\dots,x_{T-1})\\)를 초기화를 하자.\n\nxx=[0.99]\nxx\n\n[0.99]\n\n\n\n현재의 \\(x_i\\)값은 \\(x_0=0.99\\) 만 있지만 궁극적으로는 \\({\\cal B}(2,6)\\)에서 생성된 샘플로 채우고 싶다.\n\n\n\nstep2: 후보 샘플링\n- 하나의 \\(y\\) 을 균등분포에서 샘플링한다.\n\nnp.random.seed(1)\ny = np.random.rand()\ny\n\n0.417022004702574\n\n\n\n\nstep3: 비교\n- \\(xx[0]\\) vs \\(y\\)\n\nplt.plot(_x,f(_x))\nplt.scatter(xx[0],0,color='C0')\nplt.scatter(xx[0],f(xx[0]),color='C0')\nplt.scatter(y,0,color='C1')\nplt.scatter(y,f(y),color='C1')\n\n&lt;matplotlib.collections.PathCollection at 0x7ff3eca2eb50&gt;\n\n\n\n\n\n\n\nstep4: 선택\n- 비교결과: \\(xx[0]\\) 보다 \\(y\\)가 나은것 같다\n- 선택: \\(xx[1]=y\\) 로 하자\n\n만약에 \\(xx[0]\\)이 \\(y\\)보다 나은것 같다면? 그냥 \\(xx[1]=xx[0]\\)으로 선택\n\n\n\nstep 1~4 반복: 망했음\n\nT = 100000\nxx = [0.99]\nfor t in range(T):\n    y = np.random.rand()\n    if f(xx[t]) &lt; f(y):\n        xx.append(y) \n    else:\n        xx.append(xx[t])\n\n\nplt.hist(xx,bins=50);"
  },
  {
    "objectID": "posts/3. MCMC/2023-05-30-13wk-2.html#시도2-성공",
    "href": "posts/3. MCMC/2023-05-30-13wk-2.html#시도2-성공",
    "title": "13wk-2: MCMC (1)",
    "section": "시도2: 성공",
    "text": "시도2: 성공\n\nstep1: 초기화\n- 베타분포에서 10만개의 샘플을 뽑아보자.\n- 일단 \\({\\bf xx}=(x_0,x_1,\\dots,x_{T-1})\\)를 초기화를 하자.\n\nxx=[0.99]\nxx\n\n[0.99]\n\n\n\n현재의 \\(x_i\\)값은 \\(x_0=0.99\\) 만 있지만 궁극적으로는 \\({\\cal B}(2,6)\\)에서 생성된 샘플로 채우고 싶다.\n\n\n\nstep2: 후보 샘플링\n- 하나의 \\(y\\) 을 균등분포에서 샘플링한다.\n\nnp.random.seed(1)\ny = np.random.rand()\ny\n\n0.417022004702574\n\n\n\n\nstep3: 비교\n- \\(xx[0]\\) vs \\(y\\)\n\nplt.plot(_x,f(_x))\nplt.scatter(xx[0],0,color='C0')\nplt.scatter(xx[0],f(xx[0]),color='C0')\nplt.scatter(y,0,color='C1')\nplt.scatter(y,f(y),color='C1')\n\n&lt;matplotlib.collections.PathCollection at 0x7ff85ef61b20&gt;\n\n\n\n\n\n\n\nstep4: 선택\n- 비교결과: \\(xx[0]\\) 보다 \\(y\\)가 나은것 같다\n- 선택: 그렇지만 무조건 \\(xx[1]=y\\) 로 선택하면 큰일나겠음.. 아래의 확률로 선택하자!\n\n확률 \\(\\frac{f(xx[0])}{f(xx[0])+f(y)}\\) 로 \\(xx[1]=xx[0]\\)을 선택!\n확률 \\(\\frac{f(y)}{f(xx[0])+f(y)}\\) 로 \\(y\\)를 선택!\n\n\n\nstep 1~4 반복: 이게 된다고?\n\nT = 100000\nxx = [0.99]\nfor t in range(T):\n    y = np.random.rand()\n    thresh_prob = f(y)/(f(xx[t])+f(y)) ## thresh_prob 가 클수록 y가 선택\n    _u = np.random.rand()\n    if _u &lt; thresh_prob:\n        xx.append(y) \n    else:\n        xx.append(xx[t])\n\n\nplt.hist(xx,bins=50);\n\n\n\n\n\nplt.hist(np.random.beta(2,6,size=100000),bins=50);"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-01-14wk-1.html",
    "href": "posts/3. MCMC/2023-06-01-14wk-1.html",
    "title": "14wk-1,2: MCMC (2)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zN7idmV8iVcOs4zV2Lg8WE\n\n\n이 강의는 14wk-1, 14wk-2 의 강의가 합쳐져 있습니다."
  },
  {
    "objectID": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천x-이산형",
    "href": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천x-이산형",
    "title": "14wk-1,2: MCMC (2)",
    "section": "샘플추천X & 이산형",
    "text": "샘플추천X & 이산형\n- 모티브: 위의 예제에서 전이행렬이 꼭 아래와 같을 필요는 없는것 아닌가?\n\nP\n\narray([[0.4, 0.6],\n       [0.9, 0.1]])\n\n\n- 우리의 목표: 아래와 같은 분포 \\({\\boldsymbol \\pi}\\)를 따르는 확률변수 \\(X\\)를 생성하기만 하면 되는 것 아닌가?\n\n\n\n\\(X\\)\n\\(0\\)\n\\(1\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(0.6\\)\n\\(0.4\\)\n\n\n\n이러한 정상분포를 가지는 에르고딕 마코프체인의 전이행렬 \\({\\bf P}\\)를 역으로 설계해보자.\n\nP = np.array([[0.6,0.4],\n              [0.6,0.4]])\n\n\nP\n\narray([[0.6, 0.4],\n       [0.6, 0.4]])\n\n\n- DBC 체크: 아래의 detailed balance condition을 만족하기만 하면 target distribution \\({\\boldsymbol \\pi}\\)는 새롭게 설계한 \\({\\bf P}\\)를 가지는 HMC \\(\\{X_t\\}\\)의 정상분포라 주장할 수 있다.\n\\[\\forall i,j \\in E:~ \\pi_ip_{ij}=\\pi_jp_{ji}\\]\n이 예제의 경우\n\\[\\forall i,j \\in E:~ \\pi_i\\pi_{j}=\\pi_j\\pi_{i}\\]\n가 되므로 성립한다.\n- 따라서 전이행렬 \\({\\bf P}\\)를 가지는 마코프체인은 \\({\\boldsymbol \\pi}^\\top=[0.4,0.6]\\)를 정상분포로 가지는 마코프체인이다.7 이 마코프체인은 IRR 이므로 정상분포 \\({\\boldsymbol \\pi}\\)는 유일한 정상분포가 되고, 따라서 PR조건이 만족된다. 또한 AP를 만족하므로 에르고딕 마코프체인이 된다.\n7 DBC를 만족하기 때문에\nP\n\narray([[0.6, 0.4],\n       [0.6, 0.4]])\n\n\n\ndef doctor_strange(x0):\n    xx = [x0]\n    for t in range(10500): \n        _u = np.random.rand()\n        if _u &lt; 0.4:\n            xx.append(1)\n        else:\n            xx.append(0)\n    return xx \n\n\nxx = doctor_strange(0)\n\n\nplt.hist(xx[501:],bins=100);\n\n\n\n\n- 이러한 방식은 유한차원으로 확장가능하다. (그런데 귀찮다)\n\n\n\n\\(X\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(0.6\\)\n\\(0.2\\)\n\\(0.2\\)\n\n\n\n\nP = np.array([[0.6,0.2,0.2],\n              [0.6,0.2,0.2],\n              [0.6,0.2,0.2]])\n\n\ndef doctor_strange(x0):\n    xx = [x0]\n    for t in range(10500): \n        _u = np.random.rand()\n        if _u &lt; 0.6:\n            xx.append(0)\n        elif _u&lt; 0.8:\n            xx.append(1)\n        else:\n            xx.append(2)        \n    return xx \n\n\nxx = doctor_strange(0)\n\n\nplt.hist(xx[501:],bins=100);"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천-이산형",
    "href": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천-이산형",
    "title": "14wk-1,2: MCMC (2)",
    "section": "샘플추천 & 이산형",
    "text": "샘플추천 & 이산형\n- 아래의 분포를 고려하자.\n\nnp.random.seed(43052)\nu = np.random.rand(10)\nπ = (u/u.sum()).reshape(-1,1)\nπ.T\n\narray([[0.12977311, 0.00786117, 0.13310662, 0.09836388, 0.01944822,\n        0.01858917, 0.13959302, 0.15544153, 0.14440391, 0.15341937]])\n\n\n\nplt.stem(π)\n\n&lt;StemContainer object of 3 artists&gt;\n\n\n\n\n\n- 이러한 분포에서 샘플을 뽑는 상황을 고려하자.\n\n위의 코드로는 못하겠다.\n다른 방법은 없을까?\n\n\n그냥 저번시간처럼 하자. \\(x\\)에서 \\(x'\\)으로 가는 확률을 다 정의하지 말고, \\(x'\\)를 적당히 추천받고 옮겨탈지 말지 결정하자.\n\n- 저번시간 테크닉: \\(X(\\omega_1)=x\\)가 주어졌을때 \\(X'(\\omega_1)=x'\\)를 뽑는 방법!\n\n\\(x\\)가 주어졌다고 가정하자.\n\\(x'\\)의 후보로 \\(Y(\\omega^\\ast)=y\\)를 뽑는다. \\(Y \\sim {\\boldsymbol p}_Y:=[\\frac{1}{10},\\dots,\\frac{1}{10}]\\)\n\\(x'\\)은 \\(x\\)가 적절한지, 아니면 추천받은 \\(y\\)가 적절한지 따져보고 결정한다. 즉 아래의 확률로 \\(x'=y\\)를 선택한다.\n\n\\[\\frac{\\pi_y}{\\pi_x + \\pi_y}\\]\n- 의문: 저렇게 막 만들어도 에르고딕한지 어떻게 알지?\n\n당연히 몰라요.\n조사를 좀 해봐야 합니다.\n\n- DBC condition 체크\n\\[\\forall i,j \\in E:~ \\pi_ip_{ij}=\\pi_jp_{ji}\\]\n노테이션을 살짝 변경하면 아래와 같다.\n\\[\\forall x,x' \\in E:~ \\pi_xp_{xx'}=\\pi_{x'}p_{x'x}\\]\n여기에서 \\(p_{xx'}\\)와 \\(p_{x'x}\\)를 각각 구하면 아래와 같다.\n\\[p_{xx'} = \\frac{1}{10}\\frac{\\pi_{x'}}{\\pi_x + \\pi_{x'}}\\]\n\\[p_{x'x} = \\frac{1}{10}\\frac{\\pi_{x}}{\\pi_x + \\pi_{x'}}\\]\n따라서 DBC가 성립한다.\n- 이론전개: DBC가 만족되었으므로 \\({\\boldsymbol \\pi}\\)는 정상분포가 된다. 그리고 이 마코프체인은 IRR 이므로 정상분포는 유일해진다. 또한 IRR-HMC에서는 유일한 정상분포의 존재와 PR이 동치이므로 이 마코프체인은 PR이 된다. 또한 이 마코프체인은 AP조건을 만족한다. 따라서 이 마코프체인은 에르고딕이 된다.\n\ndef doctor_strange(x0):\n    xx = [x0]\n    for t in range(100500): \n        y = np.random.choice(range(10))\n        acceptance_prob = π[y]/(π[xx[t]]+π[y]) ## acceptance_prob 가 클수록 y가 선택\n        _u = np.random.rand()\n        if _u &lt; acceptance_prob:\n            xx.append(y)\n        else:\n            xx.append(xx[t])\n    return xx \n\n\nxx = doctor_strange(0)\n\n\nplt.stem(π*100000)\nplt.hist(xx[501:],bins=100,color='C1',alpha=0.8);\n\n\n\n\n참고\n### 비교를 위해 이전에 만들었던 코드를 확인해보자.\nT = 100000\nxx = [0.99]\nfor t in range(T):\n    y = np.random.rand()\n    thresh_prob = f(y)/(f(xx[t])+f(y)) ## thresh_prob 가 클수록 y가 선택\n    _u = np.random.rand()\n    if _u &lt; thresh_prob:\n        xx.append(y) \n    else:\n        xx.append(xx[t])"
  },
  {
    "objectID": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천-연속형",
    "href": "posts/3. MCMC/2023-06-01-14wk-1.html#샘플추천-연속형",
    "title": "14wk-1,2: MCMC (2)",
    "section": "샘플추천 & 연속형",
    "text": "샘플추천 & 연속형\n- 아래와 같은 pdf \\(f_X(x)\\)를 가지는 확률변수를 만들고 싶다면?\n\ng = scipy.special.gamma\n\n\ndef f(x): \n    return g(2+6)/(g(2)*g(6)) * x**(2-1) * (1-x)**(6-1)\n\n\n_x = np.linspace(0,1,1000)\nplt.plot(_x, f(_x))\n\n\n\n\n- 어? 잠깐만..\n\n이전예제: \\(E= \\{0,1,2,3,4,5,6,7,8,9\\}\\)\n지금예제: \\(E= [0,1]\\)\n\n상태공간 \\(E\\) coutable 이 아니잖아? (그래도 일단 진행해보자)\n- 테크닉: \\(X(\\omega_1)=x\\)가 주어졌을때 \\(X'(\\omega_1)=x'\\)를 뽑는 방법!\n\n\\(x\\)가 주어졌다고 가정하자.\n\\(x'\\)의 후보로 \\(Y(\\omega^\\ast)=y\\)를 뽑는다. \\(Y \\sim {\\cal U}\\)\n\\(x'\\)은 \\(x\\)가 적절한지, 아니면 추천받은 \\(y\\)가 적절한지 따져보고 결정한다. 즉 아래의 확률로 \\(x'=y\\)를 선택한다.\n\n\\[\\frac{f_X(y)}{f_X(x) + f_X(y)}\\]\n- DBC condition 체크\n이전예제\n\\[\\forall x,x' \\in E:~ \\pi_xp_{xx'}=\\pi_{x'}p_{x'x}\\]\n지금예제\n\\[\\forall x,x' \\in E:~ f_X(x)p_{xx'}=f_X(x')p_{x'x}\\]\n여기에서 \\(p_{xx'}\\)와 \\(p_{x'x}\\)는 대충 아래와 같이 쓸 수 있을것 같다.\n\\[p_{xx'} = f_Y(x')\\frac{f_X(x')}{f_X(x) + f_X(x')}\\]\n\\[p_{x'x} = f_Y(x)\\frac{f_X(x)}{f_X(x) + f_X(x')}\\]\n우선 \\(f_Y(x')=f_Y(x)=1\\) 이므로 지금까지의 논의가 맞다면 DBC는 만족된다.\n- 의문1: 좀 이상한데? \\(f_X(x)\\)는 \\(\\pi_x\\)와 다르게 확률을 의미하는게 아니잖아?\n- 의문2: 애초에 HMC \\(\\{X_t\\}\\)를 coutable한 state space를 가진다고 정의하지 않았어?"
  },
  {
    "objectID": "posts/4. 강화학습/Untitled.html",
    "href": "posts/4. 강화학습/Untitled.html",
    "title": "q_net",
    "section": "",
    "text": "- 데이터를 모아보자.\n\ncurrent_states = collections.deque(maxlen=50) \nactions = collections.deque(maxlen=50) \nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50) \n\ncurrent_state, _ = env.reset()\nfor t in range(500): \n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n\n    current_states.append(current_state) \n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward)\n    terminations.append(terminated) \n    \n    current_state = next_state \n    if terminated: break \n\n- 이전코드에서 아래에 대응하는 부분을 잘 구현하면 된다.\n## 1. q[x,y,a] \nagent.q = np.zeros([4,4,4])  \n\n## 2. q_estimate  \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = reward + 0.99 * q_future\nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimate 와 q_realistic 를 비슷하게 만들어주는 역할을 하는 코드 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff\n1. q_net를 설정\n\nq_net = torch.nn.Sequential(\n    torch.nn.Linear(8,128), # 8개의 상태공간 \n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64), \n    torch.nn.ReLU(),\n    torch.nn.Linear(64,32), \n    torch.nn.ReLU(),\n    torch.nn.Linear(32,4) # 4개의 action값들 \n)\n\n\ntorch.tensor(current_state)\n\ntensor([ 0.3533,  0.0950, -0.0055, -0.0028, -1.5255,  0.0227,  1.0000,  0.0000])\n\n\n\nq_net(torch.tensor(current_state))\n\ntensor([-0.1506,  0.0799, -0.0546,  0.1738], grad_fn=&lt;AddBackward0&gt;)\n\n\n1. q_net를 설정 (배치버전)\n\ntorch.tensor(current_states)\n\n/tmp/ipykernel_141614/982066375.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/torch/csrc/utils/tensor_new.cpp:245.)\n  torch.tensor(current_states)\n\n\ntensor([[ 0.0945,  1.2319,  0.3431, -0.5353, -0.4019, -0.3059,  0.0000,  0.0000],\n        [ 0.0977,  1.2194,  0.3319, -0.5592, -0.4147, -0.2562,  0.0000,  0.0000],\n        [ 0.1013,  1.2076,  0.3634, -0.5258, -0.4279, -0.2634,  0.0000,  0.0000],\n        [ 0.1048,  1.1952,  0.3725, -0.5556, -0.4432, -0.3064,  0.0000,  0.0000],\n        [ 0.1086,  1.1834,  0.3910, -0.5309, -0.4593, -0.3222,  0.0000,  0.0000],\n        [ 0.1124,  1.1709,  0.3910, -0.5576, -0.4754, -0.3222,  0.0000,  0.0000],\n        [ 0.1161,  1.1579,  0.3909, -0.5843, -0.4915, -0.3222,  0.0000,  0.0000],\n        [ 0.1200,  1.1442,  0.3982, -0.6136, -0.5094, -0.3575,  0.0000,  0.0000],\n        [ 0.1241,  1.1307,  0.4350, -0.6065, -0.5269, -0.3503,  0.0000,  0.0000],\n        [ 0.1284,  1.1165,  0.4440, -0.6372, -0.5467, -0.3968,  0.0000,  0.0000],\n        [ 0.1326,  1.1018,  0.4369, -0.6606, -0.5647, -0.3598,  0.0000,  0.0000],\n        [ 0.1369,  1.0864,  0.4466, -0.6908, -0.5851, -0.4066,  0.0000,  0.0000],\n        [ 0.1412,  1.0705,  0.4382, -0.7134, -0.6031, -0.3619,  0.0000,  0.0000],\n        [ 0.1453,  1.0541,  0.4288, -0.7361, -0.6188, -0.3138,  0.0000,  0.0000],\n        [ 0.1494,  1.0371,  0.4210, -0.7588, -0.6324, -0.2707,  0.0000,  0.0000],\n        [ 0.1535,  1.0196,  0.4210, -0.7855, -0.6459, -0.2707,  0.0000,  0.0000],\n        [ 0.1577,  1.0014,  0.4295, -0.8163, -0.6617, -0.3168,  0.0000,  0.0000],\n        [ 0.1618,  0.9826,  0.4295, -0.8430, -0.6776, -0.3168,  0.0000,  0.0000],\n        [ 0.1660,  0.9631,  0.4295, -0.8696, -0.6934, -0.3167,  0.0000,  0.0000],\n        [ 0.1703,  0.9430,  0.4378, -0.9011, -0.7117, -0.3655,  0.0000,  0.0000],\n        [ 0.1746,  0.9223,  0.4459, -0.9318, -0.7322, -0.4095,  0.0000,  0.0000],\n        [ 0.1789,  0.9009,  0.4535, -0.9624, -0.7548, -0.4523,  0.0000,  0.0000],\n        [ 0.1832,  0.8789,  0.4453, -0.9843, -0.7750, -0.4040,  0.0000,  0.0000],\n        [ 0.1876,  0.8563,  0.4511, -1.0155, -0.7972, -0.4442,  0.0000,  0.0000],\n        [ 0.1920,  0.8330,  0.4576, -1.0464, -0.8215, -0.4853,  0.0000,  0.0000],\n        [ 0.1965,  0.8091,  0.4664, -1.0788, -0.8485, -0.5410,  0.0000,  0.0000],\n        [ 0.2011,  0.7850,  0.4853, -1.0827, -0.8762, -0.5538,  0.0000,  0.0000],\n        [ 0.2059,  0.7603,  0.4909, -1.1135, -0.9058, -0.5921,  0.0000,  0.0000],\n        [ 0.2106,  0.7350,  0.4908, -1.1402, -0.9354, -0.5921,  0.0000,  0.0000],\n        [ 0.2154,  0.7090,  0.4970, -1.1727, -0.9675, -0.6420,  0.0000,  0.0000],\n        [ 0.2206,  0.6830,  0.5452, -1.1749, -0.9997, -0.6447,  0.0000,  0.0000],\n        [ 0.2260,  0.6563,  0.5502, -1.2072, -1.0343, -0.6917,  0.0000,  0.0000],\n        [ 0.2313,  0.6290,  0.5501, -1.2340, -1.0689, -0.6916,  0.0000,  0.0000],\n        [ 0.2372,  0.6013,  0.6080, -1.2512, -1.1027, -0.6766,  0.0000,  0.0000],\n        [ 0.2431,  0.5731,  0.6030, -1.2710, -1.1338, -0.6211,  0.0000,  0.0000],\n        [ 0.2498,  0.5449,  0.6818, -1.2721, -1.1649, -0.6224,  0.0000,  0.0000],\n        [ 0.2564,  0.5162,  0.6772, -1.2927, -1.1935, -0.5727,  0.0000,  0.0000],\n        [ 0.2638,  0.4872,  0.7504, -1.3095, -1.2215, -0.5597,  0.0000,  0.0000],\n        [ 0.2713,  0.4575,  0.7503, -1.3362, -1.2495, -0.5597,  0.0000,  0.0000],\n        [ 0.2786,  0.4274,  0.7475, -1.3566, -1.2751, -0.5113,  0.0000,  0.0000],\n        [ 0.2861,  0.3966,  0.7474, -1.3833, -1.3006, -0.5113,  0.0000,  0.0000],\n        [ 0.2934,  0.3653,  0.7459, -1.4048, -1.3242, -0.4716,  0.0000,  0.0000],\n        [ 0.3009,  0.3334,  0.7480, -1.4378, -1.3502, -0.5205,  0.0000,  0.0000],\n        [ 0.3083,  0.3008,  0.7479, -1.4645, -1.3763, -0.5205,  0.0000,  0.0000],\n        [ 0.3157,  0.2678,  0.7453, -1.4840, -1.3995, -0.4653,  0.0000,  0.0000],\n        [ 0.3231,  0.2340,  0.7465, -1.5177, -1.4254, -0.5179,  0.0000,  0.0000],\n        [ 0.3306,  0.1997,  0.7464, -1.5443, -1.4513, -0.5178,  0.0000,  0.0000],\n        [ 0.3380,  0.1648,  0.7447, -1.5632, -1.4743, -0.4587,  0.0000,  0.0000],\n        [ 0.3454,  0.1294,  0.7446, -1.5899, -1.4972, -0.4587,  0.0000,  0.0000],\n        [ 0.3536,  0.0933,  0.8174, -1.6182, -1.5197, -0.4511,  1.0000,  0.0000]])\n\n\n\ntorch.tensor(np.array(current_states)).shap\n\ntensor([[ 0.0945,  1.2319,  0.3431, -0.5353, -0.4019, -0.3059,  0.0000,  0.0000],\n        [ 0.0977,  1.2194,  0.3319, -0.5592, -0.4147, -0.2562,  0.0000,  0.0000],\n        [ 0.1013,  1.2076,  0.3634, -0.5258, -0.4279, -0.2634,  0.0000,  0.0000],\n        [ 0.1048,  1.1952,  0.3725, -0.5556, -0.4432, -0.3064,  0.0000,  0.0000],\n        [ 0.1086,  1.1834,  0.3910, -0.5309, -0.4593, -0.3222,  0.0000,  0.0000],\n        [ 0.1124,  1.1709,  0.3910, -0.5576, -0.4754, -0.3222,  0.0000,  0.0000],\n        [ 0.1161,  1.1579,  0.3909, -0.5843, -0.4915, -0.3222,  0.0000,  0.0000],\n        [ 0.1200,  1.1442,  0.3982, -0.6136, -0.5094, -0.3575,  0.0000,  0.0000],\n        [ 0.1241,  1.1307,  0.4350, -0.6065, -0.5269, -0.3503,  0.0000,  0.0000],\n        [ 0.1284,  1.1165,  0.4440, -0.6372, -0.5467, -0.3968,  0.0000,  0.0000],\n        [ 0.1326,  1.1018,  0.4369, -0.6606, -0.5647, -0.3598,  0.0000,  0.0000],\n        [ 0.1369,  1.0864,  0.4466, -0.6908, -0.5851, -0.4066,  0.0000,  0.0000],\n        [ 0.1412,  1.0705,  0.4382, -0.7134, -0.6031, -0.3619,  0.0000,  0.0000],\n        [ 0.1453,  1.0541,  0.4288, -0.7361, -0.6188, -0.3138,  0.0000,  0.0000],\n        [ 0.1494,  1.0371,  0.4210, -0.7588, -0.6324, -0.2707,  0.0000,  0.0000],\n        [ 0.1535,  1.0196,  0.4210, -0.7855, -0.6459, -0.2707,  0.0000,  0.0000],\n        [ 0.1577,  1.0014,  0.4295, -0.8163, -0.6617, -0.3168,  0.0000,  0.0000],\n        [ 0.1618,  0.9826,  0.4295, -0.8430, -0.6776, -0.3168,  0.0000,  0.0000],\n        [ 0.1660,  0.9631,  0.4295, -0.8696, -0.6934, -0.3167,  0.0000,  0.0000],\n        [ 0.1703,  0.9430,  0.4378, -0.9011, -0.7117, -0.3655,  0.0000,  0.0000],\n        [ 0.1746,  0.9223,  0.4459, -0.9318, -0.7322, -0.4095,  0.0000,  0.0000],\n        [ 0.1789,  0.9009,  0.4535, -0.9624, -0.7548, -0.4523,  0.0000,  0.0000],\n        [ 0.1832,  0.8789,  0.4453, -0.9843, -0.7750, -0.4040,  0.0000,  0.0000],\n        [ 0.1876,  0.8563,  0.4511, -1.0155, -0.7972, -0.4442,  0.0000,  0.0000],\n        [ 0.1920,  0.8330,  0.4576, -1.0464, -0.8215, -0.4853,  0.0000,  0.0000],\n        [ 0.1965,  0.8091,  0.4664, -1.0788, -0.8485, -0.5410,  0.0000,  0.0000],\n        [ 0.2011,  0.7850,  0.4853, -1.0827, -0.8762, -0.5538,  0.0000,  0.0000],\n        [ 0.2059,  0.7603,  0.4909, -1.1135, -0.9058, -0.5921,  0.0000,  0.0000],\n        [ 0.2106,  0.7350,  0.4908, -1.1402, -0.9354, -0.5921,  0.0000,  0.0000],\n        [ 0.2154,  0.7090,  0.4970, -1.1727, -0.9675, -0.6420,  0.0000,  0.0000],\n        [ 0.2206,  0.6830,  0.5452, -1.1749, -0.9997, -0.6447,  0.0000,  0.0000],\n        [ 0.2260,  0.6563,  0.5502, -1.2072, -1.0343, -0.6917,  0.0000,  0.0000],\n        [ 0.2313,  0.6290,  0.5501, -1.2340, -1.0689, -0.6916,  0.0000,  0.0000],\n        [ 0.2372,  0.6013,  0.6080, -1.2512, -1.1027, -0.6766,  0.0000,  0.0000],\n        [ 0.2431,  0.5731,  0.6030, -1.2710, -1.1338, -0.6211,  0.0000,  0.0000],\n        [ 0.2498,  0.5449,  0.6818, -1.2721, -1.1649, -0.6224,  0.0000,  0.0000],\n        [ 0.2564,  0.5162,  0.6772, -1.2927, -1.1935, -0.5727,  0.0000,  0.0000],\n        [ 0.2638,  0.4872,  0.7504, -1.3095, -1.2215, -0.5597,  0.0000,  0.0000],\n        [ 0.2713,  0.4575,  0.7503, -1.3362, -1.2495, -0.5597,  0.0000,  0.0000],\n        [ 0.2786,  0.4274,  0.7475, -1.3566, -1.2751, -0.5113,  0.0000,  0.0000],\n        [ 0.2861,  0.3966,  0.7474, -1.3833, -1.3006, -0.5113,  0.0000,  0.0000],\n        [ 0.2934,  0.3653,  0.7459, -1.4048, -1.3242, -0.4716,  0.0000,  0.0000],\n        [ 0.3009,  0.3334,  0.7480, -1.4378, -1.3502, -0.5205,  0.0000,  0.0000],\n        [ 0.3083,  0.3008,  0.7479, -1.4645, -1.3763, -0.5205,  0.0000,  0.0000],\n        [ 0.3157,  0.2678,  0.7453, -1.4840, -1.3995, -0.4653,  0.0000,  0.0000],\n        [ 0.3231,  0.2340,  0.7465, -1.5177, -1.4254, -0.5179,  0.0000,  0.0000],\n        [ 0.3306,  0.1997,  0.7464, -1.5443, -1.4513, -0.5178,  0.0000,  0.0000],\n        [ 0.3380,  0.1648,  0.7447, -1.5632, -1.4743, -0.4587,  0.0000,  0.0000],\n        [ 0.3454,  0.1294,  0.7446, -1.5899, -1.4972, -0.4587,  0.0000,  0.0000],\n        [ 0.3536,  0.0933,  0.8174, -1.6182, -1.5197, -0.4511,  1.0000,  0.0000]])"
  },
  {
    "objectID": "posts/4. 강화학습/A2.html",
    "href": "posts/4. 강화학습/A2.html",
    "title": "A2: 강화학습 (2) – 4x4 grid",
    "section": "",
    "text": "강의영상\n\n\n\nGame2: 4x4 grid\n- 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에 도달하도록 학습하는 방법\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport IPython\n\n\n\n예비학습: 시각화\n\ndef show(states):\n    fig = plt.Figure()\n    ax = fig.subplots()\n    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n    sc = ax.scatter(0, 0, color='red', s=500)  \n    ax.text(0, 0, 'start', ha='center', va='center')\n    ax.text(3, 3, 'end', ha='center', va='center')\n    # Adding grid lines to the plot\n    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n    def update(t):\n        sc.set_offsets(states[t])\n    ani = FuncAnimation(fig,update,frames=len(states))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\nshow([[0,0],[0,1],[1,1],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3]])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nEnv 클래스 구현\n- GridWorld: 강화학습에서 많이 예시로 사용되는 기본적인 시뮬레이션 환경\n\nState: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중 하나에 있을 수 있음.\nAction: 에이전트는 현재상태에서 다음상태로 이동하기 위해 상,하,좌,우 중 하나의 행동을 취할 수 있음.\nReward: 에이전트가 현재상태에서 특정 action을 하면 얻어지는 보상\nTerminated: 하나의 에피소드가 종료되었음을 나타내는 상태\n\n\naction = 3\ncurrent_state = np.array([1,1])\n\n\nnext_state = current_state + action_to_direction[action]\nnext_state\n\nNameError: name 'action_to_direction' is not defined\n\n\n\nclass GridWorld:\n    def __init__(self):\n        self.reset()\n        self.state_space = gym.spaces.MultiDiscrete([4,4])\n        self.action_space = gym.spaces.Discrete(4) \n        self._action_to_direction = { \n            0 : np.array([1, 0]), # x+ \n            1 : np.array([0, 1]), # y+ \n            2 : np.array([-1 ,0]), # x- \n            3 : np.array([0, -1]) # y- \n        }\n    def reset(self):\n        self.agent_action = None \n        self.agent_state = np.array([0,0])        \n        return self.agent_state \n    def step(self,action):\n        direction = self._action_to_direction[action]\n        self.agent_state = self.agent_state + direction\n        if self.agent_state not in env.state_space: # 4x4 그리드 밖에 있는 경우\n            reward = -10 \n            terminated = True\n            self.agent_state = self.agent_state -1/2 * direction\n        elif np.array_equal(env.agent_state, np.array([3,3])): # 목표지점에 도달할 경우 \n            reward = 100 \n            terminated = True\n        else: \n            reward = -1 \n            terminated = False         \n        return self.agent_state, reward, terminated\n\n\nenv = GridWorld()\n\n\nstates = [] \nstate = env.reset()\nstates.append(state) \nfor t in range(50):\n    action = env.action_space.sample() \n    state,reward,terminated = env.step(action)\n    states.append(state) \n    if terminated: break \n\n\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nAgent1 클래스 구현 + Run\n- 우리가 구현하고 싶은 기능\n\n.act(): 액션을 결정 –&gt; 여기서는 그냥 랜덤액션\n.save_experience(): 데이터를 저장 –&gt; 여기에 일단 초점을 맞추자\n.learn(): 데이터로에서 학습 –&gt; 패스\n\n- 첫번째 시도\n\nclass Agent1:\n    def __init__(self,env):\n        self.action_space = env.action_space\n        self.state_spcae = env.state_space \n        self.n_experiences = 0 \n        self.n_episodes = 0 \n        self.score = 0 \n        \n        # episode-wise info \n        self.scores = [] \n        self.playtimes = []\n\n        # time-wise info\n        self.current_state = None \n        self.action = None \n        self.reward = None \n        self.next_state = None         \n        self.terminated = None \n\n        # replay_buffer \n        self.actions = []\n        self.current_states = [] \n        self.rewards = []\n        self.next_states = [] \n        self.terminations = [] \n\n    def act(self):\n        self.action = self.action_space.sample() \n\n    def save_experience(self):\n        self.actions.append(self.action) \n        self.current_states.append(self.current_state)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences += 1 \n        self.score = self.score + self.reward \n        \n    def learn(self):\n        pass \n\n\nenv = GridWorld() \nagent = Agent1(env) \nfor _ in range(20):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        # agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n    ## 덜 본질적인 코드 \n    print(\n        f\"Epsiode: {agent.n_episodes} \\t\"\n        f\"Score: {agent.scores[-1]} \\t\"\n        f\"Playtime: {agent.playtimes[-1]}\"\n    )   \n\nEpsiode: 1  Score: -21  Playtime: 12\nEpsiode: 2  Score: -10  Playtime: 1\nEpsiode: 3  Score: -11  Playtime: 2\nEpsiode: 4  Score: -10  Playtime: 1\nEpsiode: 5  Score: -10  Playtime: 1\nEpsiode: 6  Score: -11  Playtime: 2\nEpsiode: 7  Score: -18  Playtime: 9\nEpsiode: 8  Score: 93   Playtime: 8\nEpsiode: 9  Score: -13  Playtime: 4\nEpsiode: 10     Score: -13  Playtime: 4\nEpsiode: 11     Score: -18  Playtime: 9\nEpsiode: 12     Score: -10  Playtime: 1\nEpsiode: 13     Score: -10  Playtime: 1\nEpsiode: 14     Score: -10  Playtime: 1\nEpsiode: 15     Score: -10  Playtime: 1\nEpsiode: 16     Score: -16  Playtime: 7\nEpsiode: 17     Score: -10  Playtime: 1\nEpsiode: 18     Score: -24  Playtime: 15\nEpsiode: 19     Score: -13  Playtime: 4\nEpsiode: 20     Score: -10  Playtime: 1\n\n\n\nsum(agent.playtimes[:7])\n\n28\n\n\n\nsum(agent.playtimes[:8])\n\n36\n\n\n\nstates = [np.array([0,0])] + agent.next_states[28:36]\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n우연히 잘맞춘 케이스\n\n\n\n환경의 이해 (1차원적 이해)\n- 무작위로 10000판을 진행해보자.\n\nenv = GridWorld() \nagent = Agent1(env) \nfor _ in range(10000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        # agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n\n\nagent.n_experiences\n\n33249\n\n\n- 데이터관찰\n\nagent.current_states[0], agent.actions[0], agent.rewards[0], agent.next_states[0]\n\n(array([0, 0]), 0, -1, array([1, 0]))\n\n\n\nagent.current_states[1], agent.actions[1], agent.rewards[1], agent.next_states[1]\n\n(array([1, 0]), 1, -1, array([1, 1]))\n\n\n\nagent.current_states[2], agent.actions[2], agent.rewards[2], agent.next_states[2]\n\n(array([1, 1]), 1, -1, array([1, 2]))\n\n\n\nagent.current_states[3], agent.actions[3], agent.rewards[3], agent.next_states[3]\n\n(array([1, 2]), 2, -1, array([0, 2]))\n\n\n\nagent.current_states[4], agent.actions[4], agent.rewards[4], agent.next_states[4]\n\n(array([0, 2]), 1, -1, array([0, 3]))\n\n\n- 환경을 이해하기 위한 기록 (1)\n\nq = np.zeros([4,4,4])\ncount = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i] \n    a = agent.actions[i] \n    q[x,y,a] = q[x,y,a] + agent.rewards[i] \n    count[x,y,a] = count[x,y,a] + 1 \n\n\ncount[count == 0] = 0.01 \nq = q/count\n\n\nq[:,:,3]\n\narray([[-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,   0.]])\n\n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1. 100.]\n [-10. -10. -10.   0.]]\n\naction = 1\naction-value function = \n [[ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1. 100.   0.]]\n\naction = 2\naction-value function = \n [[-10. -10. -10. -10.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.   0.]]\n\naction = 3\naction-value function = \n [[-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.   0.]]\n\n\n\n- 환경을 이해하기 위한 기록 (2)\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n    q_realistic = agent.rewards[i] # 실제 답 \n    diff = q_realistic - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값 \n    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 \n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[-1.         -1.         -1.         -0.99866234]\n [-1.         -1.         -1.         -0.99851783]\n [-0.99999999 -1.         -0.99999593 98.43103943]\n [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n\naction = 1\naction-value function = \n [[-1.         -1.         -1.         -9.98591939]\n [-1.         -1.         -0.99999996 -9.99588862]\n [-1.         -0.99999999 -0.99999593 -9.92731143]\n [-0.99915694 -0.99971289 98.50948746  0.        ]]\n\naction = 2\naction-value function = \n [[-10.         -10.          -9.99999999  -9.99065864]\n [ -1.          -1.          -0.99999999  -0.99923914]\n [ -1.          -1.          -0.99999321  -0.9884667 ]\n [ -0.99946866  -0.99981905  -0.99465672   0.        ]]\n\naction = 3\naction-value function = \n [[-10.          -1.          -1.          -0.99919909]\n [-10.          -1.          -1.          -0.99866234]\n [ -9.99999999  -1.          -0.99999285  -0.99541881]\n [ -9.99347658  -0.99987363  -0.99776587   0.        ]]\n\n\n\n\n\n환경의 깊은 이해 (좀 더 고차원적인 이해)\n- action=1 일때 각 state의 가치 (=기대보상)\n\nq[:,:,1]\n\narray([[-1.        , -1.        , -1.        , -9.98591939],\n       [-1.        , -1.        , -0.99999996, -9.99588862],\n       [-1.        , -0.99999999, -0.99999593, -9.92731143],\n       [-0.99915694, -0.99971289, 98.50948746,  0.        ]])\n\n\n- 분석1\n\nq[3,2,1]\n\n98.50948746175251\n\n\n\n상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은 100근처 –&gt; 합리적임\n\n- 분석2\n\nq[3,1,1]\n\n-0.9997128867462345\n\n\n\n상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은 -1 근처 –&gt; 합리적일까??\n\n- 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지 합리적이지 못함\n- 상황상상\n\n빈 종이를 줌\n빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n0을 쓸때와 1을 쓸때 보상이 다름\n무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면 10만원을 보상을 준다는 것을 “알게 되었음”\n이때 빈 종이의 가치는 5만원인가? 10만원인가? –&gt; 10만원아니야?\n\n- 직관: 생각해보니 현재 \\(s=(3,1)\\) \\(a=1\\)에서 추정된(esitated) 값은 q[3,1,1]= -0.9997128867462345 이지만1, 현실적으로는 “실제보상(-1)과 잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n1 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음\nq_estimated = q[3,1,1]\nq_estimated\n\n-0.9997128867462345\n\n\n\nq_realistic = (-1) + 0.99 * 100 \nq_realistic\n\n98.0\n\n\n\n여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를 결정하는 가중치” 이다.\n1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이= 십만원 으로 생각한다는 의미)\n\n- 즉 \\(q(s,a)\\)는 모든 \\(s\\), \\(a\\)에 대하여\n\\[q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)\\]\n가 성립한다면 \\(q(s,a)\\)는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을 좀 더 엄밀하게 쓰면 아래와 같다.\n\\[q(s,a) \\approx \\begin{cases} \\text{reward}(s,a) & \\text{terminated} \\\\  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated}\\end{cases}\\]\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    xx,yy = agent.next_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a] \n    if agent.terminations[i]:\n        q_realistic = agent.rewards[i]\n    else:\n        q_future = q[xx,yy,:].max()\n        q_realistic = agent.rewards[i] + 0.99 * q_future\n    diff = q_realistic - q_estimated \n    q[x,y,a] = q_estimated + 0.05 * diff \n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[87.02554961 88.94759484 90.75390245 88.54847007]\n [88.4709728  91.06852327 93.18709107 94.21998722]\n [84.98258538 91.44091272 95.48024593 98.43103943]\n [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n\naction = 1\naction-value function = \n [[87.01670813 88.59888111 85.52951661 -9.98591939]\n [88.98190464 91.03081993 91.50379877 -9.99588862]\n [90.76721433 93.24316728 95.65715857 -9.92731143]\n [89.20612688 94.47295823 98.50948746  0.        ]]\n\naction = 2\naction-value function = \n [[-10.         -10.          -9.99999999  -9.99065864]\n [ 84.96179325  86.84873675  88.0518007   80.10750712]\n [ 86.40784936  88.69218405  89.83203868  83.06339754]\n [ 86.40852121  89.09508079  89.87262647   0.        ]]\n\naction = 3\naction-value function = \n [[-10.          84.96186287  86.49128928  84.57992176]\n [-10.          86.73523202  88.56505447  86.7154156 ]\n [ -9.99999999  88.3058275   90.27264766  87.96618484]\n [ -9.99347658  80.88548565  86.63274331   0.        ]]\n\n\n\n\n\n행동 전략 수립\n- 상태 (0,0)에 있다고 가정해보자.\n\nq[0,0,:]\n\narray([ 87.02554961,  87.01670813, -10.        , -10.        ])\n\n\n\n행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n\n- 상태 (2,3)에 있다고 가정해보자.\n\nq[2,3,:]\n\narray([98.43103943, -9.92731143, 83.06339754, 87.96618484])\n\n\n\n행동 0을 하는게 유리함.\n\n- 상태 (3,2)에 있다고 가정해보자.\n\nq[3,2,:]\n\narray([-9.93439857, 98.50948746, 89.87262647, 86.63274331])\n\n\n\n행동1을 하는게 유리함\n\n- 각 상태에서 최적은 action은 아래와 같다.\n\nq[0,0,:].argmax()\n\n0\n\n\n\nq[2,3,:].argmax()\n\n0\n\n\n\nq[3,2,:].argmax()\n\n1\n\n\n- 전략(=정책)을 정리해보자.\n\npolicy = np.array(['?????']*16).reshape(4,4)\npolicy\n\narray([['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????']], dtype='&lt;U5')\n\n\n\ndirections = {0:'down', 1: 'right', 2:'up', 3:'left'} \n\n\nfor x in range(4):\n    for y in range(4):\n        policy[x,y] = directions[q[x,y,:].argmax()]\npolicy\n\narray([['down', 'down', 'down', 'down'],\n       ['right', 'down', 'down', 'down'],\n       ['right', 'right', 'right', 'down'],\n       ['right', 'right', 'right', 'down']], dtype='&lt;U5')\n\n\n\nq.max(axis=-1)\n\narray([[87.02554961, 88.94759484, 90.75390245, 88.54847007],\n       [88.98190464, 91.06852327, 93.18709107, 94.21998722],\n       [90.76721433, 93.24316728, 95.65715857, 98.43103943],\n       [89.20612688, 94.47295823, 98.50948746,  0.        ]])\n\n\n\n\nAgent2 클래스 구현 + Run\n\nclass Agent2(Agent1):\n    def __init__(self,env):\n        super().__init__(env)\n        self.q = np.zeros([4,4,4]) \n    def learn(self):\n        x,y = self.current_state\n        xx,yy = self.next_state\n        a = self.action \n        q_estimated = self.q[x,y,a] \n        if self.terminated:\n            q_realistic = self.reward\n        else:\n            q_future = q[xx,yy,:].max()\n            q_realistic = self.reward + 0.99 * q_future\n        diff = q_realistic - q_estimated \n        self.q[x,y,a] = q_estimated + 0.05 * diff \n    def act(self):\n        if self.n_experiences &lt; 3000: \n            self.action = self.action_space.sample() \n        else:\n            x,y = self.current_state \n            self.action = self.q[x,y,:].argmax()\n\n\nenv = GridWorld() \nagent = Agent2(env) \nfor _ in range(2000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n    ## 덜 본질적인 코드 \n    if (agent.n_episodes % 100) ==0:\n        print(\n            f\"Epsiode: {agent.n_episodes} \\t\"\n            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n            f\"Playtime: {np.mean(agent.playtimes[-100:])}\"\n        )   \n\nEpsiode: 100    Score: -10.36   Playtime: 3.56\nEpsiode: 200    Score: -10.9    Playtime: 3.0\nEpsiode: 300    Score: -11.02   Playtime: 3.12\nEpsiode: 400    Score: -6.64    Playtime: 4.24\nEpsiode: 500    Score: -11.08   Playtime: 3.18\nEpsiode: 600    Score: -10.53   Playtime: 3.73\nEpsiode: 700    Score: -9.96    Playtime: 3.16\nEpsiode: 800    Score: -8.6     Playtime: 2.9\nEpsiode: 900    Score: -13.6    Playtime: 7.61\nEpsiode: 1000   Score: -50.0    Playtime: 50.0\nEpsiode: 1100   Score: -50.0    Playtime: 50.0\nEpsiode: 1200   Score: -50.0    Playtime: 50.0\nEpsiode: 1300   Score: -50.0    Playtime: 50.0\nEpsiode: 1400   Score: -50.0    Playtime: 50.0\nEpsiode: 1500   Score: -50.0    Playtime: 50.0\nEpsiode: 1600   Score: -50.0    Playtime: 50.0\nEpsiode: 1700   Score: -50.0    Playtime: 50.0\nEpsiode: 1800   Score: -50.0    Playtime: 50.0\nEpsiode: 1900   Score: -50.0    Playtime: 50.0\nEpsiode: 2000   Score: -50.0    Playtime: 50.0\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nagent.q.max(-1).T\n\narray([[87.0920856 , 89.15783804, 76.99095205, 51.80408762],\n       [88.62997645, 91.25522016, 82.83298477, 55.040804  ],\n       [91.25522016, 88.84636342, 73.50910388, 22.62190625],\n       [40.8373347 , 51.7638053 , 45.96399123,  0.        ]])\n\n\n\n\nAgnet3 클래스 구현 + Run\n\nclass Agent3(Agent2):\n    def __init__(self,env):\n        super().__init__(env)\n        self.eps = 0 \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample() \n        else:\n            x,y = self.current_state \n            self.action = self.q[x,y,:].argmax()\n\n\nenv = GridWorld() \nagent = Agent3(env) \nagent.eps = 1\nfor _ in range(5000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1\n    agent.eps = agent.eps * 0.999\n    ## 덜 본질적인 코드 \n    if (agent.n_episodes % 200) ==0:\n        print(\n            f\"Epsiode: {agent.n_episodes} \\t\"\n            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n            f\"Epsilon: {agent.eps : .2f}\"\n        )   \n\nEpsiode: 200    Score: -8.49    Playtime: 3.89  Epsilon:  0.82\nEpsiode: 400    Score: -9.83    Playtime: 4.13  Epsilon:  0.67\nEpsiode: 600    Score: -10.72   Playtime: 6.12  Epsilon:  0.55\nEpsiode: 800    Score: -7.08    Playtime: 7.98  Epsilon:  0.45\nEpsiode: 1000   Score: -1.87    Playtime: 10.65 Epsilon:  0.37\nEpsiode: 1200   Score: 28.23    Playtime: 10.16 Epsilon:  0.30\nEpsiode: 1400   Score: 61.38    Playtime: 6.62  Epsilon:  0.25\nEpsiode: 1600   Score: 66.42    Playtime: 5.98  Epsilon:  0.20\nEpsiode: 1800   Score: 74.94    Playtime: 6.26  Epsilon:  0.17\nEpsiode: 2000   Score: 75.29    Playtime: 5.91  Epsilon:  0.14\nEpsiode: 2200   Score: 77.24    Playtime: 6.16  Epsilon:  0.11\nEpsiode: 2400   Score: 86.1     Playtime: 6.1   Epsilon:  0.09\nEpsiode: 2600   Score: 83.81    Playtime: 6.19  Epsilon:  0.07\nEpsiode: 2800   Score: 87.27    Playtime: 6.03  Epsilon:  0.06\nEpsiode: 3000   Score: 86.1     Playtime: 6.1   Epsilon:  0.05\nEpsiode: 3200   Score: 87.37    Playtime: 5.93  Epsilon:  0.04\nEpsiode: 3400   Score: 93.68    Playtime: 6.22  Epsilon:  0.03\nEpsiode: 3600   Score: 90.58    Playtime: 6.02  Epsilon:  0.03\nEpsiode: 3800   Score: 92.77    Playtime: 6.03  Epsilon:  0.02\nEpsiode: 4000   Score: 93.79    Playtime: 6.11  Epsilon:  0.02\nEpsiode: 4200   Score: 94.88    Playtime: 6.12  Epsilon:  0.01\nEpsiode: 4400   Score: 92.85    Playtime: 5.95  Epsilon:  0.01\nEpsiode: 4600   Score: 94.96    Playtime: 6.04  Epsilon:  0.01\nEpsiode: 4800   Score: 94.92    Playtime: 6.08  Epsilon:  0.01\nEpsiode: 5000   Score: 93.9     Playtime: 6.0   Epsilon:  0.01\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-13-7wk-1.html",
    "href": "posts/2. 마코프체인/2023-04-13-7wk-1.html",
    "title": "07wk-1: 마코프체인 (2)",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wi47mCKi03xoqvwIzkxG0H\n\n\n\nimport\n\nimport numpy as np\n\n\n\n확률과정\n- 동전을 무한히 던지는 시행을 생각하자. 동전을 10번 던져서 결과를 관찰했다고 하자. 동전을 30번째 던져서 앞면이 나올지 뒷면이 나올지 알고 싶다면?\n- 현재 삼성전자 주가는 66000이다. 20일뒤의 삼성전자 주가가 얼마일지 알고 싶다면?\n- 원래 미래를 예측하기 위해서 해야하는 과정\n\n\n\n그림1: 1400만개의 미래를 탐색중인 Doctor Strange\n\n\n- 하지만 현실적으로는 이게 너무 힘들지 않을까?\n\n\n날씨예측\n- 아래와 같이 세상의 법칙이 있다고 하자.\n\n어제 맑음 \\(\\to\\) 오늘도 맑음: 40% // 오늘은 비: 60%\n어제 비 \\(\\to\\) 오늘은 맑음: 70% // 오늘도 비 30%\n\n- 모든 \\(t\\)에 대하여 확률변수 \\(X_t\\)를 아래와 같이 정의하자.\n\n\\(X_t=\\begin{cases} 0 & \\text{맑음} \\\\ 1 & \\text{비} \\end{cases}\\)\n\n- 오늘 (2023년4월13일) 비가 왔다고 치자. 10000일 뒤에도 비가 올 확률은 얼마일까?\n\n\n풀이1\n- \\(X_t=0\\) 이라면? (\\(t\\)시점에 비가 오지 않았다면?)\n\nnp.random.rand() &lt; 0.6 \n\nFalse\n\n\n- \\(X_t=1\\) 이라면? (\\(t\\)시점에 비가 왔다면?)\n\nnp.random.rand(0) &lt; 0.3\n\narray([], dtype=bool)\n\n\n- 두 코드를 합쳐보자.\n\ndef rain(before):\n    if before == True: # 비가 왔음 \n        after = np.random.rand() &lt; 0.3\n    else: # 비가 안왔음 \n        after = np.random.rand() &lt; 0.6 \n    return after \n\n- 테스트\n\n# 비가 왔음, Xt = 1 \nsum([rain(1) for i in range(100)])\n\n30\n\n\n\n# 비가 안왔음, Xt = 0 \nsum([rain(0) for i in range(100)])\n\n60\n\n\n- 하나의 \\(\\omega\\)에 대응하는 길이가 10000인 확률과정을 관찰\n\ndef doctor_strange(today):\n    lst = [today]\n    for i in range(10000): \n        lst.append(rain(lst[i]))\n    return lst \n\n\ntoday = True # 오늘 비가 왔다는 뜻 \narr = doctor_strange(today)\n\n\nlen(arr)\n\n10001\n\n\n- 4305개의 \\(\\omega\\)에 대응하는 길이가 10000인 확률과정을 관찰\n\ntoday = True # 오늘 비가 왔다는 뜻 \narr = np.array([doctor_strange(today) for ω in range(4305)])\n\n\narr[:,-1].mean()\n\n0.4662020905923345\n\n\n- 10000일 뒤에도 비가 올 확률은 약 46% 정도 인듯\n\n\n풀이2\n- 세상의 법칙을 다시 정리해보자.\n\n\\(X_{t-1}=0 \\Rightarrow X_t \\sim Ber(0.6)\\)\n\\(X_{t-1}=1 \\Rightarrow X_t \\sim Ber(0.3)\\)\n\n- 정리하면\n\n\\(P(X_t=0)= P(X_{t-1}=0) \\times 0.4 + P(X_{t-1}=1) \\times 0.7\\)\n\\(P(X_t=1)= P(X_{t-1}=0) \\times 0.6 + P(X_{t-1}=1) \\times 0.3\\)\n\n- 매트릭스형태로 표현하면\n\n\\(\\begin{bmatrix} P(X_t=0) \\\\ P(X_t=1) \\end{bmatrix}= \\begin{bmatrix} 0.4 & 0.7 \\\\ 0.6 & 0.3 \\end{bmatrix} \\begin{bmatrix} P(X_{t-1}=0) \\\\ P(X_{t-1}=1) \\end{bmatrix}\\)\n\\({\\boldsymbol \\mu}_t = {\\bf P} {\\boldsymbol \\mu}_{t-1}\\)\n\n- 이렇게 놓고 보니까\n\n\\({\\boldsymbol \\mu}_1 ={\\bf P}{\\boldsymbol \\mu}_0\\)\n\\({\\boldsymbol \\mu}_2 ={\\bf P}{\\boldsymbol \\mu}_1={\\bf P}^2{\\boldsymbol \\mu}_0\\)\n\\(\\dots\\)\n\\({\\boldsymbol \\mu}_{10000} ={\\bf P}^{10000}{\\boldsymbol \\mu}_0\\)\n\n- 이제 계산을 해보자.\n\nμ0 = np.array([[0],[1]])\nμ0\n\narray([[0],\n       [1]])\n\n\n\nP = np.array([[0.4,0.7],[0.6,0.3]])\nP\n\narray([[0.4, 0.7],\n       [0.6, 0.3]])\n\n\n\nP@P # P의 제곱\n\narray([[0.58, 0.49],\n       [0.42, 0.51]])\n\n\n\nP@P@P@P # P의 4제곱\n\narray([[0.5422, 0.5341],\n       [0.4578, 0.4659]])\n\n\n\nP@P@P@P @ P@P@P@P # P의 8제곱 \n\narray([[0.53849182, 0.53842621],\n       [0.46150818, 0.46157379]])\n\n\n\nP@P@P@P@P@P@P@P @ P@P@P@P@P@P@P@P # P의 16제곱 \n\narray([[0.53846154, 0.53846154],\n       [0.46153846, 0.46153846]])\n\n\n\\({\\bf P}\\)가 수렴하는거 같지 않어?\n\nP@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P @ P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P \n\narray([[0.53846154, 0.53846154],\n       [0.46153846, 0.46153846]])\n\n\n대충 \\({\\bf P}^{10000} \\approx {\\bf P}^{32}\\)\n\nPlim = P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P @ P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P \nPlim \n\narray([[0.53846154, 0.53846154],\n       [0.46153846, 0.46153846]])\n\n\n\nPlim @ μ0\n\narray([[0.53846154],\n       [0.46153846]])\n\n\n- 이 풀이에 따르면 10000일 뒤에 비가 올 확률은 46% 정도이다.\n\n\n풀이3\n- 세상의 법칙을 다시 정리해보자.\n\n\\(X_{t-1}=0 \\Rightarrow X_t \\sim Ber(0.6)\\)\n\\(X_{t-1}=1 \\Rightarrow X_t \\sim Ber(0.3)\\)\n\n- 추측: 10000일 뒤에 비가 올 확률이 \\(p\\)라고 치자. 그렇다면 9999일 뒤에 비가 올 확률도 \\(p\\) 아닐까?\n이걸 가정하고 계산해보자\n1. 9999일 뒤에 비가 안 올 확률 \\(1-p\\)\n\n9999일 뒤에 비가 안오고, 10000일 뒤에는 비가 올 확률: \\(0.6(1-p)\\)\n9999일 뒤에 비가 안오고, 10000일 뒤에는 비가 안 올 확률: \\(0.4(1-p)\\)\n\n2. 9999일 뒤에 비가 올 확률 \\(p\\)\n\n9999일 뒤에 비가 오고, 10000일 뒤에도 비가 올 확률: \\(0.3p\\)\n9999일 뒤에 비가 오고, 10000일 뒤에는 비가 안 올 확률: \\(0.7p\\)\n\n따라서 \\(0.6(1-p) + 0.3p = p\\)\n풀어보면 \\(0.6/1.3 =p\\)\n\n0.6/1.3\n\n0.4615384615384615\n\n\n\n\n풀이4\n\nnp.mean(doctor_strange(True)[1:])\n\n0.462"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-09-10wk-2.html",
    "href": "posts/2. 마코프체인/2023-05-09-10wk-2.html",
    "title": "10wk-2: 마코프체인 (7)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-yMXZ2TSGxoh-rIjn8vp6cV"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제1-단위행렬",
    "href": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제1-단위행렬",
    "title": "10wk-2: 마코프체인 (7)",
    "section": "예제1: 단위행렬",
    "text": "예제1: 단위행렬\nHMC \\(\\{X_t\\}\\)의 전이행렬이 아래와 같다고 하자.\n\nP = np.array([[1,0],\n              [0,1]])\nP\n\narray([[1, 0],\n       [0, 1]])\n\n\n\\(\\{X_t\\}\\)는 유일한 정상분포를 가지는가? 가진다면 시간평균을 이용하여 정상분포를 근사하라.\n(풀이)\n이 경우는 IRR 조건이 만족되지 않으므로 유일한 정상분포가 존재하지 않음. 그래서 에르고딕정리를 이용할 수 없다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제2-순환이동",
    "href": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제2-순환이동",
    "title": "10wk-2: 마코프체인 (7)",
    "section": "예제2: 순환이동",
    "text": "예제2: 순환이동\nHMC \\(\\{X_t\\}\\)의 전이행렬이 아래와 같다고 하자.\n\nP = np.array([[0,1],\n              [1,0]])\nP\n\narray([[0, 1],\n       [1, 0]])\n\n\n\\(\\{X_t\\}\\)는 유일한 정상분포를 가지는가? 가진다면 시간평균을 이용하여 정상분포를 구하여라.\n(풀이)\n\\(\\{X_t\\}\\)는 finite and irreducible HMC 이므로 유일한 정상분포를 가진다. 시뮬레이션을 한다면\n\n\\(0,1,0,1,0,1,0, \\dots\\)\n\\(1,0,1,0,1,0,1, \\dots\\)\n\n중 하나의 열(array)이 관찰 될 것이고 두 경우 모두\n\n\\(\\big(\\frac{1}{T}\\sum_{t=0}^{T-1}I(X_t=0),\\frac{1}{T}\\sum_{t=0}^{T-1}I(X_t=1)\\big)=(\\hat{\\pi}_0,\\hat{\\pi}_1)\\approx (1/2,1/2)\\)\n\n와 같이 구할 수 있음"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제3-비가-온다-안온다",
    "href": "posts/2. 마코프체인/2023-05-09-10wk-2.html#예제3-비가-온다-안온다",
    "title": "10wk-2: 마코프체인 (7)",
    "section": "예제3: 비가 온다, 안온다",
    "text": "예제3: 비가 온다, 안온다\nHMC \\(\\{X_t\\}\\)의 전이행렬이 아래와 같다고 하자.\n\nP = np.array([[0.4,0.6],\n              [0.7,0.3]])\nP\n\narray([[0.4, 0.6],\n       [0.7, 0.3]])\n\n\n\\(\\{X_t\\}\\)는 유일한 정상분포를 가지는가? 가진다면 시간평균을 이용하여 정상분포를 구하여라.\n(풀이) 이 강의노트의 풀이4\n\ndef rain(before):\n    if before == True: # 비가 왔음 \n        after = np.random.rand() &lt; 0.3\n    else: # 비가 안왔음 \n        after = np.random.rand() &lt; 0.6 \n    return after \n\n\ndef doctor_strange(today):\n    lst = [today]\n    for i in range(10000): \n        lst.append(rain(lst[i]))\n    return lst \n\n\nnp.mean(doctor_strange(True)[1:])\n\n0.4616"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xlV_TS7zhmYyyYNKv8np4W"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#확률변수의-평균",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#확률변수의-평균",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "확률변수의 평균",
    "text": "확률변수의 평균\n- 예제1: 동전을 던지는 예제\n\n\n\n\\(\\omega\\)\n\\(x=X(\\omega)\\)\n\\(P(X=x)\\)\n\n\n\n\n\\(\\omega_1\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\omega_2\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\n\\[\\therefore E(X)=\\sum_{x=0}^{1}x P(X=x) = \\big(0\\times \\frac{1}{2} + 1 \\times \\frac{1}{2} \\big)=\\frac{1}{2}(0+1)\\]\n- 예제2: 주사위를 던지는 예제\n\n\n\n\\(\\omega\\)\n\\(x=X(\\omega)\\)\n\\(P(X=x)\\)\n\n\n\n\n\\(\\omega_1\\)\n\\(1\\)\n\\(\\frac{1}{6}\\)\n\n\n\\(\\omega_2\\)\n\\(2\\)\n\\(\\frac{1}{6}\\)\n\n\n\\(\\omega_3\\)\n\\(3\\)\n\\(\\frac{1}{6}\\)\n\n\n\\(\\omega_4\\)\n\\(4\\)\n\\(\\frac{1}{6}\\)\n\n\n\\(\\omega_5\\)\n\\(5\\)\n\\(\\frac{1}{6}\\)\n\n\n\\(\\omega_6\\)\n\\(6\\)\n\\(\\frac{1}{6}\\)\n\n\n\n\\[\\therefore E(X)=\\sum_{x=1}^{6}xP(X=x)=\\frac{1}{6}(1+2+3+4+5+6)=3\\]"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#확률벡터의-평균",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#확률벡터의-평균",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "확률벡터의 평균",
    "text": "확률벡터의 평균\n- 예제1: 동전을 2회 던지는 예제\n\n\n\n\n\n\n\n\n\\(\\omega\\)\n\\({\\boldsymbol x}={\\boldsymbol X}(\\omega)\\)\n\\(P({\\boldsymbol X}={\\boldsymbol x})\\)\n\n\n\n\n\\(\\omega_1\\)\n\\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\omega_2\\)\n\\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\omega_1\\)\n\\(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\omega_2\\)\n\\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\\(\\frac{1}{4}\\)\n\n\n\n\\[\\therefore E({\\boldsymbol X})=\\frac{1}{4}\\left(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}+\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}+\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}+\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\right)=\\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} E(X_1)\\\\ E(X_2) \\end{bmatrix}\\]\n\n\\(E(X_1)=E(X_2)\\)인 이유?? iid 이니까~\n\n- 예제2: 동전을 10회 던지는 예제\n\n\n\n\n\n\n\n\n\\(\\omega\\)\n\\({\\boldsymbol x}={\\boldsymbol X}(\\omega)\\)\n\\(P({\\boldsymbol X}={\\boldsymbol x})\\)\n\n\n\n\n\\(\\omega_1\\)\n\\([0,0,\\dots,0]^\\top\\)\n\\(\\frac{1}{2^{10}}\\)\n\n\n\\(\\omega_2\\)\n\\([0,0,\\dots,1]^\\top\\)\n\\(\\frac{1}{2^{10}}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(\\omega_{1024}\\)\n\\([1,1,\\dots,1]^\\top\\)\n\\(\\frac{1}{2^{10}}\\)\n\n\n\n\\[\\therefore E({\\boldsymbol X})=\\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\dots \\\\ \\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} E(X_1)\\\\ E(X_2) \\\\ \\dots \\\\ E(X_{10}) \\end{bmatrix}\\]"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#motivating-example",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#motivating-example",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "motivating example",
    "text": "motivating example\n- 예제1: 동전을 1000번 던지는 예제를 상상하자. 앞면이 나올 확률은 \\(p\\)이며 이 \\(p\\)는 0.5인지 모른다고 가정하자.\n\nimport numpy as np \n\n\nunknown_probability = np.random.rand()\n\n\nx = np.random.binomial(n=1,p=unknown_probability,size=1000) # X(ω) for some ω\nx\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n       1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 1])\n\n\n\n이것은 적당한 \\(\\omega\\)에 맵핑되어있는 하나의 realization 이다.\n\n- 질문: unknown_probability는 얼마일까??\n\nnp.mean(x), unknown_probability\n\n(0.796, 0.7863482228867129)\n\n\n- 비판: 문제 이상하게 푼다?\n\n\n\n\n\n\n\n\n\\(\\omega\\)\n\\({\\boldsymbol x}={\\boldsymbol X}(\\omega)\\)\n\\(P({\\boldsymbol X}={\\boldsymbol x})\\)\n\n\n\n\n\\(\\omega_1\\)\n\\([0,0,\\dots,0]^\\top\\)\n\\(\\frac{1}{2^{1000}}\\)\n\n\n\\(\\omega_2\\)\n\\([0,0,\\dots,1]^\\top\\)\n\\(\\frac{1}{2^{1000}}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(\\omega_{2^{1000}}\\)\n\\([1,1,\\dots,1]^\\top\\)\n\\(\\frac{1}{2^{1000}}\\)\n\n\n\n\\[\\therefore E({\\boldsymbol X})= \\begin{bmatrix} E(X_1)\\\\ E(X_2) \\\\ \\dots \\\\ E(X_{1000}) \\end{bmatrix}\\]\n\n\\(E(X_{1000})=\\frac{1}{2^{1000}}\\big(\\text{대충 0 혹은 1이 있는 숫자들을 더한것}\\big)=p\\)\n\n\nx[-1] # 이게 하나의 X_{1000} 에 대한 하나의 실현치일 뿐임. \n\n1\n\n\n따라서 개념상으로는 아래와 같이 시뮬레이션하여 구하는게 옳음\n\nsample1 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample2 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample3 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample4 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample5 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample6 = np.random.binomial(n=1,p=unknown_probability,size=1000) \nsample7 = np.random.binomial(n=1,p=unknown_probability,size=1000) \n\n\n(sample1[-1]+sample2[-1]+sample3[-1]+sample4[-1]+sample5[-1]+sample6[-1]+sample7[-1])/7\n\n0.8571428571428571\n\n\n\nunknown_probability\n\n0.7863482228867129\n\n\n좀 더 많이…\n\nsamples = np.stack([np.random.binomial(n=1,p=unknown_probability,size=1000) for i in range(43052)])\nsamples\n\narray([[1, 1, 1, ..., 1, 0, 1],\n       [1, 0, 1, ..., 0, 1, 1],\n       [1, 0, 0, ..., 1, 1, 1],\n       ...,\n       [1, 1, 1, ..., 1, 1, 0],\n       [0, 1, 0, ..., 1, 1, 1],\n       [1, 0, 1, ..., 1, 1, 1]])\n\n\n\nsamples.shape\n\n(43052, 1000)\n\n\n\nnp.mean(samples[:,-1]) # E(X_{1000})을 근사한것\n\n0.7862120226702592"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#용어정리의-시간",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#용어정리의-시간",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "용어정리의 시간",
    "text": "용어정리의 시간\n- 확률변수열을 표현할 때 \\(i\\)대신 \\(t\\)로 바꾼다면?\n\n\\(X_1,X_2,X_3,\\dots, X_i, \\dots, X_n\\) \\(\\Rightarrow\\) \\(X_1,X_2,X_3\\dots,X_t,\\dots X_T\\)\n\\(E(X_i)\\) \\(\\Rightarrow\\) \\(E(X_t)\\)\n\\(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\) \\(\\Rightarrow\\) \\(\\frac{1}{T}\\sum_{t=1}^{T}X_t\\)\n\n- 용어: \\(E(X_t)\\)를 앙상블평균 (ensemble average) 이라고 하고, \\(\\frac{1}{T}\\sum_{t=1}^{T}X_t\\)를 시간평균 (time average) 이라고 한다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#생각의-시간-1",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#생각의-시간-1",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "생각의 시간 (1)",
    "text": "생각의 시간 (1)\n- 원래 \\(E(X_{1000})\\)은 \\(\\frac{1}{T}\\sum_{t=1}^{T}X_t\\)와 같은 방식으로 근사계산할 수 없긴해. (말도 안되는 소리임..)\n- 예제1: 아래와 같은 확률변수열를 고려하자.\n\n\\(X_1 \\sim Ber(0.5)\\).\n\\(X_t= X_{t-1}\\) for \\(t=2,3,4,\\dots, 1000\\).\n\n\\(E(X_{1000})\\)을 구하여라. \\(E(X_{1000})\\)을 \\(\\frac{1}{T}\\sum_{t=1}^T X_t\\)와 같은 방식으로 근사할 수 있는가?\n(풀이)\n\\(E(X_{1000})=0.5\\)임. 하지만 \\(\\frac{1}{T}\\sum_{t=1}^{T}X_t\\)로 \\(E(X_{1000})\\)을 근사할 수 없음.\n시뮬1 – calculating time average of one-sample \\((x_1,\\dots,x_{1000})\\)\n\nx1 = np.random.binomial(n=1,p=0.5,size=1).item()\nx1\n\n0\n\n\n\none_sample = np.array([x1]*1000)\none_sample\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nnp.mean(one_sample)\n\n0.0\n\n\n시뮬2 – approximating ensemble average with 43052 samples\n\nsamples = np.array([[np.random.binomial(n=1,p=0.5,size=1).item()] * 1000 for i in range(43052)])\nsamples \n\narray([[1, 1, 1, ..., 1, 1, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]])\n\n\n\nnp.mean(samples[:,-1])\n\n0.5003251881445694\n\n\n- 하지만 사실 iid가정이 있다면 앙상블평균을 시간평균으로 추정해도 문제 없어.\n- 예제2: 서로 독립인 1000개의 확률변수를 \\(N(0,1)\\)에서 뽑는다고 하자.\n\n\\(X_t=\\epsilon_t \\overset{i.i.d.}{\\sim} N(0,1)\\)\n\n이때는 \\(E(X_{1000})\\)을 \\(\\frac{1}{T}\\sum_{t=1}^T X_t\\)와 같은 방식으로 근사할 수 있다.\n시뮬1 – calculating time average of one-sample \\((x_1,\\dots,x_{1000})\\)\n\none_sample = np.random.binomial(1,0.5,1000)\nnp.mean(one_sample)\n\n0.536\n\n\n시뮬2 – approximating ensemble average with 43052 samples\n\nnp.stack([np.random.binomial(1,0.5,1000) for i in range(43052)])[:,-1].mean()\n\n0.4999535445507758\n\n\n- 결론: 원래 time-average와 ensemble-average는 “전혀” 다른 개념이다. 그런데, 확률변수열이 iid일 경우는 time-average로 ensemble-average를 근사계산 할 수 있다.\n- 아래의 그림은 time-average와 ensemble-average의 차이를 파악하기 용이한 예제이다.\n\n\n\n그림1: Davidson (1994) 에서 발췌한 그림."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#ar1",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#ar1",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "AR(1)",
    "text": "AR(1)\n- 예제3: \\(\\epsilon_t \\overset{i.i.d.}{\\sim} N(0,1)\\) 일 때, 아래와 같은 확률변수 열을 고려하자.\n\n\\(X_1=\\epsilon_1\\)\n\\(X_t=\\frac{7}{8}X_{t-1} + \\epsilon_t\\) for \\(t=2,3,\\dots,T\\)\n\n\neps = np.random.randn(1000)\nx = np.zeros(1000)\nx[0] = eps[0]\nfor t in range(1,1000):\n    x[t] = (7/8)*x[t-1] +eps[t]\n\n\nimport matplotlib.pyplot as plt \n\n\nplt.plot(x,'--o',alpha=0.5)\n\n\n\n\n이때 \\(E(X_{T})\\)을 \\(\\frac{1}{T}\\sum_{t=1}^T X_t\\)와 같은 방식으로 근사할 수 있을까?\n(풀이)\n우선 독립인지 아닌지 체크해보자.\ncheck: \\(X_t\\)와 \\(X_{t-1}\\)은 독립??\n\nplt.plot(x[:-1],x[1:],'o',alpha=0.2)\n\n\n\n\n\ncorr이 있음.. \\(\\Rightarrow\\) 독립아님 \\(\\Rightarrow\\) ensemble-average를 time-average로 근사할 수 없다??\n\n\n참고로 독립이라면~\n\nplt.plot(eps,'--o',alpha=0.5)\n\n\n\n\n\nplt.plot(eps[1:],eps[:-1],'o',alpha=0.2)\n\n\n\n\n\n시뮬1 – calculating time average of one-sample \\((x_1,\\dots,x_{T})\\)\n\ndef gen(T=1000):\n    eps = np.random.randn(T)\n    x = np.zeros(T)\n    x[0] = eps[0]\n    for t in range(1,T):\n        x[t] = (7/8)*x[t-1] +eps[t]\n    return x\n\n\none_sample = gen()\nnp.mean(one_sample)\n\n-0.04427929741501683\n\n\n시뮬2 – approximating ensemble average with 43052 samples\n\nsamples = np.stack([gen() for ω in range(43052)])\n\n\nnp.mean(samples[:,-1])\n\n-0.001607068412044872\n\n\n근사 되는 것 같은데..?"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-03-30-5wk-1.html#생각의-시간-2",
    "href": "posts/2. 마코프체인/2023-03-30-5wk-1.html#생각의-시간-2",
    "title": "05wk-1: 마코프체인 (1)",
    "section": "생각의 시간 (2)",
    "text": "생각의 시간 (2)\n- 확률변수는 값이 랜덤으로 바뀌는 변수느낌이 아니라 \\(X: \\Omega \\to \\mathbb{R}\\) 인 잴 수 있는 함수임.\n- 확률벡터는 값이 랜덤으로 바뀌는 벡터느낌이 아니라 \\(X: \\Omega \\to \\mathbb{R}^d\\) 인 잴 수 있는 함수임.\n- 동전을 반복하여 던져서 관측한 아래와 같은 확률변수열(=확률벡터)\n\\[0,1,0,0,1,1,\\dots,1\\]\n은 어떠한 \\(\\omega \\in \\Omega\\)에 대응하는 하나의 realization \\({\\boldsymbol X}(\\omega)={\\boldsymbol x}\\) 임. (즉 one-sample임)\n- 그런데 확률변수열을 독립으로 얻었다면 이러한 one-sample을 쪼개서 마치 여러개의 샘플을 얻은것처럼 생각할 수 있으며 이때\n\\[E(X_T)\\approx \\frac{1}{T}\\sum_{t=1}^{T}X_t\\]\n와 같은 방식으로 근사할 수 있음.\n- 사실상 \\(E(X_1)=E(X_2)=\\dots=E(X_T) \\approx \\frac{1}{T}\\sum_{t=1}^{T}X_t\\) 이므로 (\\(\\because\\) iid) 결국 아직 관측되지 않은 미래시점 \\(T+1\\)의 값에 대해서도\n\\[E(X_{T+1}) \\approx \\frac{1}{T}\\sum_{t=1}^T X_t\\]\n라고 주장할 수 있음.\n- 이렇게 one-sample을 여러개의 조각으로 쪼개는 기법은 iid에서만 성립할 것 같음. 만약에 iid가정이 없다면 (시뮬2)와 같은 방식으로 여러샘플을 통하여 ensemble-average를 근사시켜야 함. 정리하면 아래와 같음.\n\none-sample만 관측가능, iid 조건 만족 \\(\\Rightarrow\\) 분석가능\n여러개의 sample 관측가능 , iid 조건 만족 \\(\\Rightarrow\\) 분석가능\none-sample만 관측가능, iid 조건 만족하지 않음 \\(\\Rightarrow\\) 분석불가능??\n여러개의 sample 관측가능 , iid 조건 만족하지 않음 \\(\\Rightarrow\\) 분석가능\n\n- 문제: 그런데 실제로 우리가 다루고 싶은 자료의 형태는 3의 경우가 많다.\n- 소망: 그래서 iid가 아니지만 마치 iid인것 처럼 one-sample을 가지고 분석하고 싶다.\n\n앞으로 해야 할 것: 독립인듯 독립아닌 독립같은 확률과정은 없을까?\n\n\n독립인듯 독립아닌 독립같은 확률과정\n\nfig, ax = plt.subplots(3,3,figsize=(10,10))\nax[0][0].plot(x[:-1],x[1:],'o',alpha=0.1)\nax[0][1].plot(x[:-2],x[2:],'o',alpha=0.1)\nax[0][2].plot(x[:-3],x[3:],'o',alpha=0.1)\nax[1][0].plot(x[:-4],x[4:],'o',alpha=0.1)\nax[1][1].plot(x[:-5],x[5:],'o',alpha=0.1)\nax[1][2].plot(x[:-6],x[6:],'o',alpha=0.1)\nax[2][0].plot(x[:-7],x[7:],'o',alpha=0.1)\nax[2][1].plot(x[:-8],x[8:],'o',alpha=0.1)\nax[2][2].plot(x[:-9],x[9:],'o',alpha=0.1)"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-13-7wk-2.html",
    "href": "posts/2. 마코프체인/2023-04-13-7wk-2.html",
    "title": "07wk-2: 마코프체인 (3)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zAMowm9anbqZG0fCAmFI_1"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-13-7wk-2.html#formular",
    "href": "posts/2. 마코프체인/2023-04-13-7wk-2.html#formular",
    "title": "07wk-2: 마코프체인 (3)",
    "section": "formular",
    "text": "formular\n- 저번시간에 살펴본 날씨모형은 결국 아래와 같은 모형이었다.\n\\[\\begin{bmatrix}\nP(X_{t+1}=0) \\\\\nP(X_{t+1}=1)\n\\end{bmatrix}= \\begin{bmatrix} 0.8 & 0.1 \\\\ 0.2 & 0.9 \\end{bmatrix} \\begin{bmatrix}\nP(X_{t}=0) \\\\\nP(X_{t}=1)\n\\end{bmatrix}\\]\n양변에 트랜스포즈를 취하게 되면\n\\[\\begin{bmatrix}\nP(X_{t+1}=0) &\nP(X_{t+1}=1)\n\\end{bmatrix}= \\begin{bmatrix}\nP(X_{t}=0) &\nP(X_{t}=1)\n\\end{bmatrix}\\begin{bmatrix} 0.8 & 0.2 \\\\ 0.1 & 0.9 \\end{bmatrix} \\]\n수식화하면 아래와 같이 된다. (보통 이러한 형태로 책에 많이 쓰니까 이 형태로 외울것!)\n\\[{\\boldsymbol \\mu}_{t+1}^\\top ={\\boldsymbol \\mu}_{t}^\\top {\\bf P}\\]\n\n참고: \\(X_t\\)는 0 혹은 1의 값을 가질수 있는데, 이렇게 \\(X_t\\)가 가질 수 있는 값들을 모은 공간을 상태공간이라고 하고 기호로는 \\(V=\\{0,1\\}\\)와 같이 표현한다.\n\n\n참고: 여기에서 확률과정 \\(\\{X_t\\}\\)는 이전시점의 값 \\(X_{t-1}\\)에 의하여서만 결정된다. 이러한 확률과정을 마코프체인이라고 한다.\n\n\n참고: 이때 매트릭스 \\({\\bf P}\\)를 transition matrix 라고 한다.\n\n- \\({\\bf P}\\)의 의미 (\\(\\star\\))\n\\({\\bf P}\\)의 각 원소를 아래와 같이 두자.\n\n\\({\\bf P} = \\begin{bmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{bmatrix}\\)\n\n\\({\\bf P}\\)의 \\((i,j)\\)의 원소는 \\(i \\to j\\)로 이동할 확률을 의미한다. 즉 \\(p_{00}\\), \\(p_{01}\\), \\(p_{10}\\), \\(p_{11}\\) 은 각각 아래를 의미한다.\n\n\\(p_{00}\\): \\(0 \\to 0\\)일 확률. 즉 \\(P(X_t = 0 | X_{t-1} = 0)\\)\n\\(p_{01}\\): \\(0 \\to 1\\)일 확률. 즉 \\(P(X_t = 1 | X_{t-1} = 0)\\)\n\\(p_{10}\\): \\(1 \\to 0\\)일 확률. 즉 \\(P(X_t = 0 | X_{t-1} = 1)\\)\n\\(p_{11}\\): \\(1 \\to 1\\)일 확률. 즉 \\(P(X_t = 1 | X_{t-1} = 1)\\)\n\n- \\({\\boldsymbol \\mu}\\)의 의미 (\\(\\star\\))\n\n\\({\\boldsymbol \\mu}_t\\)는 \\(X_t\\)의 pmf를 의미한다.\n\\({\\boldsymbol \\mu}_0\\)는 \\(X_0\\)의 pmf를 의미한다. 즉 초기분포를 의미한다.\n\\({\\boldsymbol \\mu}\\)자체가 어떠한 분포를 의미한다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-13-7wk-2.html#특징들",
    "href": "posts/2. 마코프체인/2023-04-13-7wk-2.html#특징들",
    "title": "07wk-2: 마코프체인 (3)",
    "section": "특징들",
    "text": "특징들\n- 특징1: \\({\\bf P}\\)는 수렴한다. 즉 \\({\\bf P}^{\\infty}\\)가 존재한다.\n\nP = np.array([[0.8, 0.2],[0.1, 0.9]])\nP\n\narray([[0.8, 0.2],\n       [0.1, 0.9]])\n\n\n\nnp.linalg.matrix_power(P,1),np.linalg.matrix_power(P,10),np.linalg.matrix_power(P,30),np.linalg.matrix_power(P,50)\n\n(array([[0.8, 0.2],\n        [0.1, 0.9]]),\n array([[0.35216502, 0.64783498],\n        [0.32391749, 0.67608251]]),\n array([[0.33334836, 0.66665164],\n        [0.33332582, 0.66667418]]),\n array([[0.33333335, 0.66666665],\n        [0.33333333, 0.66666667]]))\n\n\n\nPlim = np.linalg.matrix_power(P,100)\n\n- 특징2: \\({\\bf P}^{\\infty}\\)의 each column은 모두 동일한 값을 가진다. \\(\\Rightarrow\\) \\(\\mu\\)에 어떠한 값을 넣어도 \\({\\boldsymbol \\mu}^\\top{\\bf P}^{\\infty}={\\boldsymbol \\pi}^\\top = [1/3, 2/3]\\) \\(\\Rightarrow\\) \\({\\bf P}\\)의 아무 row 나 선택하여 그것을 \\({\\boldsymbol \\pi}^\\top\\)라고 두자. \\({\\boldsymbol \\pi}\\)는 \\(X_{\\infty}\\)의 pmf가 된다.\n\nμ = np.array([[0.5],[0.5]]) \nμ.T @ Plim\n\narray([[0.33333333, 0.66666667]])\n\n\n\nπ = np.array([1/3,2/3]).reshape(2,1)\nπ\n\narray([[0.33333333],\n       [0.66666667]])\n\n\n\n\\(X_{\\infty}=\\begin{cases} 0 & w.p.~ 1/3 \\\\ 1 & w.p.~ 2/3 \\end{cases}\\)\n\n\n참고: 여기에서 \\({\\boldsymbol \\pi}\\)를 확률과정 \\(\\{X_t\\}\\)의 정상분포 (stationary distribution) 라고 한다.\n\n- 특징3: \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\) 가 성립한다.\n\n근데 이건 왜 이러지?\n\n\nπ.T @ P\n\narray([[0.33333333, 0.66666667]])\n\n\n당연히 다른 분포 \\({\\boldsymbol \\mu}\\)에 대하여서는 성립하지 않음\n\nμ = np.array([[0.5],[0.5]]) \nμ.T @ P\n\narray([[0.45, 0.55]])\n\n\n\n참고: 여기에서 수식 \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\) 자체가 정상분포의 정의가 된다. 즉 마코프체인 \\(\\{X_t\\}\\)의 트랜지션 매트릭스가 \\({\\bf P}\\)일때, \\({\\boldsymbol \\pi}^\\top {\\bf P} = {\\boldsymbol \\pi}^\\top\\)를 만족하는 \\({\\boldsymbol \\pi}\\)가 존재한다면 \\({\\boldsymbol \\pi}\\)를 확률과정 \\(\\{X_t\\}\\)의 정상분포라고 한다.\n\n- 특징4: 초기분포 \\({\\boldsymbol \\mu}_0\\)를 \\({\\boldsymbol \\pi}\\)로 설정하면 \\(\\{X_t\\}\\)는 모든 \\(t\\)에 대하여 동일한 분포를 가진다. (독립은 아니다)\n\nπ # 초기분포: X0의 pmf \n\narray([[0.33333333],\n       [0.66666667]])\n\n\n\nX0 = np.random.rand() &lt; 2/3\n# X0 = np.random.rand() &gt; 0.52941176\n\n\narr = np.array([doctor_strange(np.random.rand() &lt; 2/3) for i in range(4305)])\narr\n\narray([[False, False,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True, False, False],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ..., False, False, False],\n       [False,  True, False, ..., False, False, False]])\n\n\n\nplt.plot(arr[0][-100:])\n\n\n\n\n\narr[:,0]*1\n\narray([0, 1, 1, ..., 1, 1, 0])\n\n\n\narr[:,-1].sum()\n\n2850\n\n\n\narr[:,0].sum()\n\n2840\n\n\n\nfig, ax = plt.subplots(3,3)\nax[0][0].hist(arr[:,0]*1,alpha=0.5);\nax[0][1].hist(arr[:,500]*1,alpha=0.5);\nax[0][2].hist(arr[:,1000]*1,alpha=0.5);\nax[1][0].hist(arr[:,1500]*1,alpha=0.5);\nax[1][1].hist(arr[:,2000]*1,alpha=0.5);\nax[1][2].hist(arr[:,2500]*1,alpha=0.5);\nax[2][0].hist(arr[:,3000]*1,alpha=0.5);\nax[2][1].hist(arr[:,3500]*1,alpha=0.5);\nax[2][2].hist(arr[:,4000]*1,alpha=0.5);\nfig.tight_layout()\n\n\n\n\n\nplt.hist(arr[0]*1)\n\n(array([3512.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n        6489.]),\n array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n특징4의 변형: 초기분포가 \\({\\boldsymbol \\pi}\\)가 아니더라도 적당한 시점 \\(T_0\\) 이후에는 \\(\\{X_t\\}_{t\\geq T_0}\\)는 동일한분포를 가진다고 볼 수 있다.\n\n참고: 특징4는 후에 MCMC를 이해하는 중요한 예제가 된다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-04-13-7wk-2.html#특징3을-위한-약간의-해설",
    "href": "posts/2. 마코프체인/2023-04-13-7wk-2.html#특징3을-위한-약간의-해설",
    "title": "07wk-2: 마코프체인 (3)",
    "section": "특징3을 위한 약간의 해설",
    "text": "특징3을 위한 약간의 해설\n편의상 \\({\\bf P}^{\\star}={\\bf P}^{\\infty}\\) 라고 하자. 이미 살펴본 것 처럼\n\n\\({\\bf P}^\\star {\\bf P} = {\\bf P}^\\star\\)\n\n가 성립한다. 특징2에서 살펴본것 처럼 임의의 \\({\\boldsymbol \\mu}\\)에 대하여 \\({\\boldsymbol \\mu}^\\top {\\bf P}^{\\star} = {\\boldsymbol \\pi}^\\top\\) 가 항상 성립함을 확인할 수 있다. 이 수식을 살짝 변형하면\n\n\\({\\boldsymbol \\mu}^\\top {\\bf P}^{\\star} = {\\boldsymbol \\pi}^\\top\\)\n\\(\\Rightarrow ({\\boldsymbol \\mu}^\\top{\\bf P}^{\\star}){\\bf P} = {\\boldsymbol \\pi}^\\top\\)\n\\(\\Rightarrow {\\boldsymbol \\pi}^\\top{\\bf P} = {\\boldsymbol \\pi}^\\top\\)\n\n이다. 따라서 특징3이 유도된다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-wPUBATQ2c1npACK21EgDsx"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#약한조건-약한정리-강한조건-강한정리",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#약한조건-약한정리-강한조건-강한정리",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "약한조건, 약한정리, 강한조건, 강한정리",
    "text": "약한조건, 약한정리, 강한조건, 강한정리\n- 정리: 어떠한 조건을 만족하면, 어떠한 결론이 나온다.\n\n결론: 우리가 원하는 것.\n조건: 우리가 원하는 것을 얻기 위한 고난과정.\n\n- 결론이 동일하다면 조건이 약할 수록 유리하다.\n\n정리1: 수업에 온라인으로 참석하거나 오프라인으로 참석한다면 모두 출석으로 인정한다.\n정리2: 수업에 오프라인으로 참석할때만 출석으로 인정한다.\n\n\n정리2의 조건이 만족되면 정리1의 조건은 자동으로 만족된다. 따라서 정리2의 조건이 더 강한 조건이다. 조건이 강할수록 불리하므로 정리2가 더 불리하다.\n\n- 조건이 동일하다면 결론이 강한 쪽이 유리하다.\n\n정리1: 중간고사와 기말고사를 모두 응시한다면, B학점 이상이다.\n정리2: 중간고사와 기말고사를 모두 응시한다면, A학점 이상이다.\n\n\n정리2의 결론이 만족되면 정리1의 결론은 자동으로 만족되므로 정리2의 결론이 더 강하다. 결론은 강할수록 유리하므로 정리2가 더 유리하다."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#헷갈리는-표현-infty의-포함",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#헷갈리는-표현-infty의-포함",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "헷갈리는 표현: \\(\\infty\\)의 포함",
    "text": "헷갈리는 표현: \\(\\infty\\)의 포함\n- 자연수집합 \\(\\mathbb{N}\\)은 \\(\\{\\infty\\}\\)를 포함하지 않는다. 마찬가지로 실수집합 \\(\\mathbb{R}\\) 역시 \\(\\{-\\infty\\}, \\{\\infty\\}\\)를 포함하지 않는다. 만약에 이를 포함하고 싶을 경우는 아래와 같이 표현한다.\n\n\\(\\mathbb{R} \\cup \\{-\\infty\\} \\cup \\{\\infty\\} = \\bar{\\mathbb{R}}\\)\n\\(\\mathbb{N} \\cup \\{-\\infty\\}\\)\n\n여기에서 \\(\\bar{\\mathbb{R}}\\)은 확장된 실수라고 부르는데 교재에따라 사용하기도 하고 사용하지 않기도 한다.\n- 만약에 \\(\\mathbb{N}\\)이 \\(\\{\\infty\\}\\)를 포함한다면\n\n\\(\\forall n \\in \\mathbb{N}:~ 0&lt;\\frac{1}{n} \\leq 1\\)\n\n와 같은 표현은 불가능할 것이다.\n- 구간에 대한 표현들: 구간에 대한 몇가지 표현을 정리하면 아래와 같다.\n\n\\((-\\infty, b] = \\{x: x\\leq b, ~x,b \\in \\mathbb{R}\\}\\)\n\\((-\\infty, b) = \\{x: x &lt; b,~ x,b \\in \\mathbb{R}\\}\\)"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#이론의-정리",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#이론의-정리",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "이론의 정리",
    "text": "이론의 정리\n- \\(\\{X_t\\}\\)는 HMC 라고 하자.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaseNO\n대표예제\nFINITE\nIRR(연결)\nAP(비주기)\n\\({\\bf P}\\)의 수렴\n극한분포유일존재\n정상분포존재\n정상분포유일\n에르고딕정리를 만족\n에르고딕\n\n\n\n\n1\n\nO\nX\nX\nX\nX\nO\nX\nX\nX\n\n\n2\n단위행렬\nO\nX\nO\nO\nX\nO\nX\nX\nX\n\n\n3\n순환이동\nO\nO\nX\nX\nX\nO\nO\nO\nX\n\n\n4\n나이스\nO\nO\nO\nO\nO\nO\nO\nO\nO\n\n\n\n- CaseNO==1 의 예제"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#emprical-분포-정상분포-극한분포",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#emprical-분포-정상분포-극한분포",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "Emprical 분포, 정상분포, 극한분포",
    "text": "Emprical 분포, 정상분포, 극한분포\n- Emprical distribution, 정상분포, 극한분포\n\n\\(\\bar{\\boldsymbol \\pi}^\\top\\): 하나의 \\(\\omega\\)에 대한 확률변수열 \\(\\{X_t\\}\\)의 emprical distribution (time average로 분포를 추정)\n\\({\\boldsymbol \\pi}^\\top\\): 모든 \\(\\omega\\)를 고려하였을 경우 확률변수열 \\(\\{X_t\\}\\)의 정상분포.\n\\({\\bf p}_{\\star}^\\top\\): \\(\\omega\\)와 무관하게 \\({\\bf P}\\)의 극한으로 얻어지는 \\(\\{X_t\\}\\)의 극한분포. // 마코프체인에 특화\n\n- 표로 정리하면 아래와 같다.\n\n\n\n\n\n\n\n\n\n\nEmpirical 분포\n정상분포\n극한분포\n\n\n\n\n용어가 통용되는 범위\n모든 확률과정\n모든 확률과정\n마코프체인\n\n\n기호\n\\(\\bar{\\boldsymbol \\pi} = \\frac{1}{T}\\begin{bmatrix}{\\tt sum}(X_t==0) \\\\ {\\tt sum}(X_t==1)\\end{bmatrix}\\)\n\\({\\boldsymbol \\pi}=\\begin{bmatrix}\\mathbb{E}(I(X=0))\\\\ \\mathbb{E}(I(X=1)) \\end{bmatrix}\\)\n\\({\\bf p}_{\\star}= \\lim_{t\\to \\infty}\\begin{bmatrix}p_{?0}^{(t)} \\\\ p_{?1}^{(t)} \\end{bmatrix}\\)\n\n\n\\(\\omega\\)의 고려\n하나의 \\(\\omega\\)만 고려해 계산\n모든 \\(\\omega\\) 고려해 계산\n\\(\\omega\\)를 고려하지 않고 계산\n\n\n통계느낌(분포느낌?)\nO\nO\nX\n\n\n이론적인값?\nX\nO\nO\n\n\n데이터와 관련\nO\nX\nX\n\n\n극한과 관련\nO\nX\nO\n\n\nLLN과 관련\nO\nO\nX\n\n\n느낌\n데이터로 계산한 평균값\n이론적인 기대값\n이론적인 수렴값"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#이론들의-의미를-다시-고찰",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#이론들의-의미를-다시-고찰",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "이론들의 의미를 다시 고찰",
    "text": "이론들의 의미를 다시 고찰\n- 에르고딕정리: 임피리컬분포와 정상분포에 관련한 정리 (LLN보다 더 약한 조건을 가짐)\n\n의미1: 에르고딕 정리를 만족하지 못하는 경우 하나의 확률변수열을 이용한 추론이 불가능\n\n\n## 예시\nP =np.array([[1,1,0,0],\n             [1,1,0,0],\n             [0,0,1,1],\n             [0,0,1,1]])/2\nP\n\narray([[0.5, 0.5, 0. , 0. ],\n       [0.5, 0.5, 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n\n의미2: 에르고딕 정리를 만족하는 경우 \\(X_1,X_2,\\dots\\)이 동일한 분포를 가지지 않지만 무시하고 \\(\\pi\\)를 추정할 수있다.\n\n- 극한분포와 관련된 이론: 극한분포의 느낌은 초기값에 대한 삭제임\n\n극한분포는 어차피 \\(\\omega\\)를 신경안씀\n\\({\\boldsymbol \\mu}_0\\)는 아무상관이 없음\n결국 극한분포가 존재한다면 시간의존성이 삭제된다는 의미임"
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-11-11wk-1.html#스토리정리",
    "href": "posts/2. 마코프체인/2023-05-11-11wk-1.html#스토리정리",
    "title": "11wk-1: 마코프체인 (8)",
    "section": "스토리정리",
    "text": "스토리정리\n- FINITE HMC는 일단 정상분포라는게 존재함. 그런데 유일하지 않을 수 있음.\n- FINITE HMC는 크게보면 IRR인 케이스와 IRR 아닌 케이스로 나누어짐\n\n그런데 IRR이 아닌 케이스는 IRR인 케이스들의 조합으로 나누어 생각할 수 있음\n그래서 어차피 신경쓸 필요 없음.\n따라서 모든 마코프체인은 IRR이라고 가정해버려도 무방\n\n- 만약에 HMC가 (1) FINITE (2) IRR 이면 유일한 정상분포가 존재.\n\n심지어 이 조건에서는 에르고딕정리를 이용해서 임피리컬 분포로 정상분포를 estimate 할 수 있음.\n\n- 그러면 HMC가 (1) FINITE (2) IRR 이면 다 끝?\n\n언뜻 생각하면 그런거 같음.\n그런데 에르고딕 정리를 만족한다고 해서 초기분포에 대한 기억이 사라지는건 아님\n몇 가지 응용예제에서는 초기분포에 대한 의존성을 삭제시키는 것이 매우 중요함.\n이걸 위해서는 AP조건이 추가되어야 함.\n\n- HMC가 (1) FINITE (2) IRR (3) AP 라면 아주 좋음."
  },
  {
    "objectID": "posts/2. 마코프체인/2023-05-23-12wk-2.html",
    "href": "posts/2. 마코프체인/2023-05-23-12wk-2.html",
    "title": "12wk-2: 마코프체인 (11)",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yZaqMvt2jojFKOeDaomqzi\n\n\n\n예비학습\n- 약어: \\(X\\)가 \\(\\mathbb{N}_0\\)에서 값을 가지는 이산형확률변수이고 \\({\\boldsymbol \\mu}^\\top\\) 가 \\(X\\)의 분포라고 하자. 이해를 위해서 아래와 같은 확률분포표를 가정한다면\n\n\n\n\\(X\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(0.1\\)\n\\(0.2\\)\n\\(0.7\\)\n\n\n\n\\({\\boldsymbol \\mu}^\\top = [0.1,0.2,0.7]\\) 이다. 이럴 경우 평균은\n\n\\(\\mathbb{E}(X)= 0\\times 0.1 + 1\\times 0.2 + 2 \\times 0.7\\)\n\n와 같이 표현가능한데, 이를 좀 더 명확하게 하기 위하여\n\n\\(\\mathbb{E}_{\\boldsymbol \\mu}(X)=0\\times 0.1 + 1\\times 0.2 + 2 \\times 0.7\\)\n\n라고 표현하기도 한다. 마찬가지로\n\n\\(\\mathbb{P}(X=0)=0.1\\)\n\\(\\mathbb{P}(X=1)=0.2\\)\n\\(\\mathbb{P}(X=2)=0.7\\)\n\n를 좀 더 명확하게 하기 위해서\n\n\\(\\mathbb{P}_{\\boldsymbol \\mu}(X=0)=0.1\\)\n\\(\\mathbb{P}_{\\boldsymbol \\mu}(X=1)=0.2\\)\n\\(\\mathbb{P}_{\\boldsymbol \\mu}(X=2)=0.7\\)\n\n와 같이 표현하기도 한다.\n- 예시: \\(X\\)의 분포 \\({\\boldsymbol \\mu}\\)와 \\({\\boldsymbol \\nu}\\)가 각각 아래와 같다고 하자.\n\\({\\boldsymbol \\mu}\\)의 정의\n\n\n\n\\(X\\)\n\\(1\\)\n\\(6\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(0\\)\n\\(1\\)\n\n\n\n\\({\\boldsymbol \\nu}\\)의 정의\n\n\n\n\\(X\\)\n\\(1\\)\n\\(6\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(1\\)\n\\(0\\)\n\n\n\n이 경우 아래의 표현들이 가능하다.\n\n\\(\\mathbb{E}_{\\boldsymbol \\mu}(X)=6\\), \\(\\mathbb{E}_{\\boldsymbol \\nu}(X)=1\\)\n\\(\\mathbb{P}_{\\boldsymbol \\mu}(X=1)=0\\), \\(\\mathbb{P}_{\\boldsymbol \\mu}(X=6)=1\\), \\(\\mathbb{P}_{\\boldsymbol \\nu}(X=1)=1\\), \\(\\mathbb{P}_{\\boldsymbol \\nu}(X=6)=0\\)\n\n- 약어: 확률변수 \\(X\\)가 \\(X=x\\)에서만 확률을 가지고 그 외에는 1이라고 할 경우 분포 \\({\\boldsymbol \\mu}\\)를 \\({\\boldsymbol \\delta}_x\\)라고 표현하기도 한다. 따라서 이 경우\n\n\\(\\mathbb{P}_{{\\boldsymbol \\delta}_x}(X=x)=1\\), \\(\\mathbb{P}_{{\\boldsymbol \\delta}_x}(X\\neq x)=0\\)\n\\(\\mathbb{E}_{{\\boldsymbol \\delta}_x}(X)=x\\)\n\n와 같은 표현들이 가능하다.\n- 예시: \\(X\\)의 분포 \\({\\boldsymbol \\mu}\\)와 \\({\\boldsymbol \\nu}\\)가 각각 아래와 같다고 하자.\n\\({\\boldsymbol \\mu}\\)의 정의\n\n\n\n\\(X\\)\n\\(1\\)\n\\(6\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(0\\)\n\\(1\\)\n\n\n\n\\({\\boldsymbol \\nu}\\)의 정의\n\n\n\n\\(X\\)\n\\(1\\)\n\\(6\\)\n\n\n\n\n\\(\\mathbb{P}(X=k)\\)\n\\(1\\)\n\\(0\\)\n\n\n\n이 경우 아래의 표현들이 가능하다. (??)1\n1 사실 이렇게 쓰는걸 본적은 없음\n\\(\\mathbb{E}_{{\\boldsymbol \\delta}_6}(X)=6\\), \\(\\mathbb{E}_{{\\boldsymbol \\delta}_1}(X)=1\\)\n\\(\\mathbb{P}_{{\\boldsymbol \\delta}_6}(X=1)=0\\), \\(\\mathbb{P}_{{\\boldsymbol \\delta}_6}(X=6)=1\\), \\(\\mathbb{P}_{{\\boldsymbol \\delta}_1}(X=1)=1\\), \\(\\mathbb{P}_{{\\boldsymbol \\delta}_1}(X=6)=0\\)\n\n- 약어: \\(\\{X_t\\}\\)가 상태공간 \\(E\\)에서 정의된 HMC 라고 하자.\n\n\\(\\mathbb{P}_{i}(X_t=k):=\\mathbb{P}_{\\boldsymbol {\\boldsymbol \\delta}_i}(X_t=k)=\\mathbb{P}(X_t=k| X_0=i)\\)2\n1의 표현에서 \\({\\boldsymbol \\delta}_x\\) 대신에 일반적인 \\({\\boldsymbol \\mu}\\)를 쓰기도 함.\n\\(\\mathbb{P}_{\\boldsymbol \\pi}(X_t=k)=\\pi_k\\)3\n\n2 \\(X_0 \\sim {\\boldsymbol \\delta}_i^\\top\\) 를 가정하고 구한 확률3 \\(X_t \\sim {\\boldsymbol \\pi}^\\top\\) 를 가정하고 구한 확률인것 처럼 보임, 하지만 사실 \\(X_0 \\sim {\\boldsymbol \\pi}^\\top\\)를 가정한 것\n\\(\\mathbb{P}_{i}\\), \\(\\mathbb{P}_{\\boldsymbol \\pi}\\) 등이 자명한 기호는 아니므로 교재마다 초반부에 정의하고 들어감헷갈리는 편임. 일반적인 기호와 충돌이 오지만 정상분포일 경우 그 의미가 같음.\n\n\n\nnature (cont)\n- 정의: \\(\\{X_t\\}\\)가 상태공간 \\(E\\)에서 정의된 HMC 라고 하자. \\(y \\in E\\), \\(t \\in \\mathbb{N}_0\\)에 대하여 아래와 같은 기호를 정의하자.\n\n\\(T_y = \\min\\{t: X_t=y, t\\geq 1\\}=\\min\\{t\\geq 1: X_t=y\\}\\)\n\\(P_y(T_y&lt;\\infty)\\)\n\n- 의미:\n\n나그네가 마을 \\(y\\) 에 \\(t=0\\), \\(t=2\\), \\(t=5\\), \\(t=88\\) 에 방문하였다고 하자.\n\\(\\{t: X_t=y, t\\geq 1\\}=\\{2,5,88\\}\\)\n\\(\\min\\{t: X_t=y, t\\geq 1\\}=2\\)\n\\(T_y=\\) 나그네가 마을 \\(y\\)에 처음으로 방문한 시점, 단 \\(t=0\\)인 경우는 제외함.\n\\(T_y=\\infty\\) \\(\\Leftrightarrow\\) 나그네가 마을 \\(y\\)에 갈 일이 없음\n\\(T_y&lt;\\infty\\) \\(\\Leftrightarrow\\) 나그네가 마을 \\(y\\)에 언제가는 돌아옴\n\\(\\mathbb{P}_y(T_y&lt;\\infty)=\\) 초기상태를 마을 \\(y\\)에 출발한 나그네4가 언젠가 다시 \\(y\\)로 돌아올 확률\n7의 의미는 “마을 \\(y\\)에 존재하던 나그네가 언젠가 다시 마을 \\(y\\)로 돌아올 확률”이라 해석해도 무방하다.\n\n4 \\(t=0\\) 시점에 마을 \\(y\\)에 존재하던 나그네- 정의: \\(\\{X_t\\}\\)가 상태공간 \\(E\\)에서 정의된 HMC 라고 하자. \\(T_i\\)를 상태 \\(i \\in E\\)에 대한 return time 이라고 하자. 만약에 상태 \\(i \\in E\\) 가 아래의 식을 만족한다면\n\\[\\mathbb{P}_i(T_i&lt;\\infty)=1\\]\n\\(i\\)는 recurrent 하다고 표현하고, 그렇지 않으면 \\(i\\)는 transient 하다고 표현한다. 만약에 recurrent state \\(i\\)가 아래식을 만족한다면\n\\[\\mathbb{E}_i[T_i] &lt; \\infty\\]\nstate \\(i\\)를 positive recurrent 라고 하고 그렇지 않으면 null recurrent 라고 한다.\n- 이론: HMC \\(\\{X_t\\}\\)가 IRR 이라면 모든 \\(i\\)는 같은 nature 를 가진다. 즉 \\(\\{X_t\\}\\)가 IRR 이면 아래중의 하나이다.\n\n모든 상태가 transient 하다.\n모든 상태가 null recurrent 하다.\n모든 상태가 positive recurrent 하다.\n\n\nHMC \\(\\{X_t\\}\\)의 모든상태가 positive recurrent 이면, positive recurrent markov chain 이라고 간단히 부른다. 나머지 역시 마찬가지\n\n- Thm(상태의분해1): \\(\\{X_t\\}\\)가 HMC라고 하고, \\(E\\)를 \\(\\{X_t\\}\\)가 정의되는 상태공간이라고 하자. 기호 \\(\\leftrightarrow\\)는 \\(E\\)에서 정의된 euivalence relation이 된다. 따라서 집합 \\(E\\)의 원소는 \\(\\leftrightarrow\\)를 기준으로 아래와 같이 나눌 수 있다.\n\\[E = \\uplus_{k=1}^{\\infty} E_k\\]\n이때\n\n\\(E_1,E_2,E_3,\\dots\\) 는 서로소\n\\(\\forall k:\\) \\(E_k\\) 는 IRR\n\n이다.\n- Thm(상태의분해2)(Durrett 2019, Thm 5.3.5): \\(\\{X_t\\}\\)가 HMC라고 하고, \\(E\\)를 \\(\\{X_t\\}\\)가 정의되는 상태공간이라고 하자. 집합 \\(E\\)는 아래와 같이 분해할 수 있다.\n\nDurrett, Rick. 2019. Probability: Theory and Examples. Vol. 49. Cambridge university press.\n\\[E = \\uplus_{k=1}^{\\infty} E_k\\]\n이때\n\n\\(E_1,E_2,E_3,\\dots\\) 는 서로소\n\\(\\forall k:\\) \\(E_k\\) 는 IRR\n\\(\\forall k:\\) \\(E_k\\) 의 모든 원소는 PR 이거나 NR 이거나 TR\n\n이다.\n- 따라서 transition matrix는 일반적으로 아래와 같이 분해하여 생각할 수 있다.\n\n\n\n그림1: 전이행렬의 분해 (Brémaud 2020, p 117)\nBrémaud, Pierre. 2020. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues. Springer Cham.\n\n\n\n\n\nnature와 정상분포\n- 정의 \\(\\{X_t\\}\\)가 HMC라고 하자. 아래의 식을 만족하는\n\\[\\tilde{\\boldsymbol \\pi}^\\top {\\bf P} = \\tilde{\\boldsymbol \\pi}^\\top\\]\n\\({\\bf 0}\\)이 아닌 \\(\\tilde{\\boldsymbol \\pi}^\\top\\) 를 invariant measure 라고 한다. 만약에 \\(\\tilde{\\boldsymbol \\pi}^\\top\\) 이 분포의 정의를 만족하면 stationary measure 혹은 stationary distribution 이라고 부른다.\n- 예시: “오른쪽으로만 갈래” 예제에서는\n\\[\\tilde{\\boldsymbol \\pi}^\\top = [1,1,1,\\dots]\\]\n이 수식\n\\[\\tilde{\\boldsymbol \\pi}^\\top {\\bf P} = \\tilde{\\boldsymbol \\pi}^\\top\\]\n을 만족한다. 따라서 이 예제에서 \\(\\tilde{\\boldsymbol \\pi}^\\top = [1,1,1,\\dots]\\) 은 invariant measure 이다.\n- \\(\\{X_t\\}\\)가 HMC라고 하자. 각각에 대하여 아래가 성립한다.\n\n\n\n\n\n\n\n\n\n\nIRR\nnature\n\\(\\exists! \\tilde{\\boldsymbol \\pi}\\) up to multiplier\n\\(\\exists! {\\boldsymbol \\pi}\\)\n에르고딕정리(\\(\\approx\\)LLN)\n\n\n\n\n\\(O\\)\nPR\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\\(O\\)\nNR\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(O\\)\nTR\n\\(\\Delta\\)\n\\(X\\)\n\\(X\\)\n\n\n\n- 이론: \\(\\{X_t\\}\\)가 IRR-HMC5 라고 하자. \\(\\{X_t\\}\\)가 정상분포를 가진다는 조건과 유일한 정상분포를 가질 조건은 동치이다.\n5 irreducible 한 homogeneous markov chain\n즉 \\(\\{X_t\\}\\)가 IRR-HMC 일때, 정상분포가 존재한다는 사실만 보이면 자동으로 유일성이 보장된다.\n\n- Thm: \\(\\{X_t\\}\\)가 IRR-HMC 라고 하자. 그러면 positvite recurrent 와 \\(\\exists! {\\boldsymbol \\pi}\\) 은 동치조건이다. 즉\n\nIRR-HMC \\(\\{X_t\\}\\) 가 positive recurrent 하다면 항상 \\(\\{X_t\\}\\) 는 유일한 정상분포를 가진다.\nIRR-HMC \\(\\{X_t\\}\\) 가 정상분포를 가지면 (그 분포는 유일해지고) \\(\\{X_t\\}\\)는 항상 positive recurrent 하다."
  },
  {
    "objectID": "posts/1. 측도론/2023-03-23-4wk-1.html",
    "href": "posts/1. 측도론/2023-03-23-4wk-1.html",
    "title": "04wk-1: 측도론 intro (5)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-yTpksFFUby_Twan5kFTFdm"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-23-4wk-1.html#불완전한-정의",
    "href": "posts/1. 측도론/2023-03-23-4wk-1.html#불완전한-정의",
    "title": "04wk-1: 측도론 intro (5)",
    "section": "불완전한 정의",
    "text": "불완전한 정의\n- 확률변수: \\(X:\\Omega \\to \\mathbb{R}\\)인 조금 특별한 성질을 가진 함수\n\n정의역: \\(\\Omega\\)\n치역: \\(\\mathbb{R}\\)\n\n(예제1) 동전예제\n1. outcomes31: \\(H\\),\\(T\\).\n31 outcome 자체는 집합을 의미하는게 아님2. sample space: \\(\\Omega = \\{H,T\\}\\)\n3. event32: \\(\\emptyset\\), \\(\\{H\\}\\), \\(\\{T\\}\\), \\(\\{H,T\\}\\).\n32 event는 집합을 의미4. \\(\\sigma\\)-field: \\({\\cal F}=\\) \\(\\Omega\\)의 모든 부분집합의 모임\n5. probability measure function: \\(P: {\\cal F} \\to [0,1]\\) such that\n\n\\(P(\\emptyset) = 0\\)\n\\(P(\\{H\\}) = \\frac{1}{2}\\)\n\\(P(\\{T\\}) = \\frac{1}{2}\\)\n\\(P(\\Omega) = 1\\)\n\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that\n\n\\(X(H)=1\\)\n\\(X(T)=0\\)\n\n만약에 편의상 \\(\\Omega=\\{H,T\\}=\\{\\omega_1,\\omega_2\\}\\)와 같이 사용한다면\n\n\\(X(\\omega_1)=1\\)\n\\(X(\\omega_2)=0\\)"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-23-4wk-1.html#헷갈려-1-starstarstar",
    "href": "posts/1. 측도론/2023-03-23-4wk-1.html#헷갈려-1-starstarstar",
    "title": "04wk-1: 측도론 intro (5)",
    "section": "헷갈려 (1) (\\(\\star\\star\\star\\))",
    "text": "헷갈려 (1) (\\(\\star\\star\\star\\))\n- 질문1: 아래의 표현 중 옳은 것은?\n\n\\(X(H)=0\\)33\n\\(P(\\{H\\})=\\frac{1}{2}\\)34\n\\(P(\\{\\omega_1\\})=\\frac{1}{2}\\)35\n\\(P(H)=\\frac{1}{2}\\)36\n\\(P(\\{H,T\\})=1\\)37\n\\(P(\\omega_1)=\\frac{1}{2}\\)38\n\n33 O34 O35 O36 X37 O38 X- 질문2: 질문1의 4번의 표현을 많이 본적 있다. 예를들어서 고등학교에서 두 사건의 독립에 대해 배울때 아래와 같은 방식으로 표현했었다. // 출처: 네이버 블로그\n\n두 사건 \\(A\\), \\(B\\)에 대하여 \\(P(B|A) =P(B|A^c) =P(B)\\) 이면 두 사건이 독립이라고 한다~~\n\n그렇다면 이 표현은 틀린걸까?\n(해설)\n여기에서 사건 \\(A\\), \\(B\\)는 event을 의미하며 outcome을 의미하는게 아님. 즉 \\(A\\), \\(B\\)는 집합임.\n암기: 확률은 항상 집합을 입력으로 받아야 함!!\n- 질문3(\\(\\star\\star\\star\\)): 수리통계 시간에서 아래와 같은 표현 본 적 있다.\n\\[P(X=1)=\\frac{1}{2}\\]\n그런데 \\(P\\)의 입력으로는 집합이 들어가야하는데, \\(X=1\\)은 그냥 수식임. 그렇다면 이 표현은 틀린 표현일까??\n(해설)\n사실 \\(P(X=1)\\)의 의미는 아래와 같은 표현의 축약형이다.\n\\[P\\big(\\{\\omega: X(\\omega)=1 \\} \\big)\\]\n\\(\\{\\omega: X(\\omega)=1\\} = \\{\\omega_1\\} = \\{H\\}\\) 를 의미하므로 결국\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n이 된다. 따라서 옳은 표현이다."
  },
  {
    "objectID": "posts/1. 측도론/2023-03-23-4wk-1.html#확률변수에-대한-통찰-1",
    "href": "posts/1. 측도론/2023-03-23-4wk-1.html#확률변수에-대한-통찰-1",
    "title": "04wk-1: 측도론 intro (5)",
    "section": "확률변수에 대한 통찰 (1)",
    "text": "확률변수에 대한 통찰 (1)\n- 아래와 같은 표현을 다시 관찰하자.\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n통찰1. 확률변수가 “함수”라는 사실을 떠올리고 \\(1\\)이라는 값이 확률변수의 “상(image)” 라는 사실을 떠올리면, \\(\\{\\omega: X(\\omega)=1\\}\\)은 1에 대한 “역상(inverse image)”이라고 해석할 수 있다.39\n39 참고로 image는 수학책에서 3가지 뜻으로 혼용해서 쓰이는데, 이 문맥에서는 “Image of an element”를 의미함. ref통찰2. 확률변수의 상은 \\(\\mathbb{R}\\)에 맺히게 되고, 확률변수의 역상은 \\(\\Omega\\)의 부분집합 중 하나에 맺히게 된다.\n통찰3. 문제는 확률변수의 역상이 항상 잴 수 있는 집합에 맺힌다는 보장이 있냐라는 것이다… 즉 이 예제로 한정하면\n\\[\\{\\omega: X(\\omega)=1\\} \\in {\\cal F}\\]\n임을 보장해야 한다는 것이다.\n통찰4. 당연히 이러한 보장을 할 수는 없어보인다. 따라서 \\(X\\)를 단지 그냥\n\n\\(X: \\mathbb{\\Omega} \\to \\mathbb{R}\\)로 가는 함수\n\n가 아니라\n\n\\(X: \\mathbb{\\Omega} \\to \\mathbb{R}\\)로 가는 함수 & 역상이 항상 잴 수 있는 집합이어야 함.\n\n이라는 조건이 필요하다.\n- 역상이 잴 수 있는 집합인 함수를 간단히 잴 수 있는 함수 (measurable function) 라고 한다."
  },
  {
    "objectID": "posts/1. 측도론/2023-03-29-5wk-2-hw1.html",
    "href": "posts/1. 측도론/2023-03-29-5wk-2-hw1.html",
    "title": "05wk-2: HW1",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-xsTHLqiyPTgay2jwnssbnC\n\n\n\n배점\n\n\n\n문항\n점수\n\n\n\n\n1-(1)\n15\n\n\n1-(2)\n15\n\n\n2-(1)\n5\n\n\n2-(2)\n5\n\n\n2-(3)\n10\n\n\n2-(4)\n10\n\n\n3-(1)\n20\n\n\n3-(2)\n20\n\n\n\n\n\n1. Cardinality\n(1) \\(\\mathbb{Q}\\)의 cardinality가 \\(\\aleph_0\\)임을 증명하라.\n(풀이) 생략\n(2) \\(\\mathbb{R}\\)의 cardinality가 \\(\\aleph_0\\)이 아님을 보여라.\n(풀이) 생략\n\n\n2. \\(\\sigma\\)-field\n(1) \\(\\Omega=\\{H,T\\}\\)일 때, 다음 중 시그마필드의 정의를 만족하는 집합을 모두 골라라.\n\n\\({\\cal F}=\\{\\emptyset\\}\\)\n\\({\\cal F}=\\{\\Omega\\}\\)\n\\({\\cal F}=\\{\\emptyset, \\Omega\\}\\)\n\\({\\cal F}=\\{\\{H\\}\\}\\)\n\\({\\cal F}=\\{\\{H\\}, \\{T\\}\\}\\)\n\\({\\cal F}=\\{\\emptyset, \\{H\\}, \\Omega\\}\\)\n\\({\\cal F}=\\{\\emptyset, \\{T\\}, \\Omega\\}\\)\n\\({\\cal F}=\\{\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\}\\)\n\n(풀이) 3,8 이 시그마필드이다. // 시그마필드가 아닌이유를 서술할 필요없음. 답만 쓰면 인정함.\n(2) \\(\\Omega=\\{1,2,3,4\\}\\) 일 때,\n\\[{\\cal A}=\\{ \\{1\\}, \\{1,2\\}\\}\\]\n이라고 하자. \\(\\sigma({\\cal A})\\)를 구하여라.\n(풀이) // 이 문제역시 답만 써도 인정\n\\(\\sigma({\\cal A}) = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\}, \\{1,2\\},\\{1,3,4\\}, \\{2,3,4\\}, \\Omega\\}\\)\n\n요령: \\(\\{3,4\\}\\)를 한세트로 보면 편리하다. 즉 전체사건을 \\(\\{1\\},\\{2\\},\\{3,4\\}\\)로 쪼갠뒤에 3개의 원소만 있다고 생각하고 모든 부분집합을 쓰면 된다.\n\n(3) \\(\\Omega=\\{1,2,3,4,\\dots,100\\}\\) 일 때,\n\\[{\\cal A}=\\{ \\{1\\}, \\{1,2\\}, \\{1,2,3\\},\\{1,2,3,4\\}\\}\\]\n이라고 하자. 아래의 물음에 답하여라.\n\n\\(\\{2\\} \\in \\sigma({\\cal A})\\) 인가?\n\\(\\{2,3,4\\} \\in \\sigma({\\cal A})\\) 인가?\n\\(\\{3,4,5\\} \\in \\sigma({\\cal A})\\) 인가?\n\\(\\Omega - \\{1,2,3\\} \\in \\sigma({\\cal A})\\) 인가?\n\n(풀이) // 답만쓰면 인정하지 않음.\n시그마필드의 정의는 아래와 같다.\n\n시그마필드 \\({\\cal F} \\subset 2^\\Omega\\) 는 1. 전체집합을 포함하고, 2. 여집합에 닫혀있고 3. 가산합집합에 닫혀있는 collection 이다.\n\n먼저 강의노트를 참고하여 아래의 사실을 보이자.\n\n\\(A,B \\in {\\cal F} \\Rightarrow A \\cap B \\in {\\cal F}\\)\n\\(A,B \\in {\\cal F} \\Rightarrow A - B \\in {\\cal F}\\)\n\n이제 풀어보자.\n1번\n\n\\(\\{1,2\\} \\in \\sigma({\\cal A})\\), \\(\\{1\\} \\in \\sigma({\\cal A})\\)\n\\(\\Rightarrow\\) \\(\\{1,2\\} - \\{1\\} = \\{2\\} \\in \\sigma({\\cal A})\\) (\\(\\because\\) (b))\n\n2번\n\n\\(\\{1,2\\}-\\{1\\} = \\{2\\} \\in \\sigma({\\cal A})\\), \\(\\{1,2,3\\} - \\{1,2\\} = \\{3\\} \\in \\sigma({\\cal A})\\), \\(\\{1,2,3,4\\} - \\{1,2,3\\} = \\{4\\} \\in \\sigma({\\cal A})\\).\n\\(\\Rightarrow\\) \\(\\{2\\}\\cup \\{3\\} \\cup \\{4\\} = \\{2,3,4\\} \\in \\sigma({\\cal A})\\)\n\n3번\n\n\\(\\{3,4,5\\} \\not \\in \\sigma({\\cal A})\\)\n\n4번\n\n\\(\\{1,2,3\\}\\in \\sigma({\\cal A})\\)\n\\(\\Rightarrow \\{1,2,3\\}^c \\in \\sigma({\\cal A})\\)\n\n(4) \\(\\Omega=[0,2\\pi)\\) 일 때,\n\\[{\\cal A}=\\{ [a,b): 0\\leq a&lt; b\\leq 2\\pi\\}\\]\n이라고 하자. 아래의 물음에 답하여라.\n\n\\([\\frac{\\pi}{2},\\pi) \\in \\sigma({\\cal A})\\) 인가?\n\\(\\{\\pi\\} \\in \\sigma({\\cal A})\\) 인가?\n\\(\\{0,\\frac{\\pi}{2},\\pi,\\frac{3\\pi}{2}\\} \\in \\sigma({\\cal A})\\) 인가?\n\\((\\frac{\\pi}{2},\\pi) \\in \\sigma({\\cal A})\\) 인가?\n\n\n수업시간에 2번문제 잘못해설했어요. (문제도 잘못냈어요, 너무 어렵게 냈어요. )\n\n(풀이)\n\n\\(a=\\frac{\\pi}{2}\\), \\(b=\\pi\\)\n\\([0,\\pi)~ \\bigcup ~\\cup_{i=1}^{\\infty}[\\pi+\\frac{1}{n}, 2\\pi) = [0,\\pi) \\cup (\\pi,2\\pi)\\) \\(\\Rightarrow\\) \\(\\Omega - \\big([0,\\pi) \\cup (\\pi,2\\pi) \\big) = \\{\\pi\\} \\in \\sigma({\\cal A})\\)\n\n\n\\([\\pi+\\frac{1}{n}, 2\\pi)\\)는 각각 잴 수 있는 집합이므로 \\(\\cup_{i=1}^{\\infty}[\\pi+\\frac{1}{n}, 2\\pi)\\) 역시 잴 수 있는 집합이다.\n여기에서 \\(\\cup_{i=1}^{\\infty}[\\pi+\\frac{1}{n}, 2\\pi)=(\\pi,2\\pi)\\) 로 볼 수 있는데, 그 이유는 \\(\\cup_{i=1}^{\\infty}[\\pi+\\frac{1}{n}, 2\\pi)\\)는 \\(\\pi\\)보다 큰 모든수를 포함하지만 \\(\\pi\\)는 포함할 수 없기 때문이다.\n\\([0, \\pi)\\)도 당연히 잴 수 있는 집합이다.\n잴 수 있는 집합의 교집합은 잴 수 있으므로 \\([0,\\pi)~ \\cup ~(\\pi,2\\pi)\\) 역시 잴 수 있는 집합이다.\n따라서 \\([0,2\\pi) - \\big([0,\\pi) \\cup (\\pi,2\\pi) \\big)\\) 역시 잴 수 있다.\n\n\n\\(\\{\\pi\\}\\)를 잴수 있다는 것과 동일한 논리전개로 \\(\\{0\\},\\{\\frac{\\pi}{2}\\}, \\{\\frac{3\\pi}{2}\\}\\) 모두 잴 수 있는 집합이고, 따라서 이들의 합집합도 잴 수 있다.\n\\([\\frac{\\pi}{2}, \\pi)\\)를 잴 수 있고 \\(\\{\\frac{\\pi}{2}\\}\\)를 잴 수 있으므로 \\([\\frac{\\pi}{2}, \\pi) - \\{\\frac{\\pi}{2}\\}\\) 역시 잴 수 있다.\n\n\n\n3. 확률과 확률변수\n(1) 아래와 같은 measurable space \\((\\Omega, {\\cal F})\\)를 고려하자.\n\n\\(\\Omega=\\{a,b,c,d\\}\\)\n\\({\\cal F}=2^\\Omega\\)\n\n아래와 같은 확률변수 \\(X: \\Omega \\to \\{1,2,3,4\\}\\) 를 고려하자. 다음중 올바른 표현은?\n\n\\(X(a)\\)\n\\(X(\\{a\\})\\)\n\\(P(a)\\)\n\\(P(\\{a\\})\\)\n\\(P(X=1)\\)\n\\(X = \\begin{cases} 1 & w.p.~\\frac{1}{2} \\\\ 2 & w.p. ~\\frac{1}{6} \\\\ 3 & w.p. ~\\frac{1}{6} \\\\ 4 & w.p. ~\\frac{1}{6} \\end{cases}\\)\n\n(풀이) 생략 (이 문제는 그대로 낼거라서요, 풀이 생략합니다. 스스로 해보세요. 시험에서는 답만쓰면 정답으로 인정합니다)\n(2) 아래와 같은 measurable space를 고려하자.\n\n\\(\\Omega=\\{a,b,c,d\\}\\)\n\\({\\cal F} =\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\{a\\}\\}\\).\n\n아래와 같은 function \\(X:\\Omega \\to A:=\\{1,2,3,4\\}\\), \\(Y:\\Omega \\to B:=\\{1,2\\}\\)을 고려하자.\n\n\\(X(a)=1, X(b)=2, X(c)=3, X(d)=4\\)\n\\(Y(a)=1, Y(b)=2, Y(c)=2, Y(c)=2\\)\n\n아래의 물음에 답하라.\n\n\\(X\\)는 \\((\\Omega,{\\cal F}) \\to (A,2^{A})\\) 인가? 즉 \\(X\\)는 \\((\\Omega,{\\cal F})\\) 에서의 확률변수인가?\n\\(Y\\)는 \\((\\Omega,{\\cal F}) \\to (B,2^{B})\\) 인가? 즉 \\(Y\\)는 \\((\\Omega,{\\cal F})\\) 에서의 확률변수인가?\n\n(풀이)\n\\(X\\)는 확률변수가 아님\n집합 \\(\\{2\\} \\subset 2^A\\)에 대하여 \\(\\{\\omega: X(\\omega) \\in \\{2\\}\\}=\\{b\\} \\not \\in \\sigma({\\cal A})\\) 이므로 \\(X\\)는 확률변수가 아님\n\\(Y\\)는 확률변수임\n\\(2^B = \\{\\emptyset,\\{1\\},\\{2\\},B\\}\\) 의 모든 부분집합 \\(B^\\ast\\)에 대하여 \\(\\{\\omega: X(\\omega) \\in B^\\ast\\} \\in \\sigma({\\cal A})\\) 이 성립함.\n\n\\(\\{\\omega: X(\\omega) \\in \\emptyset\\} = \\emptyset \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: X(\\omega) \\in \\{1\\}\\} = \\{a\\} \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: X(\\omega) \\in \\{2\\}\\} = \\{b,c,d\\} \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: X(\\omega) \\in B\\} = \\{a,b,c,d\\} \\in \\sigma({\\cal A})\\)"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-28-4wk-2.html",
    "href": "posts/1. 측도론/2023-03-28-4wk-2.html",
    "title": "04wk-2: 측도론 intro (6)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-x7Z5LJZOG4At6NWHs757XG"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-28-4wk-2.html#셀-수-있는",
    "href": "posts/1. 측도론/2023-03-28-4wk-2.html#셀-수-있는",
    "title": "04wk-2: 측도론 intro (6)",
    "section": "셀 수 있는",
    "text": "셀 수 있는\n- 셀 수 있는 집합과 셀 수 없는 집합.\n\ncountable: finite, countable many\nuncountable: uncountable many\n\n- 예시1: countable set, uncountable set\n\n\\(\\{1,2,3,4,5\\}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{N}\\)은 셀 수 있는 집합이다.\n\\(\\mathbb{Z}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{Q}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{R}\\)은 셀 수 없는 집합이다.\n\n- 예시2: countable sum: 아래는 모두 countable sum을 의미한다.\n\n\\(\\sum_{i=1}^{n}a_i\\).\n\\(\\sum_{i \\in I} a_i\\), where \\(I=\\{1,2,3,\\dots,10\\}\\).\n\\(\\sum_{i=1}^{\\infty} a_i\\), \\(\\sum_{i=0}^{\\infty} a_i\\).\n\\(\\sum_{i \\in \\mathbb{N}}a_i\\).\n\\(\\sum_{x \\in \\mathbb{Q}}m(\\{x\\})\\), where \\(m\\) is Lebesgue measure\n\n- 예시3: countable union: 아래는 countalbe union을 의미한다.\n\n\\(\\cup_{i=1}^n A_i\\)\n\\(\\cup_{i=1}^{\\infty} A_i\\)\n\\(\\cup_{x \\in \\mathbb{Q}} \\{x\\}\\)\n\n- 예시4: 아래는 uncountable sum을 의미한다.\n\n\\(\\sum_{x \\in [0,1]}m(\\{x\\})\\), where \\(m\\) is Lebesgue measure\n\n- 예시5: 아래는 uncountable union을 의미한다.\n\n\\(\\cup_{x \\in [0,1]} \\{x\\}\\)"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-28-4wk-2.html#여러가지-집합",
    "href": "posts/1. 측도론/2023-03-28-4wk-2.html#여러가지-집합",
    "title": "04wk-2: 측도론 intro (6)",
    "section": "여러가지 집합",
    "text": "여러가지 집합\n\n\n\n집합 (\\(\\mathbb{R}\\)의 부분집합)\n카디널리티\n분류\n르벡메져\n\n\n\n\n\\(\\{1,2,3\\}\\)\n3\n가산집합\n0\n\n\n\\(\\mathbb{N}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\(\\mathbb{Z}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\(\\mathbb{Q}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\([0,1]\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,1]\\cap \\mathbb{Q}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\([0,1]\\cup \\mathbb{Q}\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,1]\\cap \\mathbb{Q}^c\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,\\infty)\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n\\(\\infty\\)\n\n\n비탈리집합\n\\(2^{\\aleph_0}\\)\n비가산집합\nNA\n\n\n칸토어집합\n\\(2^{\\aleph_0}\\)\n비가산집합\n0"
  },
  {
    "objectID": "posts/1. 측도론/2023-03-28-4wk-2.html#확률변수의-엄밀한-정의",
    "href": "posts/1. 측도론/2023-03-28-4wk-2.html#확률변수의-엄밀한-정의",
    "title": "04wk-2: 측도론 intro (6)",
    "section": "확률변수의 엄밀한 정의",
    "text": "확률변수의 엄밀한 정의\n- 확률변수 (머리속): \\(X:\\Omega \\to \\mathbb{R}\\) 인 잴 수 있는 함수.\n- 확률변수 (엄밀하게): 두 개의 잴 수 있는 공간 \\((\\Omega,{\\cal F})\\)와 \\((\\mathbb{R}, {\\cal R})\\)이 있다고 하자. 확률변수 \\(X\\)는 아래를 만족하는 함수 \\(X:\\Omega \\to \\mathbb{R}\\) 이다.\n\\[\\forall B \\in {\\cal R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n\nNote1: \\(\\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\) for all \\(B \\in {\\cal R}\\) 이라 쓰기도 함. 쓰는사람 마음~\n\n\nNote2: \\({\\cal R}\\)은 Borel sets라고 부른다. 의미는 \\(\\mathbb{R}\\)의 부분집합중 잴 수 있는 부분집합의 모임이라는 뜻이다. (즉 \\({\\cal F}\\)의 의미와 같다) \\({\\cal B}\\)의 원소는 Borel set이라고 부른다.\n\n- 왜 정의가 아래와 같지 않을까?\n\\[\\forall B \\subset \\mathbb{R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n\n위의 질문을 위한 보충학습\n(예제) 바늘이 하나 있는 시계\n1. outcomes: \\(0,1,\\frac{\\pi}{3},\\frac{2\\pi}{5},\\pi\\dots\\)\n2. sample space: \\(\\Omega = (0,2\\pi]\\)\n3. event: \\(\\emptyset\\), \\([0,\\frac{2}{\\pi})\\), \\(\\{2\\pi\\}\\), \\(\\dots\\)\n4. \\(\\sigma\\)-field: \\({\\cal F}\\). \\(\\Omega\\)의 부분집합 중 잴 수 있는 집합의 모임.\n5. probability measure function: \\(P: \\Omega \\to [0,1]\\) such that\n\n\\(P(\\emptyset) = 0\\)\n\\(P([0,\\frac{2}{\\pi}) = \\frac{1}{4}\\)\n\\(P(\\{2\\pi\\}) = 0\\)\n\\(P(\\Omega) = 1\\)\n\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that \\(X(\\omega)=\\omega\\). // 사실 \\(X: (0,2\\pi] \\to (0,2\\pi]\\)\n6을 주목하자. 만약에 비탈리집합 \\(V \\subset \\mathbb{R}\\)에 대한 inverse image는 비탈리집합 그 자체가 된다. 따라서 아래와 같이 된다.\n\\[P(X \\in V)=P\\big(\\{\\omega: X(\\omega) \\in V\\}\\big)=P(V)\\]\n\\(V\\)는 잴 수 없는 집합이므로 \\(P(V)\\)와 같은 표현을 불가함.\n결론: 확률변수 \\(X\\)를 고려할때 정의역의 치역 양쪽의 measurable space를 고려해야함.\n\n- 교재의 정의1\n\n\n\n그림1: Durret에서 긁어온 확률변수의 정의\n\n\n- 교재의 정의2\n\n\n\n그림2: Durret에서 긁어온 확률변수의 정의2\n\n\n- \\(X\\)가 랜덤변수라는 것을 기호로 간단하게 \\(X \\in {\\cal F}\\) 혹은 \\(X : (\\Omega, {\\cal F}) \\to (\\mathbb{R},{\\cal R})\\)라고 쓴다.\n\n사실 \\(X: (\\Omega,{\\cal F}) \\to (\\mathbb{R}, {\\cal R})\\)은 \\(X\\)가 잴 수 있는 함수 (measurable function, measurable map) 임을 나타내는 기호이다.\n\n- “\\(X\\)를 확률변수라고 하자.” 라는 의미? 지금 까지 해온 모든 논의가 압축된 표현…\n\n확률이라는건 원래 모든 \\(\\Omega\\)에서는 잘 정의되지 않음.\n그래도 \\(\\Omega\\)의 부분집합중 잴 수 있는 집합이라는 것이 있는데 그게 \\({\\cal F}\\)야.\n이 두개를 세트로 묶어서 \\((\\Omega,{\\cal F})\\) 이라고 하고 이를 잴 수 있는 공간이라고 하자.\n이 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\) 에서는 이제 확률 \\(P\\)를 정의 할 수 있어.\n한편 \\(\\Omega\\)의 원소는 숫자로 되어있지 않으니까 이를 숫자화시키는 어떠한 함수가 필요한데 이것을 우리는 \\(X\\)라고 할 것임.\n그런데 \\(P(X=1)\\)와 같은 표현이 가능하려면 \\(X\\)의 inverse image가 \\({\\cal F}\\)의 원소이어야 하는데 이게 항상 가능한 것은 아니므로 \\(X\\)를 잴 수 있는 함수라고 추가가정 해야 함.\n\n\n“\\(X\\)를 확률변수라고 하자” 라고 선언하는 것은 아래의 효과를 가진다. (1) \\(\\Omega\\)에 대응하는 \\({\\cal F}\\)가 잘 정의되어 있다고 하자. (2) \\(P\\) 역시 잘 정의되어 있다고 하자. (3) \\(\\mathbb{R}\\)와 \\({\\cal R}\\)이 잘 정의되어 있다고 하자. (4) \\(X: (\\Omega,{\\cal F}) \\to (\\mathbb{R},{\\cal R})\\) 이 잘 정의되어 있다고 하자."
  },
  {
    "objectID": "posts/1. 측도론/2023-03-28-4wk-2.html#헷갈려-2-starstarstar",
    "href": "posts/1. 측도론/2023-03-28-4wk-2.html#헷갈려-2-starstarstar",
    "title": "04wk-2: 측도론 intro (6)",
    "section": "헷갈려 (2) (\\(\\star\\star\\star\\))",
    "text": "헷갈려 (2) (\\(\\star\\star\\star\\))\n- 확률변수에 대한 오해1: 학률변수 = 값이 랜덤으로 바뀌는 변수??\n\n함수: \\(y=f(x)\\), \\(f\\): function, \\(x\\): input \\(y\\): output\n확률변수: \\(x=X(\\omega)\\), \\(X\\): function, \\(\\omega\\): outcome1, \\(x\\): realization\n확률변수는 함수이지만 보통 \\(X(\\omega)\\)와 같이 쓰지 않고 \\(X\\)라고 쓴다. \\(\\Rightarrow\\) 혼란의 이유\n\n1 입력인데 outcome임, 여기서부터 너무 헷갈려!!- 확률변수에 대한 오해2: 확률변수는 결과가 랜덤으로 변한다??\n\n확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n\n- 확률변수에 대한 오해3: 아니야.. 확률변수는 결과가 랜덤으로 바뀌는 느낌이 맞아. 아래의 예시를 봐!\n\\[X = \\begin{cases} 0 & w.p. \\frac{1}{2} \\\\ 1 & w.p. \\frac{1}{2} \\end{cases}\\]\n\n\\(X\\)는 진짜 변수처럼 보이긴함.\n심지어 변수의 값이 랜덤으로 변하는 것 같음.\n\n(해설)\n정확하게는 아래 표현이 맞다.\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\{H\\} \\\\ 1 & \\omega \\in \\{T\\} \\end{cases} \\quad \\text{where } P(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2}.\\]\n- 확률변수에 대한 오해2에 대한 추가설명\n\n확률변수는 결과가 랜덤으로 변하는 함수가 아님, 확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n단지 입력 outcome이 실험에 따라 랜덤으로 변할 수 있는 것임!!\n\n- 요약해보면,\n\n확률변수는 확률과 관련없다.\n간접적으로는 관련이 있다. \\(\\because\\) X의 역상 = \\(\\Omega\\)의 부분집합 = \\(P\\)의 정의역\n\n- 표현연습: \\(P(X=1), P(X \\in \\{0,1\\}),\\dots ...\\)"
  },
  {
    "objectID": "posts/2023-06-20-15wk-2-fin.html",
    "href": "posts/2023-06-20-15wk-2-fin.html",
    "title": "15wk-2: 기말고사",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy\n\n\n1. 다음을 읽고 참 거짓을 판단하여라. (30점)\n(1) 전이확률행렬 \\({\\bf P}={\\bf I}\\)를 가지는 HMC는 irreducible 하다.\n(2) HMC \\(\\{X_t\\}\\)가 유한한 상태공간을 가진다면 irreducible 조건은 positive recurrent 를 암시한다.\n(3) HMC \\(\\{X_t\\}\\)가 irreducible 하다면 항상 전이확률행렬 \\({\\bf P}\\)가 수렴한다.\n(4) HMC \\(\\{X_t\\}\\)가 irreducible 하다면 positive recurrent 조건과 유일한 정상분포를 가질 조건이 동치이다.\n(5) HMC \\(\\{X_t\\}\\)가 정상분포를 가진다면 DBC (detailed balance condition) 을 만족한다.\n\n\n2. 페이지랭크 알고리즘 (20점)\n아래는 7개의 website에 대한 web graph이다.\n\n\n\n\nflowchart LR\n  0 --&gt;|1/2| 1\n  1 --&gt;|1/2| 0\n  0 --&gt;|1/2| 2\n  1 --&gt;|1/2| 2\n  2 --&gt;|1| 3 \n  4 --&gt;|1| 3\n  3 --&gt;|1| 5 \n  6 --&gt;|1| 5\n\n\n\n\n\n구글의 페이지랭크 알고리즘을 이용하여 위의 website들의 중요도를 랭킹하라. 단, 이때 구글매트릭스를 만들기 위한 \\(\\alpha\\)는 0.85로 설정하라.\nhint: 아래의 매트릭스를 적절하게 수정하여 만들어라.\n\nP = np.array([0.0, 1/2, 1/2, 0.0, 0.0, 0.0, 0.0,\n              1/2, 0.0, 1/2, 0.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n              0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ### 이 부분은 다 0이므로 수정이 필요함!!\n              0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]).reshape(7,7)\nP\n\narray([[0. , 0.5, 0.5, 0. , 0. , 0. , 0. ],\n       [0.5, 0. , 0.5, 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 1. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 1. , 0. ],\n       [0. , 0. , 0. , 1. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 1. , 0. ]])\n\n\n\n\n3. MH-알고리즘 (20점)\n매트로폴리스 헤이스팅스 알고리즘을 사용하여 \\(X_t \\sim {\\cal B}(2,6)\\)인 따르는 확률변수열 \\(\\{X_t\\}\\)를 샘플링하라. 샘플링결과를 히스토그램으로 시각화하고, 시각화 결과를\nnp.random.beta(2,6,size=100000)\n으로 생성한 결과와 비교하라.\n\n\n4. LDA (30점)\n아래는 장하니 학생의 2023년 확률과정론 필기자료를 바탕으로 작성한 코퍼스이다.\n\nD = {'doc1': ['기대값', '기대값', '르벡메져', '잴 수 있는 집합', 'outcome', '확률과정', '잴 수 있는 집합', '잴 수 있는 집합', '잴 수 있는 집합', '시그마필드', '평균', '기대값', '기대값', '전사', '확률변수', 'event', '전단사', '시그마필드', '전단사', '르벡메져', '시그마필드', '시그마필드', '확률', '확률', '메져', '메져', '확률', '확률과정', '르벡메져', '기대값'], 'doc2': ['기대값', '확률변수', '르벡메져', '전단사', '기대값', '확률', '확률', '단사', '확률변수', '르벡메져', '확률변수', '확률', '잴 수 있는 집합', '기대값', '시그마필드', '르벡메져', '시그마필드', 'countable', 'countable', '가산집합', '확률과정', '확률', '기대값', '기대값', '카디널리티', '기대값', '잴 수 있는 함수', '확률과정', '잴 수 있는 집합', '잴 수 있는 함수'], 'doc3': ['전단사', 'event', '전사', '시그마필드', '평균', '카디널리티', 'countable', '전단사', 'event', '평균', '메져', '확률변수', '확률변수', '기대값', '기대값', 'event', '기대값', '확률과정', '카디널리티', '확률', '단사', 'outcome', 'countable', '가산집합', '가산집합', '확률변수', '기대값', '확률변수', '잴 수 있는 집합', '전사'], 'doc4': ['잴 수 있는 함수', '가산집합', '평균', '가산집합', '확률', '확률변수', '메져', 'outcome', 'countable', '잴 수 있는 함수', '기대값', 'countable', '전사', '르벡메져', 'outcome', '잴 수 있는 집합', '카디널리티', 'outcome', '시그마필드', '가산집합', '르벡메져', '잴 수 있는 함수', '카디널리티', '메져', '시그마필드', '확률과정', 'event', '가산집합', '단사', '가산집합'], 'doc5': ['확률', 'countable', '기대값', '카디널리티', '전사', 'outcome', '시그마필드', '메져', '잴 수 있는 함수', '확률변수', '전사', '단사', '기대값', '가산집합', 'event', '평균', 'event', 'outcome', '잴 수 있는 집합', 'outcome', '카디널리티', '확률', '메져', '평균', '평균', '잴 수 있는 집합', '기대값', '확률과정', '확률', '시그마필드'], 'doc6': ['단사', '평균', '기대값', '확률', 'event', '확률', '잴 수 있는 함수', 'outcome', '메져', '확률변수', '시그마필드', '잴 수 있는 집합', '카디널리티', '전사', '기대값', '시그마필드', '잴 수 있는 함수', '전사', '확률과정', '카디널리티', '전단사', '메져', '단사', '전단사', '시그마필드', 'countable', '카디널리티', '확률과정', '확률변수', '메져'], 'doc7': ['확률변수', '메져', '메져', '잴 수 있는 함수', '르벡메져', '단사', '확률변수', '평균', '카디널리티', '평균', '전사', '확률변수', '평균', '확률변수', '잴 수 있는 집합', '가산집합', '메져', '확률과정', '확률변수', '르벡메져', 'event', '카디널리티', 'countable', '기대값', '르벡메져', '잴 수 있는 집합', '시그마필드', '메져', '메져', '메져'], 'doc8': ['기대값', 'countable', '확률', '전단사', '확률', '단사', '확률변수', 'event', '확률변수', '잴 수 있는 집합', '가산집합', 'countable', '전단사', 'event', 'outcome', '전사', '확률변수', '확률', '카디널리티', '시그마필드', '가산집합', '기대값', '전단사', 'outcome', '전사', 'event', 'event', 'outcome', '평균', '가산집합'], 'doc9': ['단사', '기대값', '기대값', 'outcome', '잴 수 있는 집합', '전단사', '메져', '시그마필드', '가산집합', '전사', '단사', '르벡메져', '기대값', '기대값', '가산집합', '확률변수', '시그마필드', 'event', 'outcome', '르벡메져', '단사', '잴 수 있는 함수', '전사', 'event', '기대값', '단사', '전단사', '전단사', '평균', '시그마필드'], 'doc10': ['기대값', '시그마필드', '시그마필드', '단사', '확률변수', 'event', 'event', '확률과정', '시그마필드', '르벡메져', '카디널리티', '확률변수', '카디널리티', '전사', 'countable', '단사', '단사', '르벡메져', 'countable', '평균', '전단사', '시그마필드', '가산집합', '기대값', '전사', '잴 수 있는 집합', '시그마필드', 'outcome', '가산집합', '확률변수'], 'doc11': ['확률변수', '마코프체인', '확률과정', 'irreducible', '상태공간', 'ergodic', '극한분포', 'ergodic', 'recurrent', '확률변수', '극한분포', '전이확률', '전이확률', 'homogeneous', 'transient', '상태공간', 'theorem', 'detailed balance condition', 'theorem', '마코프체인', '마코프체인', '정상분포', '분포', 'detailed balance condition', '확률변수', 'detailed balance condition', '정상분포', 'homogeneous', '전이확률', 'theorem'], 'doc12': ['aperiodic', '정상분포', '확률과정', '극한분포', 'homogeneous', '분포', '확률과정', '마코프체인', 'homogeneous', '확률변수', '확률과정', 'irreducible', '정상분포', 'detailed balance condition', '상태공간', 'ergodic', 'transient', '확률변수', '전이확률', 'ergodic', '상태공간', 'theorem', 'detailed balance condition', '확률변수', '상태공간', '전이확률', '전이확률', '마코프체인', '극한분포', '극한분포'], 'doc13': ['irreducible', '마코프체인', '마코프체인', '상태공간', '마코프체인', '상태공간', 'irreducible', 'recurrent', 'irreducible', '마코프체인', '정상분포', 'detailed balance condition', 'irreducible', 'detailed balance condition', '정상분포', 'ergodic', 'theorem', 'aperiodic', '전이확률', '확률과정', '확률변수', '분포', 'aperiodic', 'transient', '전이확률', 'homogeneous', 'ergodic', 'theorem', '분포', '상태공간'], 'doc14': ['확률과정', '확률변수', 'theorem', 'recurrent', 'detailed balance condition', 'homogeneous', 'irreducible', 'ergodic', '마코프체인', '분포', '상태공간', 'transient', 'ergodic', '상태공간', 'homogeneous', '정상분포', '상태공간', 'theorem', '확률변수', 'transient', '분포', '마코프체인', 'detailed balance condition', '확률과정', 'aperiodic', 'ergodic', 'ergodic', 'irreducible', '분포', 'ergodic'], 'doc15': ['detailed balance condition', '전이확률', '분포', '분포', '확률과정', '확률변수', 'homogeneous', 'irreducible', 'homogeneous', '확률과정', 'theorem', '마코프체인', '정상분포', '상태공간', '확률과정', '마코프체인', 'recurrent', '확률변수', 'transient', '마코프체인', 'homogeneous', 'irreducible', 'transient', 'detailed balance condition', '마코프체인', '극한분포', 'detailed balance condition', 'ergodic', 'theorem', '전이확률'], 'doc16': ['마코프체인', '정상분포', 'detailed balance condition', '확률과정', 'homogeneous', 'aperiodic', '극한분포', 'aperiodic', 'homogeneous', 'transient', '마코프체인', '전이확률', '극한분포', 'irreducible', 'irreducible', 'recurrent', 'transient', '확률변수', '정상분포', '극한분포', '확률변수', 'recurrent', '극한분포', 'recurrent', '상태공간', '분포', 'ergodic', '마코프체인', 'theorem', 'homogeneous'], 'doc17': ['극한분포', '전이확률', '정상분포', 'irreducible', '상태공간', 'ergodic', 'ergodic', '상태공간', '극한분포', 'transient', '상태공간', 'ergodic', 'irreducible', 'transient', '확률과정', 'ergodic', 'irreducible', '마코프체인', 'aperiodic', '전이확률', '확률변수', 'aperiodic', 'detailed balance condition', '전이확률', '전이확률', 'detailed balance condition', 'ergodic', '마코프체인', 'transient', '극한분포'], 'doc18': ['정상분포', '상태공간', 'transient', '확률변수', 'detailed balance condition', 'aperiodic', 'homogeneous', '극한분포', '정상분포', 'ergodic', 'aperiodic', 'transient', 'recurrent', '확률과정', '확률변수', 'homogeneous', '상태공간', 'aperiodic', '확률변수', 'irreducible', '확률과정', 'theorem', '정상분포', 'irreducible', '전이확률', 'aperiodic', '전이확률', '확률과정', '정상분포', 'ergodic'], 'doc19': ['극한분포', 'recurrent', 'aperiodic', 'irreducible', '확률변수', 'aperiodic', 'ergodic', 'transient', '극한분포', 'homogeneous', 'theorem', 'ergodic', 'homogeneous', 'transient', '극한분포', 'transient', 'irreducible', 'homogeneous', '분포', '정상분포', '정상분포', 'aperiodic', 'ergodic', '확률과정', '확률과정', '정상분포', 'irreducible', '마코프체인', '극한분포', 'homogeneous'], 'doc20': ['확률과정', 'detailed balance condition', '확률변수', 'aperiodic', 'irreducible', '확률과정', 'recurrent', 'theorem', 'transient', '전이확률', 'recurrent', 'recurrent', 'aperiodic', '전이확률', '확률변수', 'ergodic', '확률과정', '상태공간', '마코프체인', '확률변수', 'recurrent', '분포', '마코프체인', '마코프체인', 'ergodic', 'aperiodic', '마코프체인', 'ergodic', '확률변수', 'homogeneous'], 'doc21': ['페이지랭크', 'MDP', '메트로폴리스-헤이스팅스', '페이지랭크', '메트로폴리스-헤이스팅스', '샘플링', '구글매트릭스', '마코프체인', '몬테카를로', '강화학습', 'MCMC', '계층모형', '구글매트릭스', '메트로폴리스-헤이스팅스', '샘플링', '구글', '구글매트릭스', '베이지안', '계층모형', '몬테카를로', '몬테카를로', '몬테카를로', '계층모형', '계층모형', 'MCMC', '강화학습', '베이지안', '계층모형', 'MDP', '메트로폴리스-헤이스팅스'], 'doc22': ['강화학습', '강화학습', '구글매트릭스', 'MCMC', '계층모형', 'LDA', '강화학습', 'MCMC', '메트로폴리스-헤이스팅스', 'LDA', '구글매트릭스', '계층모형', '구글', '구글', '샘플링', '구글', '구글', '메트로폴리스-헤이스팅스', 'LDA', '베이지안', '강화학습', '마코프체인', '베이지안', '메트로폴리스-헤이스팅스', '강화학습', '마코프체인', '몬테카를로', '메트로폴리스-헤이스팅스', 'MCMC', '구글'], 'doc23': ['강화학습', '마코프체인', '계층모형', 'LDA', 'MDP', '구글매트릭스', '계층모형', '베이지안', '마코프체인', '계층모형', 'MCMC', '구글매트릭스', '몬테카를로', 'MCMC', 'LDA', '마코프체인', '베이지안', '강화학습', '구글', '베이지안', '몬테카를로', 'MCMC', '몬테카를로', '마코프체인', '강화학습', '구글', '구글매트릭스', '몬테카를로', '몬테카를로', '샘플링'], 'doc24': ['베이지안', 'MDP', '구글', '메트로폴리스-헤이스팅스', 'LDA', 'MCMC', '계층모형', '몬테카를로', 'MDP', '구글매트릭스', '마코프체인', '페이지랭크', '계층모형', '구글매트릭스', '구글매트릭스', 'MCMC', '베이지안', '계층모형', '구글매트릭스', '구글', 'MCMC', '페이지랭크', '구글매트릭스', 'MCMC', 'MDP', '몬테카를로', '몬테카를로', 'MCMC', '구글', '계층모형'], 'doc25': ['구글매트릭스', 'MCMC', 'LDA', '마코프체인', '페이지랭크', '구글매트릭스', '메트로폴리스-헤이스팅스', '페이지랭크', '구글매트릭스', '강화학습', 'MDP', 'MCMC', '페이지랭크', '베이지안', '몬테카를로', '페이지랭크', '구글매트릭스', '몬테카를로', '몬테카를로', '계층모형', '베이지안', '페이지랭크', 'MDP', '샘플링', '구글매트릭스', '구글', '샘플링', 'MDP', '마코프체인', 'LDA'], 'doc26': ['메트로폴리스-헤이스팅스', '샘플링', '샘플링', 'MDP', '계층모형', 'MDP', '마코프체인', '샘플링', '강화학습', '샘플링', '강화학습', '강화학습', '베이지안', '샘플링', '마코프체인', '계층모형', '강화학습', '샘플링', '샘플링', '베이지안', '강화학습', '강화학습', '마코프체인', '메트로폴리스-헤이스팅스', 'MDP', '마코프체인', '페이지랭크', '구글매트릭스', '계층모형', '마코프체인'], 'doc27': ['LDA', '구글매트릭스', '페이지랭크', 'MDP', '구글', '페이지랭크', '몬테카를로', 'MCMC', '몬테카를로', '계층모형', '메트로폴리스-헤이스팅스', '샘플링', '페이지랭크', '계층모형', 'MCMC', '계층모형', '구글매트릭스', '구글', 'LDA', '페이지랭크', '메트로폴리스-헤이스팅스', '메트로폴리스-헤이스팅스', 'LDA', 'MCMC', '페이지랭크', '몬테카를로', 'MDP', 'MCMC', 'MCMC', '샘플링'], 'doc28': ['구글', '몬테카를로', 'LDA', '강화학습', 'MCMC', '샘플링', '계층모형', '강화학습', '몬테카를로', '베이지안', '구글', '페이지랭크', '강화학습', '샘플링', '페이지랭크', '몬테카를로', '베이지안', '구글매트릭스', '페이지랭크', '메트로폴리스-헤이스팅스', '강화학습', '강화학습', '샘플링', 'MCMC', '페이지랭크', 'LDA', '구글', '샘플링', 'MCMC', '마코프체인'], 'doc29': ['페이지랭크', '계층모형', '샘플링', '구글', 'MCMC', '구글매트릭스', 'MCMC', '구글매트릭스', '메트로폴리스-헤이스팅스', '몬테카를로', 'LDA', '구글매트릭스', '페이지랭크', '구글', '페이지랭크', '샘플링', '몬테카를로', '구글', '강화학습', 'MDP', '계층모형', '메트로폴리스-헤이스팅스', '계층모형', 'LDA', 'MDP', '구글매트릭스', 'MCMC', 'LDA', '계층모형', '메트로폴리스-헤이스팅스'], 'doc30': ['구글매트릭스', 'MCMC', '메트로폴리스-헤이스팅스', 'MCMC', '계층모형', '메트로폴리스-헤이스팅스', '강화학습', '베이지안', '계층모형', '샘플링', '베이지안', '베이지안', '베이지안', '구글', 'MDP', '계층모형', '강화학습', '마코프체인', '몬테카를로', 'MCMC', 'LDA', '페이지랭크', '몬테카를로', '구글매트릭스', 'MDP', '구글', '구글매트릭스', '몬테카를로', '몬테카를로', '강화학습']}\ndf = pd.DataFrame(D)\ndf.head()\n\n\n\n\n\n\n\n\ndoc1\ndoc2\ndoc3\ndoc4\ndoc5\ndoc6\ndoc7\ndoc8\ndoc9\ndoc10\n...\ndoc21\ndoc22\ndoc23\ndoc24\ndoc25\ndoc26\ndoc27\ndoc28\ndoc29\ndoc30\n\n\n\n\n0\n기대값\n기대값\n전단사\n잴 수 있는 함수\n확률\n단사\n확률변수\n기대값\n단사\n기대값\n...\n페이지랭크\n강화학습\n강화학습\n베이지안\n구글매트릭스\n메트로폴리스-헤이스팅스\nLDA\n구글\n페이지랭크\n구글매트릭스\n\n\n1\n기대값\n확률변수\nevent\n가산집합\ncountable\n평균\n메져\ncountable\n기대값\n시그마필드\n...\nMDP\n강화학습\n마코프체인\nMDP\nMCMC\n샘플링\n구글매트릭스\n몬테카를로\n계층모형\nMCMC\n\n\n2\n르벡메져\n르벡메져\n전사\n평균\n기대값\n기대값\n메져\n확률\n기대값\n시그마필드\n...\n메트로폴리스-헤이스팅스\n구글매트릭스\n계층모형\n구글\nLDA\n샘플링\n페이지랭크\nLDA\n샘플링\n메트로폴리스-헤이스팅스\n\n\n3\n잴 수 있는 집합\n전단사\n시그마필드\n가산집합\n카디널리티\n확률\n잴 수 있는 함수\n전단사\noutcome\n단사\n...\n페이지랭크\nMCMC\nLDA\n메트로폴리스-헤이스팅스\n마코프체인\nMDP\nMDP\n강화학습\n구글\nMCMC\n\n\n4\noutcome\n기대값\n평균\n확률\n전사\nevent\n르벡메져\n확률\n잴 수 있는 집합\n확률변수\n...\n메트로폴리스-헤이스팅스\n계층모형\nMDP\nLDA\n페이지랭크\n계층모형\n구글\nMCMC\nMCMC\n계층모형\n\n\n\n\n5 rows × 30 columns\n\n\n\n아래는 이 자료의 일부문서 (문서1, 문서15, 문서30)을 시각화한 것이다. 문서1에는 “측도론” 관련 단어들이, 문서15에는 “마코프체인” 관련 단어들이, 그리고 문서30에는 “마코체인의 응용”과 관련된 단어들이 포함되어 있다는 것을 알수 있다.\n\ndf.stack().reset_index().rename({'level_1':'doc',0:'word'},axis=1).groupby(['doc','word']).agg('count').\\\nstack().reset_index().rename({0:'count'},axis=1).query('doc in [\"doc1\",\"doc15\",\"doc30\"]').\\\nplot.bar(backend='plotly',x='word',y='count',facet_row=\"doc\",height=800)\n\n                                                \n\n\n즉 이 문서에는 총 3개의 토픽에 해당하는 단어가 있으며, 각 토픽은 (1) 측도론 (2) 마코프체인 (3) 마코프체인의 응용이다. 토픽수를 3으로 설정한 뒤 Latent Dirichlet Allocation (LDA)1 를 이용하여 각 단어를 적절한 토픽으로 분류하라.\n\n\n1 Blei, Ng, and Jordan (2003)\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "확률과정론\nguebin@jbnu.ac.kr\n자연과학대학 본관 205호"
  }
]