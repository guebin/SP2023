{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A2: 강화학습 (2) – 4x4 grid\n",
        "\n",
        "최규빈  \n",
        "2023-08-30\n",
        "\n",
        "# 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-zHvVuJ92xfdypwHwDFgg8k&si=iI4IhthblTsJTmIv>\n",
        "\n",
        "# Game2: 4x4 grid\n",
        "\n",
        "`-` 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에\n",
        "도달하도록 학습하는 방법\n",
        "\n",
        "# imports"
      ],
      "id": "34afd1c9-1878-428a-b6bf-5975abe4b0b2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "5b1cfd48-d338-4ddd-8b59-f8ab593ebda9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 예비학습: 시각화"
      ],
      "id": "0ac4c133-9867-4cdc-992b-e0192fd1cead"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    def update(t):\n",
        "        sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "0b3ec243-70a1-46e7-bd95-2bb4a5338898"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "show([[0,0],[0,1],[1,1],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3]])"
      ],
      "id": "5379f2af-62d5-43ff-b16f-4689e7a53fe5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Env 클래스 구현\n",
        "\n",
        "`-` GridWorld: 강화학습에서 많이 예시로 사용되는 기본적인 시뮬레이션\n",
        "환경\n",
        "\n",
        "1.  **State**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **Action**: 에이전트는 현재상태에서 다음상태로 이동하기 위해\n",
        "    상,하,좌,우 중 하나의 행동을 취할 수 있음.\n",
        "3.  **Reward**: 에이전트가 현재상태에서 특정 action을 하면 얻어지는 보상\n",
        "4.  **Terminated**: 하나의 에피소드가 종료되었음을 나타내는 상태"
      ],
      "id": "fb35ce68-2943-419c-acb7-af874e623038"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = 3\n",
        "current_state = np.array([1,1])"
      ],
      "id": "aedc105c-7a89-46c1-8f5b-100d938085a5"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "next_state = current_state + action_to_direction[action]\n",
        "next_state"
      ],
      "id": "e50e1ecd-765b-4764-9ac4-30c4234352e5"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.action_space = gym.spaces.Discrete(4) \n",
        "        self._action_to_direction = { \n",
        "            0 : np.array([1, 0]), # x+ \n",
        "            1 : np.array([0, 1]), # y+ \n",
        "            2 : np.array([-1 ,0]), # x- \n",
        "            3 : np.array([0, -1]) # y- \n",
        "        }\n",
        "    def reset(self):\n",
        "        self.agent_action = None \n",
        "        self.agent_state = np.array([0,0])        \n",
        "        return self.agent_state \n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.agent_state = self.agent_state + direction\n",
        "        if self.agent_state not in env.state_space: # 4x4 그리드 밖에 있는 경우\n",
        "            reward = -10 \n",
        "            terminated = True\n",
        "            self.agent_state = self.agent_state -1/2 * direction\n",
        "        elif np.array_equal(env.agent_state, np.array([3,3])): # 목표지점에 도달할 경우 \n",
        "            reward = 100 \n",
        "            terminated = True\n",
        "        else: \n",
        "            reward = -1 \n",
        "            terminated = False         \n",
        "        return self.agent_state, reward, terminated\n"
      ],
      "id": "414b9e1a-c494-4c0f-abc3-16fe43d7ede5"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()"
      ],
      "id": "d90d8d11-e438-4528-ac96-107fe32c39be"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [] \n",
        "state = env.reset()\n",
        "states.append(state) \n",
        "for t in range(50):\n",
        "    action = env.action_space.sample() \n",
        "    state,reward,terminated = env.step(action)\n",
        "    states.append(state) \n",
        "    if terminated: break "
      ],
      "id": "f8bcc04d-d188-4c44-86bb-bf2394cd66d8"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "886e5543-619a-4488-a39f-77bc8a2fa254"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent1 클래스 구현 + Run\n",
        "\n",
        "`-` 우리가 구현하고 싶은 기능\n",
        "\n",
        "-   `.act()`: 액션을 결정 –\\> 여기서는 그냥 랜덤액션\n",
        "-   `.save_experience()`: 데이터를 저장 –\\> 여기에 일단 초점을 맞추자\n",
        "-   `.learn()`: 데이터로에서 학습 –\\> 패스\n",
        "\n",
        "`-` 첫번째 시도"
      ],
      "id": "d50b3d90-7d8f-4169-935e-07f6916597a2"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent1:\n",
        "    def __init__(self,env):\n",
        "        self.action_space = env.action_space\n",
        "        self.state_spcae = env.state_space \n",
        "        self.n_experiences = 0 \n",
        "        self.n_episodes = 0 \n",
        "        self.score = 0 \n",
        "        \n",
        "        # episode-wise info \n",
        "        self.scores = [] \n",
        "        self.playtimes = []\n",
        "\n",
        "        # time-wise info\n",
        "        self.current_state = None \n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.next_state = None         \n",
        "        self.terminated = None \n",
        "\n",
        "        # replay_buffer \n",
        "        self.actions = []\n",
        "        self.current_states = [] \n",
        "        self.rewards = []\n",
        "        self.next_states = [] \n",
        "        self.terminations = [] \n",
        "\n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample() \n",
        "\n",
        "    def save_experience(self):\n",
        "        self.actions.append(self.action) \n",
        "        self.current_states.append(self.current_state)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated) \n",
        "        self.n_experiences += 1 \n",
        "        self.score = self.score + self.reward \n",
        "        \n",
        "    def learn(self):\n",
        "        pass "
      ],
      "id": "e146d5e8-4c91-45ce-990f-f501db858092"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 1  Score: -21  Playtime: 12\n",
            "Epsiode: 2  Score: -10  Playtime: 1\n",
            "Epsiode: 3  Score: -11  Playtime: 2\n",
            "Epsiode: 4  Score: -10  Playtime: 1\n",
            "Epsiode: 5  Score: -10  Playtime: 1\n",
            "Epsiode: 6  Score: -11  Playtime: 2\n",
            "Epsiode: 7  Score: -18  Playtime: 9\n",
            "Epsiode: 8  Score: 93   Playtime: 8\n",
            "Epsiode: 9  Score: -13  Playtime: 4\n",
            "Epsiode: 10     Score: -13  Playtime: 4\n",
            "Epsiode: 11     Score: -18  Playtime: 9\n",
            "Epsiode: 12     Score: -10  Playtime: 1\n",
            "Epsiode: 13     Score: -10  Playtime: 1\n",
            "Epsiode: 14     Score: -10  Playtime: 1\n",
            "Epsiode: 15     Score: -10  Playtime: 1\n",
            "Epsiode: 16     Score: -16  Playtime: 7\n",
            "Epsiode: 17     Score: -10  Playtime: 1\n",
            "Epsiode: 18     Score: -24  Playtime: 15\n",
            "Epsiode: 19     Score: -13  Playtime: 4\n",
            "Epsiode: 20     Score: -10  Playtime: 1"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = Agent1(env) \n",
        "for _ in range(20):\n",
        "    ## 본질적인 코드 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act() \n",
        "        env.agent_action = agent.action  \n",
        "        # step2: agent << env \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience() \n",
        "        # step3: learn \n",
        "        # agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    ## 덜 본질적인 코드 \n",
        "    print(\n",
        "        f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "        f\"Score: {agent.scores[-1]} \\t\"\n",
        "        f\"Playtime: {agent.playtimes[-1]}\"\n",
        "    )   "
      ],
      "id": "1d53c7dc-4e20-4e65-9d10-56b071baef9a"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(agent.playtimes[:7])"
      ],
      "id": "39b306fe-1afb-4a96-9908-f931d1e96e8c"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(agent.playtimes[:8])"
      ],
      "id": "c93e1dd8-ceff-4789-8809-244f639a2d09"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[28:36]\n",
        "show(states)"
      ],
      "id": "e251a785-472d-45ce-9aa8-9e5086fdd5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   우연히 잘맞춘 케이스\n",
        "\n",
        "# 환경의 이해 (1차원적 이해)\n",
        "\n",
        "`-` 무작위로 10000판을 진행해보자."
      ],
      "id": "2c712f0d-bb26-40c3-8f71-08e9e9b1d1d4"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld() \n",
        "agent = Agent1(env) \n",
        "for _ in range(10000):\n",
        "    ## 본질적인 코드 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act() \n",
        "        env.agent_action = agent.action  \n",
        "        # step2: agent << env \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience() \n",
        "        # step3: learn \n",
        "        # agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 "
      ],
      "id": "40b12099-567e-419d-b629-d34f01810511"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "ceba972e-030e-4bd1-ae1d-5367cea52107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "42475199-1649-4495-af5f-12240eaec1af"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[0], agent.actions[0], agent.rewards[0], agent.next_states[0]"
      ],
      "id": "907a43bc-1644-4ad2-ba8f-f9296035721a"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[1], agent.actions[1], agent.rewards[1], agent.next_states[1]"
      ],
      "id": "57ae01f6-3178-477f-ba2b-b708a91338c4"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[2], agent.actions[2], agent.rewards[2], agent.next_states[2]"
      ],
      "id": "9fdc44e3-02df-4b0b-a4e4-39765d50e7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[3], agent.actions[3], agent.rewards[3], agent.next_states[3]"
      ],
      "id": "69c21c21-3995-4167-8741-becd4ac9a2a1"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[4], agent.actions[4], agent.rewards[4], agent.next_states[4]"
      ],
      "id": "4288e684-decd-4056-83bd-9f2a0b79ec35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "33bcede5-b9b6-48a7-ac71-3905cf9f3d67"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i] \n",
        "    a = agent.actions[i] \n",
        "    q[x,y,a] = q[x,y,a] + agent.rewards[i] \n",
        "    count[x,y,a] = count[x,y,a] + 1 "
      ],
      "id": "dc434942-f67f-4c5f-8315-6a6f2da7a175"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "count[count == 0] = 0.01 \n",
        "q = q/count"
      ],
      "id": "6a2698f4-75cd-463b-9256-c84a23793b1a"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[:,:,3]"
      ],
      "id": "b0da7c22-98c3-421f-888c-a441da28700d"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}\\n\" \n",
        "        f\"action-value function = \\n {q[:,:,a]}\\n\" \n",
        ")"
      ],
      "id": "c7575209-a13d-4851-a793-dc61bd46f5da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2)"
      ],
      "id": "76dadc89-da37-4965-ba6a-a999a6ed9efd"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n",
        "    q_realistic = agent.rewards[i] # 실제 답 \n",
        "    diff = q_realistic - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값 \n",
        "    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 "
      ],
      "id": "914e7423-5d32-4b5a-9a2c-3cf0324e721a"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function = \n",
            " [[-1.         -1.         -1.         -0.99866234]\n",
            " [-1.         -1.         -1.         -0.99851783]\n",
            " [-0.99999999 -1.         -0.99999593 98.43103943]\n",
            " [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n",
            "\n",
            "action = 1\n",
            "action-value function = \n",
            " [[-1.         -1.         -1.         -9.98591939]\n",
            " [-1.         -1.         -0.99999996 -9.99588862]\n",
            " [-1.         -0.99999999 -0.99999593 -9.92731143]\n",
            " [-0.99915694 -0.99971289 98.50948746  0.        ]]\n",
            "\n",
            "action = 2\n",
            "action-value function = \n",
            " [[-10.         -10.          -9.99999999  -9.99065864]\n",
            " [ -1.          -1.          -0.99999999  -0.99923914]\n",
            " [ -1.          -1.          -0.99999321  -0.9884667 ]\n",
            " [ -0.99946866  -0.99981905  -0.99465672   0.        ]]\n",
            "\n",
            "action = 3\n",
            "action-value function = \n",
            " [[-10.          -1.          -1.          -0.99919909]\n",
            " [-10.          -1.          -1.          -0.99866234]\n",
            " [ -9.99999999  -1.          -0.99999285  -0.99541881]\n",
            " [ -9.99347658  -0.99987363  -0.99776587   0.        ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}\\n\" \n",
        "        f\"action-value function = \\n {q[:,:,a]}\\n\" \n",
        ")"
      ],
      "id": "0d2778f3-b24b-4a2b-9abd-78f1a687e9ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 환경의 깊은 이해 (좀 더 고차원적인 이해)\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "7fe158c0-e626-4a2b-ab7a-a1bff5f77cb1"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[:,:,1]"
      ],
      "id": "a1214644-c901-4eb9-8d32-563df474baf2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "ef2c061c-b3c8-4920-b685-2c69b77bd1d5"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,1]"
      ],
      "id": "08ca1c41-211a-4c55-85e9-bce10f20b3d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 –\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "434ca476-a529-4575-b3dc-9fedf33bcae1"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,1,1]"
      ],
      "id": "0586f726-9795-4435-89aa-f73dd145c176"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 –\\> 합리적일까??\n",
        "\n",
        "`-` 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지\n",
        "합리적이지 못함\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n",
        "-   0을 쓸때와 1을 쓸때 보상이 다름\n",
        "-   무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면\n",
        "    10만원을 보상을 준다는 것을 “알게 되었음”\n",
        "-   이때 빈 종이의 가치는 5만원인가? 10만원인가? –\\> 10만원아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$ $a=1$에서 추정된(esitated) 값은\n",
        "`q[3,1,1]= -0.9997128867462345` 이지만[1], 현실적으로는 “실제보상(-1)과\n",
        "잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n",
        "\n",
        "[1] 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음"
      ],
      "id": "dd8e00e7-1ab7-4f7f-9b41-669fcfb1acc0"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_estimated = q[3,1,1]\n",
        "q_estimated"
      ],
      "id": "cfa4a389-21a8-4505-a864-fe28600a15c5"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_realistic = (-1) + 0.99 * 100 \n",
        "q_realistic"
      ],
      "id": "c9c2af60-dfdb-4053-8609-3aa02e559007"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이=\n",
        "    십만원 으로 생각한다는 의미)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s$, $a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을\n",
        "좀 더 엄밀하게 쓰면 아래와 같다.\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases} \\text{reward}(s,a) & \\text{terminated} \\\\  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated}\\end{cases}$$"
      ],
      "id": "b31f9ab2-a0c6-4c81-ac45-f84232b2612a"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    xx,yy = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a] \n",
        "    if agent.terminations[i]:\n",
        "        q_realistic = agent.rewards[i]\n",
        "    else:\n",
        "        q_future = q[xx,yy,:].max()\n",
        "        q_realistic = agent.rewards[i] + 0.99 * q_future\n",
        "    diff = q_realistic - q_estimated \n",
        "    q[x,y,a] = q_estimated + 0.05 * diff "
      ],
      "id": "4d6ffc83-3edb-42a7-a12c-23dc77e849cc"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function = \n",
            " [[87.02554961 88.94759484 90.75390245 88.54847007]\n",
            " [88.4709728  91.06852327 93.18709107 94.21998722]\n",
            " [84.98258538 91.44091272 95.48024593 98.43103943]\n",
            " [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n",
            "\n",
            "action = 1\n",
            "action-value function = \n",
            " [[87.01670813 88.59888111 85.52951661 -9.98591939]\n",
            " [88.98190464 91.03081993 91.50379877 -9.99588862]\n",
            " [90.76721433 93.24316728 95.65715857 -9.92731143]\n",
            " [89.20612688 94.47295823 98.50948746  0.        ]]\n",
            "\n",
            "action = 2\n",
            "action-value function = \n",
            " [[-10.         -10.          -9.99999999  -9.99065864]\n",
            " [ 84.96179325  86.84873675  88.0518007   80.10750712]\n",
            " [ 86.40784936  88.69218405  89.83203868  83.06339754]\n",
            " [ 86.40852121  89.09508079  89.87262647   0.        ]]\n",
            "\n",
            "action = 3\n",
            "action-value function = \n",
            " [[-10.          84.96186287  86.49128928  84.57992176]\n",
            " [-10.          86.73523202  88.56505447  86.7154156 ]\n",
            " [ -9.99999999  88.3058275   90.27264766  87.96618484]\n",
            " [ -9.99347658  80.88548565  86.63274331   0.        ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}\\n\" \n",
        "        f\"action-value function = \\n {q[:,:,a]}\\n\" \n",
        ")"
      ],
      "id": "ff789b47-1166-40f2-af86-1403f91a3643"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "980107d1-6dea-4bde-a71d-ff58550014ba"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:]"
      ],
      "id": "4f17808b-c852-4027-baa0-f364d2dd3ee0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "38fa0bb9-2029-41bf-8759-d7f6b8dcfaec"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:]"
      ],
      "id": "b0a53547-7ae8-4950-b10c-2146755a87b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "fa274b3a-5a1e-46a2-9c4d-227573566227"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:]"
      ],
      "id": "92aac9f3-5e47-42bb-8e62-5bf64f36cc29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동1을 하는게 유리함\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "209c7a8d-17e3-4273-9376-3af7d95bc8ee"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:].argmax()"
      ],
      "id": "f1b692c3-94b6-4c9e-8a92-ff0bb836c83b"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:].argmax()"
      ],
      "id": "982e1340-a7d2-4e2a-84d8-f4d41072227b"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:].argmax()"
      ],
      "id": "ede24e2e-ed08-405e-870e-f9b780ec5451"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "28125870-9d22-4b9d-99d9-ed0bda0509fe"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "a9cd5f06-8a65-46e7-9f74-92ec7c2d9d59"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "directions = {0:'down', 1: 'right', 2:'up', 3:'left'} "
      ],
      "id": "dfdd98b8-89b5-450c-880c-1c408f564016"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x in range(4):\n",
        "    for y in range(4):\n",
        "        policy[x,y] = directions[q[x,y,:].argmax()]\n",
        "policy"
      ],
      "id": "c2c85140-c119-4f09-8529-d293f75662e8"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "q.max(axis=-1)"
      ],
      "id": "699a6066-80e7-415c-aa87-57036e07979f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent2 클래스 구현 + Run"
      ],
      "id": "531e7206-1d17-4ac7-af1f-e16919d524ee"
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent2(Agent1):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q = np.zeros([4,4,4]) \n",
        "    def learn(self):\n",
        "        x,y = self.current_state\n",
        "        xx,yy = self.next_state\n",
        "        a = self.action \n",
        "        q_estimated = self.q[x,y,a] \n",
        "        if self.terminated:\n",
        "            q_realistic = self.reward\n",
        "        else:\n",
        "            q_future = q[xx,yy,:].max()\n",
        "            q_realistic = self.reward + 0.99 * q_future\n",
        "        diff = q_realistic - q_estimated \n",
        "        self.q[x,y,a] = q_estimated + 0.05 * diff \n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            x,y = self.current_state \n",
        "            self.action = self.q[x,y,:].argmax()"
      ],
      "id": "fbd05240-8d9f-45d9-a704-f4aff617d46c"
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 100    Score: -10.36   Playtime: 3.56\n",
            "Epsiode: 200    Score: -10.9    Playtime: 3.0\n",
            "Epsiode: 300    Score: -11.02   Playtime: 3.12\n",
            "Epsiode: 400    Score: -6.64    Playtime: 4.24\n",
            "Epsiode: 500    Score: -11.08   Playtime: 3.18\n",
            "Epsiode: 600    Score: -10.53   Playtime: 3.73\n",
            "Epsiode: 700    Score: -9.96    Playtime: 3.16\n",
            "Epsiode: 800    Score: -8.6     Playtime: 2.9\n",
            "Epsiode: 900    Score: -13.6    Playtime: 7.61\n",
            "Epsiode: 1000   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1100   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1200   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1300   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1400   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1500   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1600   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1700   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1800   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 1900   Score: -50.0    Playtime: 50.0\n",
            "Epsiode: 2000   Score: -50.0    Playtime: 50.0"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = Agent2(env) \n",
        "for _ in range(2000):\n",
        "    ## 본질적인 코드 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act() \n",
        "        env.agent_action = agent.action  \n",
        "        # step2: agent << env \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience() \n",
        "        # step3: learn \n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    ## 덜 본질적인 코드 \n",
        "    if (agent.n_episodes % 100) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\"\n",
        "        )   "
      ],
      "id": "3bf71d4d-f5b3-48f7-a437-dd0eedb4d211"
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "3cc1eb34-48f5-4096-8bdb-d9af4cab8e62"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.q.max(-1).T"
      ],
      "id": "19b890c1-a87a-4371-ab09-093271b4a0e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agnet3 클래스 구현 + Run"
      ],
      "id": "09c13539-38e2-4581-9767-82b12c5b122c"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent3(Agent2):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0 \n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            x,y = self.current_state \n",
        "            self.action = self.q[x,y,:].argmax()"
      ],
      "id": "50575733-9c15-4c33-81fd-9a8b321a54d0"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 200    Score: -8.49    Playtime: 3.89  Epsilon:  0.82\n",
            "Epsiode: 400    Score: -9.83    Playtime: 4.13  Epsilon:  0.67\n",
            "Epsiode: 600    Score: -10.72   Playtime: 6.12  Epsilon:  0.55\n",
            "Epsiode: 800    Score: -7.08    Playtime: 7.98  Epsilon:  0.45\n",
            "Epsiode: 1000   Score: -1.87    Playtime: 10.65 Epsilon:  0.37\n",
            "Epsiode: 1200   Score: 28.23    Playtime: 10.16 Epsilon:  0.30\n",
            "Epsiode: 1400   Score: 61.38    Playtime: 6.62  Epsilon:  0.25\n",
            "Epsiode: 1600   Score: 66.42    Playtime: 5.98  Epsilon:  0.20\n",
            "Epsiode: 1800   Score: 74.94    Playtime: 6.26  Epsilon:  0.17\n",
            "Epsiode: 2000   Score: 75.29    Playtime: 5.91  Epsilon:  0.14\n",
            "Epsiode: 2200   Score: 77.24    Playtime: 6.16  Epsilon:  0.11\n",
            "Epsiode: 2400   Score: 86.1     Playtime: 6.1   Epsilon:  0.09\n",
            "Epsiode: 2600   Score: 83.81    Playtime: 6.19  Epsilon:  0.07\n",
            "Epsiode: 2800   Score: 87.27    Playtime: 6.03  Epsilon:  0.06\n",
            "Epsiode: 3000   Score: 86.1     Playtime: 6.1   Epsilon:  0.05\n",
            "Epsiode: 3200   Score: 87.37    Playtime: 5.93  Epsilon:  0.04\n",
            "Epsiode: 3400   Score: 93.68    Playtime: 6.22  Epsilon:  0.03\n",
            "Epsiode: 3600   Score: 90.58    Playtime: 6.02  Epsilon:  0.03\n",
            "Epsiode: 3800   Score: 92.77    Playtime: 6.03  Epsilon:  0.02\n",
            "Epsiode: 4000   Score: 93.79    Playtime: 6.11  Epsilon:  0.02\n",
            "Epsiode: 4200   Score: 94.88    Playtime: 6.12  Epsilon:  0.01\n",
            "Epsiode: 4400   Score: 92.85    Playtime: 5.95  Epsilon:  0.01\n",
            "Epsiode: 4600   Score: 94.96    Playtime: 6.04  Epsilon:  0.01\n",
            "Epsiode: 4800   Score: 94.92    Playtime: 6.08  Epsilon:  0.01\n",
            "Epsiode: 5000   Score: 93.9     Playtime: 6.0   Epsilon:  0.01"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = Agent3(env) \n",
        "agent.eps = 1\n",
        "for _ in range(5000):\n",
        "    ## 본질적인 코드 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act() \n",
        "        env.agent_action = agent.action  \n",
        "        # step2: agent << env \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience() \n",
        "        # step3: learn \n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    agent.eps = agent.eps * 0.999\n",
        "    ## 덜 본질적인 코드 \n",
        "    if (agent.n_episodes % 200) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "            f\"Epsilon: {agent.eps : .2f}\"\n",
        "        )   "
      ],
      "id": "50ba284c-2a6f-4218-b521-533352867e68"
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "c1513a2f-7822-4b77-af8f-6a11be9eb192"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  }
}